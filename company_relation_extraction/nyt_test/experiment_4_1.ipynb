{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#GPU\" data-toc-modified-id=\"GPU-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>GPU</a></div><div class=\"lev1 toc-item\"><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Preprocessing</a></div><div class=\"lev2 toc-item\"><a href=\"#Load-Data\" data-toc-modified-id=\"Load-Data-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Load Data</a></div><div class=\"lev3 toc-item\"><a href=\"#Load-Train-Data\" data-toc-modified-id=\"Load-Train-Data-211\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Load Train Data</a></div><div class=\"lev3 toc-item\"><a href=\"#Load-Test-Data\" data-toc-modified-id=\"Load-Test-Data-212\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Load Test Data</a></div><div class=\"lev3 toc-item\"><a href=\"#Relation-Types\" data-toc-modified-id=\"Relation-Types-213\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Relation Types</a></div><div class=\"lev2 toc-item\"><a href=\"#Participle\" data-toc-modified-id=\"Participle-22\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Participle</a></div><div class=\"lev2 toc-item\"><a href=\"#Make-Adjacency-List\" data-toc-modified-id=\"Make-Adjacency-List-23\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Make Adjacency List</a></div><div class=\"lev3 toc-item\"><a href=\"#Make-Adjacency-List-of-Train-Data\" data-toc-modified-id=\"Make-Adjacency-List-of-Train-Data-231\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Make Adjacency List of Train Data</a></div><div class=\"lev3 toc-item\"><a href=\"#Make-Adjacency-List-of-Test-Data\" data-toc-modified-id=\"Make-Adjacency-List-of-Test-Data-232\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Make Adjacency List of Test Data</a></div><div class=\"lev1 toc-item\"><a href=\"#Word-to-Vector\" data-toc-modified-id=\"Word-to-Vector-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Word to Vector</a></div><div class=\"lev2 toc-item\"><a href=\"#Tokenize-Text\" data-toc-modified-id=\"Tokenize-Text-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Tokenize Text</a></div><div class=\"lev2 toc-item\"><a href=\"#Create-Word-Embeddings-with-GloVe\" data-toc-modified-id=\"Create-Word-Embeddings-with-GloVe-32\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Create Word Embeddings with GloVe</a></div><div class=\"lev3 toc-item\"><a href=\"#Read-GloVe\" data-toc-modified-id=\"Read-GloVe-321\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Read GloVe</a></div><div class=\"lev3 toc-item\"><a href=\"#Use-Glove-to-Initialize-Embedding-Matrix\" data-toc-modified-id=\"Use-Glove-to-Initialize-Embedding-Matrix-322\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Use Glove to Initialize Embedding Matrix</a></div><div class=\"lev1 toc-item\"><a href=\"#Build-Dateset\" data-toc-modified-id=\"Build-Dateset-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Build Dateset</a></div><div class=\"lev1 toc-item\"><a href=\"#Save-Dataset\" data-toc-modified-id=\"Save-Dataset-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Save Dataset</a></div><div class=\"lev1 toc-item\"><a href=\"#Checkpoint\" data-toc-modified-id=\"Checkpoint-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Checkpoint</a></div><div class=\"lev1 toc-item\"><a href=\"#Build-Model\" data-toc-modified-id=\"Build-Model-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Build Model</a></div><div class=\"lev2 toc-item\"><a href=\"#Set-Hyperparameters\" data-toc-modified-id=\"Set-Hyperparameters-71\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Set Hyperparameters</a></div><div class=\"lev2 toc-item\"><a href=\"#Import-Libraries\" data-toc-modified-id=\"Import-Libraries-72\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Import Libraries</a></div><div class=\"lev2 toc-item\"><a href=\"#Build-Graph\" data-toc-modified-id=\"Build-Graph-73\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Build Graph</a></div><div class=\"lev2 toc-item\"><a href=\"#Model-Visualization\" data-toc-modified-id=\"Model-Visualization-74\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Model Visualization</a></div><div class=\"lev2 toc-item\"><a href=\"#Train\" data-toc-modified-id=\"Train-75\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;</span>Train</a></div><div class=\"lev1 toc-item\"><a href=\"#Evaluate\" data-toc-modified-id=\"Evaluate-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Evaluate</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Load Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentText = []\n",
    "relationMentions = []\n",
    "relationLabels = []\n",
    "entityMentions = []\n",
    "entityLabels = []\n",
    "em1Text = []\n",
    "em2Text = []\n",
    "\n",
    "with open(\"/Users/lizhn7/Downloads/DATA/nyt/train.json\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    item = json.loads(line)\n",
    "    # Exclude \"None\" label\n",
    "    if not all(i['label'] == 'None' for i in item['relationMentions']):\n",
    "        sentText.append(item['sentText'])\n",
    "        relationMentions.append(item['relationMentions'])\n",
    "        entityMentions.append(item['entityMentions'])\n",
    "    \n",
    "relationLabels = [[i['label'].split('/')[-1] for i in rM] for rM in relationMentions]\n",
    "entityLabels = [[i['text'] for i in eM] for eM in entityMentions]\n",
    "em1Text = [[i['em1Text'] for i in rM] for rM in relationMentions]\n",
    "em2Text = [[i['em2Text'] for i in rM] for rM in relationMentions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Clean words\n",
    "replaceDict = {\n",
    "               'ā': 'a', 'á': 'a', 'ǎ': 'a', 'à': 'a',\n",
    "               'ō': 'o', 'ó': 'o', 'ǒ': 'o', 'ò': 'o', 'ô': 'o', 'ö': 'o',\n",
    "               'ē': 'e', 'é': 'e', 'ě': 'e', 'è': 'e', \n",
    "               'ī': 'i', 'í': 'i', 'ǐ': 'i', 'ì': 'i',\n",
    "               'ū': 'u', 'ú': 'u', 'ǔ': 'u', 'ù': 'u', 'ü': 'u',\n",
    "               'ñ': 'n',\n",
    "               'É': 'E'\n",
    "              }\n",
    "em1Text = [[''.join([replaceDict.get(i, i) for i in e]) for e in eT] for eT in em1Text]\n",
    "em2Text = [[''.join([replaceDict.get(i, i) for i in e]) for e in eT] for eT in em2Text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t_sentText = []\n",
    "t_relationMentions = []\n",
    "t_relationLabels = []\n",
    "t_entityMentions = []\n",
    "t_entityLabels = []\n",
    "t_em1Text = []\n",
    "t_em2Text = []\n",
    "\n",
    "with open(\"/Users/lizhn7/Downloads/DATA/nyt/test.json\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    item = json.loads(line)\n",
    "    t_sentText.append(item['sentText'])\n",
    "    t_relationMentions.append(item['relationMentions'])\n",
    "    t_entityMentions.append(item['entityMentions'])\n",
    "    \n",
    "t_relationLabels = [[i['label'].split('/')[-1] for i in rM] for rM in t_relationMentions]\n",
    "t_entityLabels = [[i['text'] for i in eM] for eM in t_entityMentions]\n",
    "t_em1Text = [[i['em1Text'] for i in rM] for rM in t_relationMentions]\n",
    "t_em2Text = [[i['em2Text'] for i in rM] for rM in t_relationMentions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Clean words\n",
    "t_replaceDict = {\n",
    "               'ā': 'a', 'á': 'a', 'ǎ': 'a', 'à': 'a',\n",
    "               'ō': 'o', 'ó': 'o', 'ǒ': 'o', 'ò': 'o', 'ô': 'o', 'ö': 'o',\n",
    "               'ē': 'e', 'é': 'e', 'ě': 'e', 'è': 'e', \n",
    "               'ī': 'i', 'í': 'i', 'ǐ': 'i', 'ì': 'i',\n",
    "               'ū': 'u', 'ú': 'u', 'ǔ': 'u', 'ù': 'u', 'ü': 'u',\n",
    "               'ñ': 'n',\n",
    "               'É': 'E'\n",
    "              }\n",
    "t_em1Text = [[''.join([t_replaceDict.get(i, i) for i in e]) for e in eT] for eT in t_em1Text]\n",
    "t_em2Text = [[''.join([t_replaceDict.get(i, i) for i in e]) for e in eT] for eT in t_em2Text]\n",
    "t_entityLabels = [[''.join([t_replaceDict.get(i, i) for i in e]) for e in eT] for eT in t_entityLabels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###  Relation Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['religion',\n",
       " 'contains',\n",
       " 'country',\n",
       " 'teams',\n",
       " 'geographic_distribution',\n",
       " 'major_shareholders',\n",
       " 'administrative_divisions',\n",
       " 'place_of_birth',\n",
       " 'location',\n",
       " 'children',\n",
       " 'founders',\n",
       " 'advisors',\n",
       " 'neighborhood_of',\n",
       " 'people',\n",
       " 'nationality',\n",
       " 'profession',\n",
       " 'industry',\n",
       " 'capital',\n",
       " 'place_founded',\n",
       " 'company',\n",
       " 'major_shareholder_of',\n",
       " 'place_lived',\n",
       " 'place_of_death',\n",
       " 'ethnicity']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relationTypes = list(set([r for rl in relationLabels for r in rl]))\n",
    "relationTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['children',\n",
       " 'capital',\n",
       " 'contains',\n",
       " 'company',\n",
       " 'founders',\n",
       " 'place_lived',\n",
       " 'place_of_death',\n",
       " 'country',\n",
       " 'neighborhood_of',\n",
       " 'nationality',\n",
       " 'administrative_divisions',\n",
       " 'place_of_birth']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_relationTypes = list(set([r for rl in t_relationLabels for r in rl if r != 'None']))\n",
    "t_relationTypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Participle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cut(s):\n",
    "    \"\"\"\n",
    "    Participle\n",
    "    \"\"\"\n",
    "    pattern = r'''\n",
    "              (?x)                   # set flag to allow verbose regexps \n",
    "              (?:[A-Z]\\.)+           # abbreviations, e.g. U.S.A. \n",
    "              |\\d+(?:\\.\\d+)?%?       # numbers, incl. currency and percentages \n",
    "              |\\w+(?:[-&']\\w+)*       # words w/ optional internal hyphens/apostrophe  \n",
    "           '''  \n",
    "    return regexp_tokenize(s, pattern=pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentWords = [cut(s) for s in sentText]\n",
    "entlabWords = [[cut(s) for s in eL] for eL in entityLabels]\n",
    "em1Words = [[cut(s) for s in eL] for eL in em1Text]\n",
    "em2Words = [[cut(s) for s in eL] for eL in em2Text]\n",
    "t_sentWords = [cut(s) for s in t_sentText]\n",
    "t_entlabWords = [[cut(s) for s in eL] for eL in t_entityLabels]\n",
    "t_em1Words = [[cut(s) for s in eL] for eL in t_em1Text]\n",
    "t_em2Words = [[cut(s) for s in eL] for eL in t_em2Text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENT_LEN = 120\n",
    "MAX_ADJL_LEN = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Adjacency List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Adjacency List of Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i_entityLabels = []\n",
    "for i in range(len(sentWords)):\n",
    "    eL = []\n",
    "    sDict = list(enumerate(sentWords[i]))\n",
    "    j = 0\n",
    "    for item in entlabWords[i]:\n",
    "        el = []\n",
    "        for e in item:\n",
    "            while j < len(sDict):\n",
    "                if e == sDict[j][1]:\n",
    "                    el.append(sDict[j][0])\n",
    "                    j += 1\n",
    "                    break\n",
    "                j += 1\n",
    "        eL.append(el)\n",
    "    i_entityLabels.append(eL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmSet = []\n",
    "for item in range(len(relationMentions)):\n",
    "    rms = [(em1Words[item][i][0], em2Words[item][i][0], relationLabels[item][i]) for i in range(len(relationLabels[item]))]\n",
    "    rms = [' '.join(list(i)) for i in rms]\n",
    "    rmSet.append(rms)\n",
    "rmSet = [set(i) for i in rmSet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i_adjacencyList = []\n",
    "new_sentWords = []\n",
    "for n in range(len(sentWords)):\n",
    "    aL = []\n",
    "    for l in range(len(relationLabels[n])):\n",
    "        e1 = []\n",
    "        e2 = []\n",
    "        for item in i_entityLabels[n]:\n",
    "            if ' '.join([sentWords[n][i] for i in item]) == ' '.join(em1Words[n][l]):\n",
    "                e1.append(item[0])\n",
    "            if ' '.join([sentWords[n][i] for i in item]) == ' '.join(em2Words[n][l]):\n",
    "                e2.append(item[0])\n",
    "        c1 = [(a, b) for a in e1 for b in e2 if ' '.join([sentWords[n][a], sentWords[n][b], relationLabels[n][l]]) in rmSet[n]]\n",
    "        c2 = [(a, b) for a in e2 for b in e1 if ' '.join([sentWords[n][b], sentWords[n][a], relationLabels[n][l]]) in rmSet[n]]\n",
    "        c = c1 + c2\n",
    "        m = min([(i, abs(j[0]-j[1])) for i, j in enumerate(c)], key=lambda x: x[-1])[-1]\n",
    "        r = [i[0] for i in [(i, abs(j[0]-j[1])) for i, j in enumerate(c)] if i[-1] == m]\n",
    "        for i in r:\n",
    "            if ' '.join([sentWords[n][c[i][0]], sentWords[n][c[i][1]], relationLabels[n][l]]) in rmSet[n]:\n",
    "                al = (c[i][0], pad[0], c[i][1], pad[1], relationLabels[n][l], pad[2])\n",
    "                if al[4] in t_relationTypes and al[0] < MAX_SENT_LEN and al[2] < MAX_SENT_LEN: \n",
    "                    aL.append(al)\n",
    "    aL = sorted([list(i) for i in set(aL)])\n",
    "    if len(aL) == 1 or len(aL) == 2:\n",
    "        i_adjacencyList.append(aL)\n",
    "        new_sentWords.append(sentWords[n])\n",
    "i_adjacencyList = [sum(i, []) for i in i_adjacencyList]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Adjacency List of Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean words\n",
    "t_sentWords = [[''.join([t_replaceDict.get(i, i) for i in e]) for e in eT] for eT in t_sentWords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ti_entityLabels = []\n",
    "for i in range(len(t_sentWords)):\n",
    "    eL = []\n",
    "    t_sDict = list(enumerate(t_sentWords[i]))\n",
    "    j = 0\n",
    "    for item in t_entlabWords[i]:\n",
    "        el = []\n",
    "        for e in item:\n",
    "            while j < len(t_sDict):\n",
    "                if e == t_sDict[j][1]:\n",
    "                    el.append(t_sDict[j][0])\n",
    "                    j += 1\n",
    "                    break\n",
    "                j += 1\n",
    "        eL.append(el)\n",
    "    ti_entityLabels.append(eL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ti_trueLables = [[i for i, j in enumerate(rl) if j != 'None'] for rl in t_relationLabels]\n",
    "t_trueMentions = [[j for i, j in enumerate(rl) if j['label'] != 'None'] for rl in t_relationMentions]\n",
    "pad = ['POI', 'RE', 'EOP']\n",
    "ti_adjacencyList = []\n",
    "for n in range(len(t_sentWords)): \n",
    "    e1 = []\n",
    "    e2 = []\n",
    "    aL = []\n",
    "    for l in ti_trueLables[n]:\n",
    "        for item in ti_entityLabels[n]:\n",
    "            if ' '.join([t_sentWords[n][i] for i in item]) == ' '.join(t_em1Words[n][l]):\n",
    "                e1.append(item[0])\n",
    "            if ' '.join([t_sentWords[n][i] for i in item]) == ' '.join(t_em2Words[n][l]):\n",
    "                e2.append(item[0])\n",
    "    c = [(a, b) for a in e1 for b in e2]\n",
    "    r = c[min([(i, abs(j[0]-j[1])) for i, j in enumerate(c)], key=lambda x: x[-1])[0]]\n",
    "    aL = aL + [r[0], pad[0], r[1], pad[1], t_relationLabels[n][l], pad[2]]\n",
    "    ti_adjacencyList.append(aL)\n",
    "\n",
    "# Modify \n",
    "ti_adjacencyList[26] = [34, 'POI', 36, 'RE', 'founders', 'EOP']\n",
    "ti_adjacencyList[191] = [22, 'POI', 36, 'RE', 'country', 'EOP', 33, 'POI', 36, 'RE', 'country', 'EOP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word to Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "sentWords = [[wnl.lemmatize(t.lower()) for t in toks] for toks in new_sentWords]\n",
    "t_sentWords = [[wnl.lemmatize(t.lower()) for t in toks] for toks in t_sentWords]\n",
    "tok_sentWords = sentWords.copy()\n",
    "tok_sentWords.extend(t_sentWords)\n",
    "tokTexts = [' '.join(i) for i in tok_sentWords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 65574 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(filters='')\n",
    "tokenizer.fit_on_texts(tokTexts)\n",
    "word2index = tokenizer.word_index\n",
    "index2word = {i: w for w, i in word2index.items()}\n",
    "print('Found %s unique tokens.' % len(word2index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Word Embeddings with GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 65575\n",
    "EMBEDDING_SIZE = 300\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1917495"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_n_symbols = !wc -l /Users/lizhn7/Downloads/DATA/Glove/glove.42B.300d.txt\n",
    "glove_n_symbols = int(glove_n_symbols[0].split()[0])\n",
    "glove_n_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_index_dict = {}\n",
    "glove_embedding_weights = np.empty((glove_n_symbols, EMBEDDING_SIZE))\n",
    "globale_scale = 0.1\n",
    "with open('/Users/lizhn7/Downloads/DATA/Glove/glove.42B.300d.txt', 'r') as fp:\n",
    "    index = 0\n",
    "    for l in fp:\n",
    "        l = l.strip().split()\n",
    "        word = l[0]\n",
    "        glove_index_dict[word] = index\n",
    "        glove_embedding_weights[index, :] = [float(n) for n in l[1:]]\n",
    "        index += 1\n",
    "glove_embedding_weights *= globale_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Glove to Initialize Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer, LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate random embedding with same scale as glove\n",
    "np.random.seed(SEED)\n",
    "shape = (VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "scale = glove_embedding_weights.std() * np.sqrt(12) / 2 \n",
    "embedding = np.random.uniform(low=-scale, high=scale, size=shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61085-93.15% tokens in vocab found in glove and copied to embedding.\n"
     ]
    }
   ],
   "source": [
    "# Copy from glove weights of words that appear in index2word\n",
    "count = 0 \n",
    "for i in range(1, VOCAB_SIZE):\n",
    "    w = index2word[i]\n",
    "    g = glove_index_dict.get(w)\n",
    "    if g is None:\n",
    "        w = porter.stem(w)\n",
    "        g = glove_index_dict.get(w)\n",
    "    if g is None:\n",
    "        w = lancaster.stem(w)\n",
    "        g = glove_index_dict.get(w)\n",
    "    if g is not None:\n",
    "        embedding[i, :] = glove_embedding_weights[g, :]\n",
    "        count += 1\n",
    "print('{num_tokens}-{per:.2f}% tokens in vocab found in glove and copied to embedding.'.format(num_tokens=count, per=count/float(VOCAB_SIZE)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Dateset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_sentText = [' '.join(i) for i in sentWords]\n",
    "sentSeq = tokenizer.texts_to_sequences(new_sentText)\n",
    "sentData = pad_sequences(sentSeq, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
    "\n",
    "t_new_sentText = [' '.join(i) for i in t_sentWords]\n",
    "t_sentSeq = tokenizer.texts_to_sequences(t_new_sentText)\n",
    "t_sentData = pad_sequences(t_sentSeq, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
    "\n",
    "token2index = dict((j, i+120) for i, j in enumerate(['PAD']+pad+t_relationTypes))\n",
    "token2index['PAD'] = 0\n",
    "index2token = {i: w for w, i in token2index.items()}\n",
    "newi_adjacencyList = [[token2index[i] if i in token2index else i+1 for i in aL] for aL in i_adjacencyList]\n",
    "newi_adjacencyList = pad_sequences(newi_adjacencyList, maxlen=MAX_ADJL_LEN, padding='post', truncating='post')\n",
    "newti_adjacencyList = [[token2index[i] if i in token2index else i+1 for i in aL] for aL in ti_adjacencyList]\n",
    "newti_adjacencyList = pad_sequences(newti_adjacencyList, maxlen=MAX_ADJL_LEN, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data into a training set, a validation set and a test set\n",
    "x_train_all = sentData\n",
    "y_train_all = newi_adjacencyList\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_all, y_train_all, test_size=0.15, random_state=SEED)\n",
    "x_train_all, _, y_train_all, _  = train_test_split(x_train_all, y_train_all, test_size=0., random_state=SEED)\n",
    " \n",
    "x_test = t_sentData\n",
    "y_test = newti_adjacencyList\n",
    "\n",
    "x_test, _, y_test, _ = train_test_split(x_test, y_test, test_size=0., random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fh = h5py.File('/Users/lizhn7/Downloads/DATA/nyt/experiment_4_1/allData.h5', 'w')\n",
    "fh['x_train'] = x_train\n",
    "fh['y_train'] = y_train\n",
    "fh['x_val'] = x_val\n",
    "fh['y_val'] = y_val\n",
    "fh['x_train_all'] = x_train_all\n",
    "fh['y_train_all'] = y_train_all\n",
    "fh['x_test'] = x_test\n",
    "fh['y_test'] = y_test\n",
    "fh['embedding'] = embedding\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/Users/lizhn7/Downloads/DATA/nyt/experiment_4_1/index.pkl', 'wb') as fp:\n",
    "    pickle.dump((word2index, index2word, token2index, index2token), fp, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File('/Users/lizhn7/Downloads/DATA/nyt/experiment_4_1/allData.h5', 'r') as fh:\n",
    "    x_train = fh['x_train'][:]\n",
    "    y_train = fh['y_train'][:]\n",
    "    x_val = fh['x_val'][:]\n",
    "    y_val = fh['y_val'][:]\n",
    "    x_train_all = fh['x_train_all'][:]\n",
    "    y_train_all = fh['y_train_all'][:]\n",
    "    x_test = fh['x_test'][:]\n",
    "    y_test = fh['y_test'][:]\n",
    "    embedding = fh['embedding'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/Users/lizhn7/Downloads/DATA/nyt/experiment_4_1/index.pkl', 'rb') as fp:\n",
    "    word2index, index2word, token2index, index2token = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#300-600-0.5-256-79-20-5\n",
    "MAX_SENT_LEN = 120\n",
    "MAX_ADJL_LEN = 12\n",
    "VOCAB_SIZE = len(word2index)+1\n",
    "NUM_CLASSES = 136\n",
    "EMBEDDING_SIZE = 300\n",
    "\n",
    "ENC_RNN_SIZE = 300\n",
    "DEC_RNN_SIZE = 600\n",
    "DROPOUT_RATE = 0.5\n",
    "NUM_EPOCHS = 256\n",
    "BATCH_SIZE = 79\n",
    "STEPS_PER_EPOCH = 20\n",
    "TEST_STEPS = len(x_test)//BATCH_SIZE\n",
    "\n",
    "VALIDATION_STEPS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_EPOCHS: \t\t256\n",
      "STEPS_PER_EPOCH: \t20\n",
      "TEST_STEPS: \t\t5\n",
      "VALIDATION_STEPS: \t3\n",
      "TRAIN_BATCHES: \t\t5120\n",
      "NUM_BATCHES \t\t612\n"
     ]
    }
   ],
   "source": [
    "print('NUM_EPOCHS: \\t\\t%d' % NUM_EPOCHS)\n",
    "print('STEPS_PER_EPOCH: \\t%d' % STEPS_PER_EPOCH)\n",
    "print('TEST_STEPS: \\t\\t%d' % TEST_STEPS)\n",
    "print('VALIDATION_STEPS: \\t%d' % VALIDATION_STEPS)\n",
    "print('TRAIN_BATCHES: \\t\\t%d' % (NUM_EPOCHS * STEPS_PER_EPOCH))\n",
    "print('NUM_BATCHES \\t\\t%d' % (len(x_train)//BATCH_SIZE+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, Dropout, Bidirectional, LSTM, RepeatVector, TimeDistributed, Dense\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.callbacks import*\n",
    "from keras.utils import to_categorical\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "sequence = Input(shape=(MAX_SENT_LEN,), name='INPUT') \n",
    "emb_seq = Embedding(VOCAB_SIZE, EMBEDDING_SIZE, weights=[embedding], mask_zero=True, input_length=MAX_SENT_LEN, trainable=True, name='EMBEDDING')(sequence)\n",
    "emb_seq = Dropout(DROPOUT_RATE)(emb_seq)\n",
    "blstm = Bidirectional(LSTM(ENC_RNN_SIZE, return_sequences=True, implementation=0), merge_mode='concat', name='ENC_BLSTM_1')(emb_seq)\n",
    "blstm = Dropout(DROPOUT_RATE)(blstm)\n",
    "blstm = Bidirectional(LSTM(ENC_RNN_SIZE, return_sequences=False, implementation=0), merge_mode='concat', name='ENC_BLSTM_2')(blstm)\n",
    "blstm = Dropout(DROPOUT_RATE)(blstm)\n",
    "context = RepeatVector(MAX_ADJL_LEN, name='CONTEXT')(blstm)\n",
    "lstm = LSTM(DEC_RNN_SIZE, return_sequences=True, implementation=0, name='DEC_LSTM')(context)\n",
    "lstm = Dropout(DROPOUT_RATE)(lstm)\n",
    "output = TimeDistributed(Dense(NUM_CLASSES, activation='softmax'), name='OUTPUT')(lstm)\n",
    "model = Model(inputs=sequence, outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "INPUT (InputLayer)           (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "EMBEDDING (Embedding)        (None, 120, 300)          19672500  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 120, 300)          0         \n",
      "_________________________________________________________________\n",
      "ENC_BLSTM_1 (Bidirectional)  (None, 120, 600)          1442400   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 120, 600)          0         \n",
      "_________________________________________________________________\n",
      "ENC_BLSTM_2 (Bidirectional)  (None, 600)               2162400   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "CONTEXT (RepeatVector)       (None, 12, 600)           0         \n",
      "_________________________________________________________________\n",
      "DEC_LSTM (LSTM)              (None, 12, 600)           2882400   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 12, 600)           0         \n",
      "_________________________________________________________________\n",
      "OUTPUT (TimeDistributed)     (None, 12, 136)           81736     \n",
      "=================================================================\n",
      "Total params: 26,241,436\n",
      "Trainable params: 26,241,436\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"775pt\" viewBox=\"0.00 0.00 294.26 775.00\" width=\"294pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 771)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-771 290.2588,-771 290.2588,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 5059486048 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>5059486048</title>\n",
       "<polygon fill=\"none\" points=\"80.0762,-730.5 80.0762,-766.5 206.1826,-766.5 206.1826,-730.5 80.0762,-730.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.1294\" y=\"-744.3\">INPUT: InputLayer</text>\n",
       "</g>\n",
       "<!-- 5059486944 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>5059486944</title>\n",
       "<polygon fill=\"none\" points=\"57.1655,-657.5 57.1655,-693.5 229.0933,-693.5 229.0933,-657.5 57.1655,-657.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.1294\" y=\"-671.3\">EMBEDDING: Embedding</text>\n",
       "</g>\n",
       "<!-- 5059486048&#45;&gt;5059486944 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>5059486048-&gt;5059486944</title>\n",
       "<path d=\"M143.1294,-730.4551C143.1294,-722.3828 143.1294,-712.6764 143.1294,-703.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"146.6295,-703.5903 143.1294,-693.5904 139.6295,-703.5904 146.6295,-703.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4624427888 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>4624427888</title>\n",
       "<polygon fill=\"none\" points=\"79.3276,-584.5 79.3276,-620.5 206.9312,-620.5 206.9312,-584.5 79.3276,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.1294\" y=\"-598.3\">dropout_1: Dropout</text>\n",
       "</g>\n",
       "<!-- 5059486944&#45;&gt;4624427888 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>5059486944-&gt;4624427888</title>\n",
       "<path d=\"M143.1294,-657.4551C143.1294,-649.3828 143.1294,-639.6764 143.1294,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"146.6295,-630.5903 143.1294,-620.5904 139.6295,-630.5904 146.6295,-630.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5058290128 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>5058290128</title>\n",
       "<polygon fill=\"none\" points=\"0,-511.5 0,-547.5 286.2588,-547.5 286.2588,-511.5 0,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.1294\" y=\"-525.3\">ENC_BLSTM_1(lstm_1): Bidirectional(LSTM)</text>\n",
       "</g>\n",
       "<!-- 4624427888&#45;&gt;5058290128 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>4624427888-&gt;5058290128</title>\n",
       "<path d=\"M143.1294,-584.4551C143.1294,-576.3828 143.1294,-566.6764 143.1294,-557.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"146.6295,-557.5903 143.1294,-547.5904 139.6295,-557.5904 146.6295,-557.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5058848248 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>5058848248</title>\n",
       "<polygon fill=\"none\" points=\"79.3276,-438.5 79.3276,-474.5 206.9312,-474.5 206.9312,-438.5 79.3276,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.1294\" y=\"-452.3\">dropout_2: Dropout</text>\n",
       "</g>\n",
       "<!-- 5058290128&#45;&gt;5058848248 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>5058290128-&gt;5058848248</title>\n",
       "<path d=\"M143.1294,-511.4551C143.1294,-503.3828 143.1294,-493.6764 143.1294,-484.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"146.6295,-484.5903 143.1294,-474.5904 139.6295,-484.5904 146.6295,-484.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5050471928 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>5050471928</title>\n",
       "<polygon fill=\"none\" points=\"0,-365.5 0,-401.5 286.2588,-401.5 286.2588,-365.5 0,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.1294\" y=\"-379.3\">ENC_BLSTM_2(lstm_2): Bidirectional(LSTM)</text>\n",
       "</g>\n",
       "<!-- 5058848248&#45;&gt;5050471928 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>5058848248-&gt;5050471928</title>\n",
       "<path d=\"M143.1294,-438.4551C143.1294,-430.3828 143.1294,-420.6764 143.1294,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"146.6295,-411.5903 143.1294,-401.5904 139.6295,-411.5904 146.6295,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5057485512 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>5057485512</title>\n",
       "<polygon fill=\"none\" points=\"79.3276,-292.5 79.3276,-328.5 206.9312,-328.5 206.9312,-292.5 79.3276,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.1294\" y=\"-306.3\">dropout_3: Dropout</text>\n",
       "</g>\n",
       "<!-- 5050471928&#45;&gt;5057485512 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>5050471928-&gt;5057485512</title>\n",
       "<path d=\"M143.1294,-365.4551C143.1294,-357.3828 143.1294,-347.6764 143.1294,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"146.6295,-338.5903 143.1294,-328.5904 139.6295,-338.5904 146.6295,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5050327784 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>5050327784</title>\n",
       "<polygon fill=\"none\" points=\"61.4175,-219.5 61.4175,-255.5 224.8413,-255.5 224.8413,-219.5 61.4175,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.1294\" y=\"-233.3\">CONTEXT: RepeatVector</text>\n",
       "</g>\n",
       "<!-- 5057485512&#45;&gt;5050327784 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>5057485512-&gt;5050327784</title>\n",
       "<path d=\"M143.1294,-292.4551C143.1294,-284.3828 143.1294,-274.6764 143.1294,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"146.6295,-265.5903 143.1294,-255.5904 139.6295,-265.5904 146.6295,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5046117320 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>5046117320</title>\n",
       "<polygon fill=\"none\" points=\"76.5967,-146.5 76.5967,-182.5 209.6621,-182.5 209.6621,-146.5 76.5967,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.1294\" y=\"-160.3\">DEC_LSTM: LSTM</text>\n",
       "</g>\n",
       "<!-- 5050327784&#45;&gt;5046117320 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>5050327784-&gt;5046117320</title>\n",
       "<path d=\"M143.1294,-219.4551C143.1294,-211.3828 143.1294,-201.6764 143.1294,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"146.6295,-192.5903 143.1294,-182.5904 139.6295,-192.5904 146.6295,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5046533200 -->\n",
       "<g class=\"node\" id=\"node10\">\n",
       "<title>5046533200</title>\n",
       "<polygon fill=\"none\" points=\"79.3276,-73.5 79.3276,-109.5 206.9312,-109.5 206.9312,-73.5 79.3276,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.1294\" y=\"-87.3\">dropout_4: Dropout</text>\n",
       "</g>\n",
       "<!-- 5046117320&#45;&gt;5046533200 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>5046117320-&gt;5046533200</title>\n",
       "<path d=\"M143.1294,-146.4551C143.1294,-138.3828 143.1294,-128.6764 143.1294,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"146.6295,-119.5903 143.1294,-109.5904 139.6295,-119.5904 146.6295,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5042112216 -->\n",
       "<g class=\"node\" id=\"node11\">\n",
       "<title>5042112216</title>\n",
       "<polygon fill=\"none\" points=\"8.1655,-.5 8.1655,-36.5 278.0933,-36.5 278.0933,-.5 8.1655,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"143.1294\" y=\"-14.3\">OUTPUT(dense_1): TimeDistributed(Dense)</text>\n",
       "</g>\n",
       "<!-- 5046533200&#45;&gt;5042112216 -->\n",
       "<g class=\"edge\" id=\"edge10\">\n",
       "<title>5046533200-&gt;5042112216</title>\n",
       "<path d=\"M143.1294,-73.4551C143.1294,-65.3828 143.1294,-55.6764 143.1294,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"146.6295,-46.5903 143.1294,-36.5904 139.6295,-46.5904 146.6295,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_label(s):\n",
    "    \"\"\"\n",
    "    One-hot encoding\n",
    "    \"\"\"\n",
    "    gen = to_categorical(s, num_classes=NUM_CLASSES)\n",
    "    return gen\n",
    "\n",
    "def data_generator_all(data, label, batch_size):\n",
    "    \"\"\"\n",
    "    Yield batches of all data\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    while True:\n",
    "        if count >= len(data): \n",
    "            count = 0\n",
    "        x = np.zeros((batch_size, MAX_SENT_LEN))\n",
    "        y = np.zeros((batch_size, MAX_ADJL_LEN, NUM_CLASSES))\n",
    "        for i in range(batch_size):\n",
    "            n = i + count\n",
    "            if n > len(data)-1:\n",
    "                break\n",
    "            x[i, :] = data[n]\n",
    "            y[i, :, :] = gen_label(label[n])\n",
    "        count += batch_size\n",
    "        yield (x, y)\n",
    "        \n",
    "def data_generator(data, label, batch_size): \n",
    "    \"\"\"\n",
    "    Yield batches \n",
    "    \"\"\"\n",
    "    index = np.arange(len(data))\n",
    "    np.random.shuffle(index)    \n",
    "    batches = [index[range(batch_size*i, min(len(data), batch_size*(i+1)))] for i in range(len(data)//batch_size)]\n",
    "    while True:\n",
    "        for i in batches:\n",
    "            x, y = data[i], np.array(list(map(gen_label, label[i])))\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_train_all = data_generator(x_train_all, y_train_all, BATCH_SIZE)\n",
    "gen_test = data_generator_all(x_test, y_test, BATCH_SIZE)\n",
    "gen_train = data_generator(x_train, y_train, BATCH_SIZE)\n",
    "gen_val = data_generator(x_val, y_val, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Continue Trian\n",
    "# filename = 'cp_logs/.hdf5'\n",
    "# model.load_weights(filename)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = '/Users/lizhn7/Downloads/DATA/nyt/experiment_4_1/cp_logs/weights.{epoch:03d}-{val_loss:.6f}.hdf5'\n",
    "log_string = '/Users/lizhn7/Downloads/DATA/nyt/experiment_4_1/tb_logs/300-600-0.5-256-79-20-5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "tensorboard = TensorBoard(log_dir=log_string, \n",
    "                          histogram_freq=1, \n",
    "                          write_graph=False, \n",
    "                          write_grads=False, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          write_images=True, \n",
    "                          embeddings_freq=1, \n",
    "                          embeddings_layer_names=None,\n",
    "                          embeddings_metadata=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "19/20 [===========================>..] - ETA: 8s - loss: 2.9586 Epoch 00000: val_loss improved from inf to 1.85236, saving model to /Users/lizhn7/Downloads/DATA/nyt/experiment_4_1/cp_logs/weights.000-1.852359.hdf5\n",
      "20/20 [==============================] - 195s - loss: 2.9350 - val_loss: 1.8524\n",
      "Epoch 2/256\n",
      "19/20 [===========================>..] - ETA: 8s - loss: 2.1795 Epoch 00001: val_loss improved from 1.85236 to 1.30861, saving model to /Users/lizhn7/Downloads/DATA/nyt/experiment_4_1/cp_logs/weights.001-1.308611.hdf5\n",
      "20/20 [==============================] - 183s - loss: 2.1633 - val_loss: 1.3086\n",
      "Epoch 3/256\n",
      "19/20 [===========================>..] - ETA: 8s - loss: 1.8366 Epoch 00002: val_loss improved from 1.30861 to 1.15517, saving model to /Users/lizhn7/Downloads/DATA/nyt/experiment_4_1/cp_logs/weights.002-1.155174.hdf5\n",
      "20/20 [==============================] - 185s - loss: 1.8177 - val_loss: 1.1552\n",
      "Epoch 4/256\n",
      "19/20 [===========================>..] - ETA: 8s - loss: 1.5921 Epoch 00003: val_loss improved from 1.15517 to 1.03874, saving model to /Users/lizhn7/Downloads/DATA/nyt/experiment_4_1/cp_logs/weights.003-1.038742.hdf5\n",
      "20/20 [==============================] - 180s - loss: 1.5797 - val_loss: 1.0387\n",
      "Epoch 5/256\n",
      "19/20 [===========================>..] - ETA: 8s - loss: 1.4314 Epoch 00004: val_loss did not improve\n",
      "20/20 [==============================] - 184s - loss: 1.4300 - val_loss: 1.0675\n",
      "Epoch 6/256\n",
      "19/20 [===========================>..] - ETA: 8s - loss: 1.9581 Epoch 00005: val_loss improved from 1.03874 to 1.02671, saving model to /Users/lizhn7/Downloads/DATA/nyt/experiment_4_1/cp_logs/weights.005-1.026710.hdf5\n",
      "20/20 [==============================] - 181s - loss: 1.9255 - val_loss: 1.0267\n",
      "Epoch 7/256\n",
      "19/20 [===========================>..] - ETA: 8s - loss: 1.3105 Epoch 00006: val_loss improved from 1.02671 to 0.99667, saving model to /Users/lizhn7/Downloads/DATA/nyt/experiment_4_1/cp_logs/weights.006-0.996666.hdf5\n",
      "20/20 [==============================] - 185s - loss: 1.3069 - val_loss: 0.9967\n",
      "Epoch 8/256\n",
      "19/20 [===========================>..] - ETA: 8s - loss: 1.2898 Epoch 00007: val_loss did not improve\n",
      "20/20 [==============================] - 185s - loss: 1.2937 - val_loss: 1.0191\n",
      "Epoch 9/256\n",
      "19/20 [===========================>..] - ETA: 8s - loss: 1.2579 Epoch 00008: val_loss improved from 0.99667 to 0.92076, saving model to /Users/lizhn7/Downloads/DATA/nyt/experiment_4_1/cp_logs/weights.008-0.920761.hdf5\n",
      "20/20 [==============================] - 184s - loss: 1.2543 - val_loss: 0.9208\n",
      "Epoch 10/256\n",
      "19/20 [===========================>..] - ETA: 8s - loss: 1.2620 Epoch 00009: val_loss did not improve\n",
      "20/20 [==============================] - 191s - loss: 1.2675 - val_loss: 0.9371\n",
      "Epoch 11/256\n",
      "19/20 [===========================>..] - ETA: 8s - loss: 1.2263 Epoch 00010: val_loss did not improve\n",
      "20/20 [==============================] - 186s - loss: 1.2208 - val_loss: 0.9284\n",
      "Epoch 12/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 1.1906 Epoch 00011: val_loss did not improve\n",
      "20/20 [==============================] - 179s - loss: 1.1984 - val_loss: 1.0569\n",
      "Epoch 13/256\n",
      "19/20 [===========================>..] - ETA: 8s - loss: 1.2290 Epoch 00012: val_loss did not improve\n",
      "20/20 [==============================] - 198s - loss: 1.2328 - val_loss: 1.0658\n",
      "Epoch 14/256\n",
      "19/20 [===========================>..] - ETA: 8s - loss: 1.1685 Epoch 00013: val_loss did not improve\n",
      "20/20 [==============================] - 189s - loss: 1.1636 - val_loss: 0.9617\n",
      "Epoch 15/256\n",
      "19/20 [===========================>..] - ETA: 8s - loss: 1.1958 Epoch 00014: val_loss improved from 0.92076 to 0.91011, saving model to /Users/lizhn7/Downloads/DATA/nyt/experiment_4_1/cp_logs/weights.014-0.910107.hdf5\n",
      "20/20 [==============================] - 192s - loss: 1.1895 - val_loss: 0.9101\n",
      "Epoch 16/256\n",
      "19/20 [===========================>..] - ETA: 8s - loss: 1.1738 Epoch 00015: val_loss did not improve\n",
      "20/20 [==============================] - 184s - loss: 1.1737 - val_loss: 0.9711\n",
      "Epoch 17/256\n",
      "19/20 [===========================>..] - ETA: 8s - loss: 1.1275 Epoch 00016: val_loss improved from 0.91011 to 0.86877, saving model to /Users/lizhn7/Downloads/DATA/nyt/experiment_4_1/cp_logs/weights.016-0.868772.hdf5\n",
      "20/20 [==============================] - 184s - loss: 1.1309 - val_loss: 0.8688\n",
      "Epoch 18/256\n",
      "19/20 [===========================>..] - ETA: 8s - loss: 1.1305 Epoch 00017: val_loss did not improve\n",
      "20/20 [==============================] - 185s - loss: 1.1309 - val_loss: 0.9483\n",
      "Epoch 19/256\n",
      "19/20 [===========================>..] - ETA: 8s - loss: 1.0733 Epoch 00018: val_loss did not improve\n",
      "20/20 [==============================] - 186s - loss: 1.0688 - val_loss: 0.8707\n",
      "Epoch 20/256\n",
      "19/20 [===========================>..] - ETA: 8s - loss: 1.0996 Epoch 00019: val_loss did not improve\n",
      "20/20 [==============================] - 184s - loss: 1.0960 - val_loss: 0.8784\n",
      "Epoch 21/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 1.0837 Epoch 00020: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 1.0864 - val_loss: 0.9113\n",
      "Epoch 22/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 1.0735 Epoch 00021: val_loss improved from 0.86877 to 0.84919, saving model to /Users/lizhn7/Downloads/DATA/nyt/experiment_4_1/cp_logs/weights.021-0.849194.hdf5\n",
      "20/20 [==============================] - 177s - loss: 1.0716 - val_loss: 0.8492\n",
      "Epoch 23/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 1.0448 Epoch 00022: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 1.0411 - val_loss: 0.8530\n",
      "Epoch 24/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 1.0478 Epoch 00023: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 1.0424 - val_loss: 0.8915\n",
      "Epoch 25/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 1.0258 Epoch 00024: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 1.0252 - val_loss: 0.8647\n",
      "Epoch 26/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.9891 Epoch 00025: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.9906 - val_loss: 0.8833\n",
      "Epoch 27/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.9822 Epoch 00026: val_loss improved from 0.84919 to 0.80739, saving model to /Users/lizhn7/Downloads/DATA/nyt/experiment_4_1/cp_logs/weights.026-0.807387.hdf5\n",
      "20/20 [==============================] - 176s - loss: 0.9779 - val_loss: 0.8074\n",
      "Epoch 28/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 1.0037 Epoch 00027: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 1.0079 - val_loss: 0.8834\n",
      "Epoch 29/256\n",
      "19/20 [===========================>..] - ETA: 8s - loss: 0.9799 Epoch 00028: val_loss did not improve\n",
      "20/20 [==============================] - 184s - loss: 0.9826 - val_loss: 0.8251\n",
      "Epoch 30/256\n",
      "19/20 [===========================>..] - ETA: 8s - loss: 0.9738 Epoch 00029: val_loss did not improve\n",
      "20/20 [==============================] - 184s - loss: 0.9728 - val_loss: 0.8635\n",
      "Epoch 31/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.9988 Epoch 00030: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 1.0009 - val_loss: 0.9548\n",
      "Epoch 32/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.9385 Epoch 00031: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 0.9381 - val_loss: 0.8380\n",
      "Epoch 33/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.9407 Epoch 00032: val_loss improved from 0.80739 to 0.78129, saving model to /Users/lizhn7/Downloads/DATA/nyt/experiment_4_1/cp_logs/weights.032-0.781289.hdf5\n",
      "20/20 [==============================] - 177s - loss: 0.9364 - val_loss: 0.7813\n",
      "Epoch 34/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.9415 Epoch 00033: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 0.9417 - val_loss: 0.8114\n",
      "Epoch 35/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.9297 Epoch 00034: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 177s - loss: 0.9292 - val_loss: 0.7845\n",
      "Epoch 36/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.9006 Epoch 00035: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 0.8991 - val_loss: 0.7862\n",
      "Epoch 37/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.8717 Epoch 00036: val_loss did not improve\n",
      "20/20 [==============================] - 179s - loss: 0.8752 - val_loss: 0.8883\n",
      "Epoch 38/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.9082 Epoch 00037: val_loss improved from 0.78129 to 0.78114, saving model to /Users/lizhn7/Downloads/DATA/nyt/experiment_4_1/cp_logs/weights.037-0.781139.hdf5\n",
      "20/20 [==============================] - 177s - loss: 0.9088 - val_loss: 0.7811\n",
      "Epoch 39/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.8891 Epoch 00038: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 0.8896 - val_loss: 0.9027\n",
      "Epoch 40/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.8795 Epoch 00039: val_loss did not improve\n",
      "20/20 [==============================] - 175s - loss: 0.8775 - val_loss: 0.8255\n",
      "Epoch 41/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.8919 Epoch 00040: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.8960 - val_loss: 0.8950\n",
      "Epoch 42/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.8505 Epoch 00041: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 0.8519 - val_loss: 0.8781\n",
      "Epoch 43/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.8773 Epoch 00042: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.8804 - val_loss: 0.8725\n",
      "Epoch 44/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.8723 Epoch 00043: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.8720 - val_loss: 0.7871\n",
      "Epoch 45/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.8493 Epoch 00044: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.8542 - val_loss: 0.8198\n",
      "Epoch 46/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.8475 Epoch 00045: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 0.8467 - val_loss: 0.8120\n",
      "Epoch 47/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.8461 Epoch 00046: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 0.8436 - val_loss: 0.7986\n",
      "Epoch 48/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.8268 Epoch 00047: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.8284 - val_loss: 0.8109\n",
      "Epoch 49/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.8311 Epoch 00048: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.8337 - val_loss: 0.8983\n",
      "Epoch 50/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.8151 Epoch 00049: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.8139 - val_loss: 0.8766\n",
      "Epoch 51/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.8528 Epoch 00050: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.8537 - val_loss: 0.8021\n",
      "Epoch 52/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.8403 Epoch 00051: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 0.8364 - val_loss: 0.8168\n",
      "Epoch 53/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.8043 Epoch 00052: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.8011 - val_loss: 0.7977\n",
      "Epoch 54/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.8134 Epoch 00053: val_loss improved from 0.78114 to 0.76924, saving model to /Users/lizhn7/Downloads/DATA/nyt/experiment_4_1/cp_logs/weights.053-0.769240.hdf5\n",
      "20/20 [==============================] - 177s - loss: 0.8126 - val_loss: 0.7692\n",
      "Epoch 55/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.7890 Epoch 00054: val_loss did not improve\n",
      "20/20 [==============================] - 175s - loss: 0.7968 - val_loss: 0.8417\n",
      "Epoch 56/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.8114 Epoch 00055: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.8113 - val_loss: 0.8507\n",
      "Epoch 57/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.8224 Epoch 00056: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 0.8263 - val_loss: 0.9072\n",
      "Epoch 58/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.8140 Epoch 00057: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.8093 - val_loss: 0.8067\n",
      "Epoch 59/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.8086 Epoch 00058: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.8101 - val_loss: 0.8118\n",
      "Epoch 60/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.8172 Epoch 00059: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 0.8193 - val_loss: 0.7981\n",
      "Epoch 61/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.7947 Epoch 00060: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.7979 - val_loss: 0.8121\n",
      "Epoch 62/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.7777 Epoch 00061: val_loss did not improve\n",
      "20/20 [==============================] - 174s - loss: 0.7739 - val_loss: 0.8118\n",
      "Epoch 63/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.7902 Epoch 00062: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.7903 - val_loss: 0.8446\n",
      "Epoch 64/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.7973 Epoch 00063: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 0.7999 - val_loss: 0.8644\n",
      "Epoch 65/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.7754 Epoch 00064: val_loss improved from 0.76924 to 0.75875, saving model to /Users/lizhn7/Downloads/DATA/nyt/experiment_4_1/cp_logs/weights.064-0.758749.hdf5\n",
      "20/20 [==============================] - 177s - loss: 0.7739 - val_loss: 0.7587\n",
      "Epoch 66/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.7749 Epoch 00065: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.7774 - val_loss: 0.7978\n",
      "Epoch 67/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.8038 Epoch 00066: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.8025 - val_loss: 0.8808\n",
      "Epoch 68/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.7652 Epoch 00067: val_loss improved from 0.75875 to 0.75799, saving model to /Users/lizhn7/Downloads/DATA/nyt/experiment_4_1/cp_logs/weights.067-0.757986.hdf5\n",
      "20/20 [==============================] - 177s - loss: 0.7583 - val_loss: 0.7580\n",
      "Epoch 69/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.7685 Epoch 00068: val_loss improved from 0.75799 to 0.74750, saving model to /Users/lizhn7/Downloads/DATA/nyt/experiment_4_1/cp_logs/weights.068-0.747501.hdf5\n",
      "20/20 [==============================] - 176s - loss: 0.7663 - val_loss: 0.7475\n",
      "Epoch 70/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.7636 Epoch 00069: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.7658 - val_loss: 0.8044\n",
      "Epoch 71/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.7626 Epoch 00070: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.7655 - val_loss: 0.7677\n",
      "Epoch 72/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.7346 Epoch 00071: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.7361 - val_loss: 0.8630\n",
      "Epoch 73/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.7078 Epoch 00072: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.7094 - val_loss: 0.7996\n",
      "Epoch 74/256\n",
      "19/20 [===========================>..] - ETA: 8s - loss: 0.7449 Epoch 00073: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 180s - loss: 0.7424 - val_loss: 0.8002\n",
      "Epoch 75/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.7301 Epoch 00074: val_loss did not improve\n",
      "20/20 [==============================] - 175s - loss: 0.7337 - val_loss: 0.8324\n",
      "Epoch 76/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.7110 Epoch 00075: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.7175 - val_loss: 0.8776\n",
      "Epoch 77/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.7088 Epoch 00076: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 0.7105 - val_loss: 0.7868\n",
      "Epoch 78/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.7084 Epoch 00077: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.7071 - val_loss: 0.8078\n",
      "Epoch 79/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.7284 Epoch 00078: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 0.7259 - val_loss: 0.7547\n",
      "Epoch 80/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.7271 Epoch 00079: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 0.7261 - val_loss: 0.8309\n",
      "Epoch 81/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.6988 Epoch 00080: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 0.7017 - val_loss: 0.8013\n",
      "Epoch 82/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.6941 Epoch 00081: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.6975 - val_loss: 0.8246\n",
      "Epoch 83/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.7071 Epoch 00082: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.7020 - val_loss: 0.8038\n",
      "Epoch 84/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.6899 Epoch 00083: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 0.6869 - val_loss: 0.8770\n",
      "Epoch 85/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.6840 Epoch 00084: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.6823 - val_loss: 0.8847\n",
      "Epoch 86/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.6757 Epoch 00085: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.6772 - val_loss: 0.8157\n",
      "Epoch 87/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.6988 Epoch 00086: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.7019 - val_loss: 0.8685\n",
      "Epoch 88/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.6817 Epoch 00087: val_loss improved from 0.74750 to 0.74185, saving model to /Users/lizhn7/Downloads/DATA/nyt/experiment_4_1/cp_logs/weights.087-0.741848.hdf5\n",
      "20/20 [==============================] - 176s - loss: 0.6794 - val_loss: 0.7418\n",
      "Epoch 89/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.6655 Epoch 00088: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.6691 - val_loss: 0.8305\n",
      "Epoch 90/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.6667 Epoch 00089: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.6655 - val_loss: 0.7945\n",
      "Epoch 91/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.6670 Epoch 00090: val_loss did not improve\n",
      "20/20 [==============================] - 175s - loss: 0.6688 - val_loss: 0.8447\n",
      "Epoch 92/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.6758 Epoch 00091: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.6793 - val_loss: 0.8975\n",
      "Epoch 93/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.6905 Epoch 00092: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.6914 - val_loss: 0.8321\n",
      "Epoch 94/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.6606 Epoch 00093: val_loss did not improve\n",
      "20/20 [==============================] - 175s - loss: 0.6617 - val_loss: 0.8785\n",
      "Epoch 95/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.6750 Epoch 00094: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.6715 - val_loss: 0.7633\n",
      "Epoch 96/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.6851 Epoch 00095: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.6889 - val_loss: 0.8378\n",
      "Epoch 97/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.6640 Epoch 00096: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.6656 - val_loss: 0.8689\n",
      "Epoch 98/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.6339 Epoch 00097: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.6316 - val_loss: 0.8219\n",
      "Epoch 99/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.6552 Epoch 00098: val_loss did not improve\n",
      "20/20 [==============================] - 175s - loss: 0.6583 - val_loss: 0.8169\n",
      "Epoch 100/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.6781 Epoch 00099: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 0.6829 - val_loss: 0.8108\n",
      "Epoch 101/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.6400 Epoch 00100: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 0.6422 - val_loss: 0.8013\n",
      "Epoch 102/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.6411 Epoch 00101: val_loss did not improve\n",
      "20/20 [==============================] - 174s - loss: 0.6414 - val_loss: 0.7776\n",
      "Epoch 103/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.6733 Epoch 00102: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.6712 - val_loss: 0.8439\n",
      "Epoch 104/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.6327 Epoch 00103: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.6351 - val_loss: 0.8228\n",
      "Epoch 105/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.6357 Epoch 00104: val_loss improved from 0.74185 to 0.73860, saving model to /Users/lizhn7/Downloads/DATA/nyt/experiment_4_1/cp_logs/weights.104-0.738603.hdf5\n",
      "20/20 [==============================] - 177s - loss: 0.6442 - val_loss: 0.7386\n",
      "Epoch 106/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.6380 Epoch 00105: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.6438 - val_loss: 0.8088\n",
      "Epoch 107/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.6385 Epoch 00106: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.6404 - val_loss: 0.8095\n",
      "Epoch 108/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.6045 Epoch 00107: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.6043 - val_loss: 0.7878\n",
      "Epoch 109/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5934 Epoch 00108: val_loss did not improve\n",
      "20/20 [==============================] - 175s - loss: 0.5971 - val_loss: 0.8451\n",
      "Epoch 110/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.6217 Epoch 00109: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.6189 - val_loss: 0.8325\n",
      "Epoch 111/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.6205 Epoch 00110: val_loss did not improve\n",
      "20/20 [==============================] - 175s - loss: 0.6182 - val_loss: 0.8321\n",
      "Epoch 112/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.6067 Epoch 00111: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 0.6069 - val_loss: 0.8165\n",
      "Epoch 113/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5956 Epoch 00112: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.5979 - val_loss: 0.8005\n",
      "Epoch 114/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5881 Epoch 00113: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.5895 - val_loss: 0.8554\n",
      "Epoch 115/256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/20 [===========================>..] - ETA: 7s - loss: 0.6168 Epoch 00114: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.6102 - val_loss: 0.7623\n",
      "Epoch 116/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.6102 Epoch 00115: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 0.6099 - val_loss: 0.8583\n",
      "Epoch 117/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5823 Epoch 00116: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.5829 - val_loss: 0.7719\n",
      "Epoch 118/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5926 Epoch 00117: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.5921 - val_loss: 0.8781\n",
      "Epoch 119/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5904 Epoch 00118: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 0.5846 - val_loss: 0.8300\n",
      "Epoch 120/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5912 Epoch 00119: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 0.5910 - val_loss: 0.8819\n",
      "Epoch 121/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5817 Epoch 00120: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.5811 - val_loss: 0.8765\n",
      "Epoch 122/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5721 Epoch 00121: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.5701 - val_loss: 0.9080\n",
      "Epoch 123/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.6082 Epoch 00122: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.6044 - val_loss: 0.8758\n",
      "Epoch 124/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5794 Epoch 00123: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.5831 - val_loss: 0.8385\n",
      "Epoch 125/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5649 Epoch 00124: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 0.5665 - val_loss: 0.8244\n",
      "Epoch 126/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5695 Epoch 00125: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.5764 - val_loss: 0.8316\n",
      "Epoch 127/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5715 Epoch 00126: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.5691 - val_loss: 0.8120\n",
      "Epoch 128/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5841 Epoch 00127: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.5857 - val_loss: 0.7829\n",
      "Epoch 129/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5893 Epoch 00128: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.5912 - val_loss: 0.8697\n",
      "Epoch 130/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5581 Epoch 00129: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 0.5530 - val_loss: 0.8752\n",
      "Epoch 131/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5725 Epoch 00130: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.5752 - val_loss: 0.8583\n",
      "Epoch 132/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5872 Epoch 00131: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.5882 - val_loss: 0.9111\n",
      "Epoch 133/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5635 Epoch 00132: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.5625 - val_loss: 0.8822\n",
      "Epoch 134/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5261 Epoch 00133: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 0.5227 - val_loss: 0.8590\n",
      "Epoch 135/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5612 Epoch 00134: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.5651 - val_loss: 0.8614\n",
      "Epoch 136/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5841 Epoch 00135: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.5857 - val_loss: 0.8413\n",
      "Epoch 137/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5365 Epoch 00136: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.5411 - val_loss: 0.8976\n",
      "Epoch 138/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5454 Epoch 00137: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.5511 - val_loss: 0.9222\n",
      "Epoch 139/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5569 Epoch 00138: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.5556 - val_loss: 0.8285\n",
      "Epoch 140/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5408 Epoch 00139: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.5399 - val_loss: 0.8571\n",
      "Epoch 141/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5593 Epoch 00140: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.5564 - val_loss: 0.7589\n",
      "Epoch 142/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5581 Epoch 00141: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.5557 - val_loss: 0.8335\n",
      "Epoch 143/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5519 Epoch 00142: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.5504 - val_loss: 0.8496\n",
      "Epoch 144/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5148 Epoch 00143: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.5136 - val_loss: 0.8614\n",
      "Epoch 145/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4956 Epoch 00144: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.4956 - val_loss: 0.8706\n",
      "Epoch 146/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5438 Epoch 00145: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.5416 - val_loss: 0.9081\n",
      "Epoch 147/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5282 Epoch 00146: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.5289 - val_loss: 0.9095\n",
      "Epoch 148/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5069 Epoch 00147: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.5043 - val_loss: 0.8598\n",
      "Epoch 149/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5095 Epoch 00148: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 0.5074 - val_loss: 0.8886\n",
      "Epoch 150/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5067 Epoch 00149: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.5094 - val_loss: 0.8950\n",
      "Epoch 151/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5237 Epoch 00150: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.5260 - val_loss: 0.8485\n",
      "Epoch 152/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5163 Epoch 00151: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.5153 - val_loss: 0.8410\n",
      "Epoch 153/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5056 Epoch 00152: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.5115 - val_loss: 0.8570\n",
      "Epoch 154/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5027 Epoch 00153: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.5045 - val_loss: 0.9234\n",
      "Epoch 155/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4896 Epoch 00154: val_loss did not improve\n",
      "20/20 [==============================] - 175s - loss: 0.4906 - val_loss: 0.9318\n",
      "Epoch 156/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5099 Epoch 00155: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 178s - loss: 0.5055 - val_loss: 1.0486\n",
      "Epoch 157/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5058 Epoch 00156: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.5013 - val_loss: 0.9103\n",
      "Epoch 158/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4918 Epoch 00157: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.4916 - val_loss: 0.9684\n",
      "Epoch 159/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5236 Epoch 00158: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.5203 - val_loss: 0.9793\n",
      "Epoch 160/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5132 Epoch 00159: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.5129 - val_loss: 0.8122\n",
      "Epoch 161/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4837 Epoch 00160: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.4841 - val_loss: 0.8491\n",
      "Epoch 162/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4953 Epoch 00161: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.4897 - val_loss: 0.8154\n",
      "Epoch 163/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4884 Epoch 00162: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.4853 - val_loss: 0.8005\n",
      "Epoch 164/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5227 Epoch 00163: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 0.5274 - val_loss: 0.8693\n",
      "Epoch 165/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5105 Epoch 00164: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.5044 - val_loss: 0.9392\n",
      "Epoch 166/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4647 Epoch 00165: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.4679 - val_loss: 0.9460\n",
      "Epoch 167/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5014 Epoch 00166: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.4992 - val_loss: 0.9830\n",
      "Epoch 168/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5056 Epoch 00167: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.5027 - val_loss: 0.9502\n",
      "Epoch 169/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4763 Epoch 00168: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.4833 - val_loss: 0.9861\n",
      "Epoch 170/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4508 Epoch 00169: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.4477 - val_loss: 0.9783\n",
      "Epoch 171/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5012 Epoch 00170: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.4998 - val_loss: 0.9258\n",
      "Epoch 172/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5167 Epoch 00171: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 0.5167 - val_loss: 0.8905\n",
      "Epoch 173/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4707 Epoch 00172: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 0.4727 - val_loss: 0.8890\n",
      "Epoch 174/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4665 Epoch 00173: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.4663 - val_loss: 0.8956\n",
      "Epoch 175/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.5012 Epoch 00174: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.4985 - val_loss: 0.9153\n",
      "Epoch 176/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4707 Epoch 00175: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.4735 - val_loss: 0.9243\n",
      "Epoch 177/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4758 Epoch 00176: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.4779 - val_loss: 0.8235\n",
      "Epoch 178/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4802 Epoch 00177: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.4810 - val_loss: 0.8879\n",
      "Epoch 179/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4851 Epoch 00178: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.4804 - val_loss: 0.8867\n",
      "Epoch 180/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4467 Epoch 00179: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.4407 - val_loss: 0.9143\n",
      "Epoch 181/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4389 Epoch 00180: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.4430 - val_loss: 0.9555\n",
      "Epoch 182/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4587 Epoch 00181: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.4563 - val_loss: 0.9844\n",
      "Epoch 183/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4484 Epoch 00182: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.4465 - val_loss: 0.9832\n",
      "Epoch 184/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4446 Epoch 00183: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.4429 - val_loss: 0.9582\n",
      "Epoch 185/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4314 Epoch 00184: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 0.4339 - val_loss: 0.9245\n",
      "Epoch 186/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4335 Epoch 00185: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.4369 - val_loss: 0.9432\n",
      "Epoch 187/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4564 Epoch 00186: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 0.4512 - val_loss: 0.9156\n",
      "Epoch 188/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4481 Epoch 00187: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.4459 - val_loss: 0.8957\n",
      "Epoch 189/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4373 Epoch 00188: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.4379 - val_loss: 0.8629\n",
      "Epoch 190/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4349 Epoch 00189: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 0.4378 - val_loss: 0.9514\n",
      "Epoch 191/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4213 Epoch 00190: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.4175 - val_loss: 1.0301\n",
      "Epoch 192/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4493 Epoch 00191: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.4509 - val_loss: 0.9709\n",
      "Epoch 193/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4367 Epoch 00192: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.4407 - val_loss: 0.9660\n",
      "Epoch 194/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4237 Epoch 00193: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.4221 - val_loss: 0.9992\n",
      "Epoch 195/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4483 Epoch 00194: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.4483 - val_loss: 0.9259\n",
      "Epoch 196/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4460 Epoch 00195: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.4420 - val_loss: 0.8195\n",
      "Epoch 197/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4265 Epoch 00196: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.4237 - val_loss: 1.0003\n",
      "Epoch 198/256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4235 Epoch 00197: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.4217 - val_loss: 0.9120\n",
      "Epoch 199/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4272 Epoch 00198: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.4267 - val_loss: 0.9058\n",
      "Epoch 200/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4586 Epoch 00199: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.4509 - val_loss: 0.8813\n",
      "Epoch 201/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4366 Epoch 00200: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.4346 - val_loss: 0.9416\n",
      "Epoch 202/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4182 Epoch 00201: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.4118 - val_loss: 0.8887\n",
      "Epoch 203/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4502 Epoch 00202: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.4488 - val_loss: 0.8955\n",
      "Epoch 204/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4479 Epoch 00203: val_loss did not improve\n",
      "20/20 [==============================] - 176s - loss: 0.4452 - val_loss: 0.9213\n",
      "Epoch 205/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4291 Epoch 00204: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.4234 - val_loss: 1.0473\n",
      "Epoch 206/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.3888 Epoch 00205: val_loss did not improve\n",
      "20/20 [==============================] - 177s - loss: 0.3918 - val_loss: 0.9445\n",
      "Epoch 207/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4400 Epoch 00206: val_loss did not improve\n",
      "20/20 [==============================] - 179s - loss: 0.4403 - val_loss: 0.9761\n",
      "Epoch 208/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4386 Epoch 00207: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.4399 - val_loss: 0.9652\n",
      "Epoch 209/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4083 Epoch 00208: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.4090 - val_loss: 0.9981\n",
      "Epoch 210/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4125 Epoch 00209: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.4129 - val_loss: 0.9925\n",
      "Epoch 211/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4308 Epoch 00210: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.4253 - val_loss: 0.8902\n",
      "Epoch 212/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4224 Epoch 00211: val_loss did not improve\n",
      "20/20 [==============================] - 179s - loss: 0.4238 - val_loss: 0.9456\n",
      "Epoch 213/256\n",
      "19/20 [===========================>..] - ETA: 8s - loss: 0.4209 Epoch 00212: val_loss did not improve\n",
      "20/20 [==============================] - 180s - loss: 0.4201 - val_loss: 0.9010\n",
      "Epoch 214/256\n",
      "19/20 [===========================>..] - ETA: 7s - loss: 0.4213 Epoch 00213: val_loss did not improve\n",
      "20/20 [==============================] - 178s - loss: 0.4260 - val_loss: 0.9443\n",
      "Epoch 215/256\n",
      "19/20 [===========================>..] - ETA: 8s - loss: 0.4160 "
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(gen_train_all, \n",
    "                              steps_per_epoch=STEPS_PER_EPOCH, \n",
    "                              epochs=NUM_EPOCHS, \n",
    "                              verbose=1,\n",
    "                              callbacks=[checkpoint, tensorboard],\n",
    "                              validation_data=gen_test, \n",
    "                              validation_steps=TEST_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = '/Users/lizhn7/Downloads/DATA/nyt/experiment_3_2/cp_logs/allLSTM_w2v/weights.050-0.592349.hdf5'\n",
    "model.load_weights(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395/395 [==============================] - 13s    \n"
     ]
    }
   ],
   "source": [
    "result = np.argmax(model.predict(x_test, batch_size=BATCH_SIZE, verbose=1), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict: \t9 POI 9 RE contains EOP\n",
      "Ground-Truth: \t9 POI 10 RE country EOP\n",
      "---\n",
      "Predict: \t2 POI 15 RE contains EOP\n",
      "Ground-Truth: \t16 POI 14 RE country EOP\n",
      "---\n",
      "Predict: \t10 POI 12 RE contains POI RE EOP\n",
      "Ground-Truth: \t11 POI 13 RE nationality EOP\n",
      "---\n",
      "Predict: \t13 POI 12 RE contains EOP\n",
      "Ground-Truth: \t14 POI 9 RE contains EOP\n",
      "---\n",
      "Predict: \t13 POI 14 RE place_lived EOP\n",
      "Ground-Truth: \t12 POI 15 RE place_lived EOP\n",
      "---\n",
      "Predict: \t9 POI 8 RE contains EOP\n",
      "Ground-Truth: \t10 POI 9 RE contains EOP\n",
      "---\n",
      "Predict: \t1 POI 8 RE company EOP\n",
      "Ground-Truth: \t8 POI 1 RE founders EOP\n",
      "---\n",
      "Predict: \t13 POI 15 RE contains EOP\n",
      "Ground-Truth: \t20 POI 16 RE country EOP\n",
      "---\n",
      "Predict: \t9 POI 32 RE contains EOP\n",
      "Ground-Truth: \t19 POI 13 RE country POI 13 RE country EOP 19 POI 13 RE country POI 13 RE country EOP\n",
      "---\n",
      "Predict: \t6 POI 6 RE contains EOP RE EOP\n",
      "Ground-Truth: \t17 POI 6 RE contains EOP\n",
      "---\n",
      "Predict: \t12 POI 10 RE contains EOP\n",
      "Ground-Truth: \t26 POI 29 RE company EOP\n",
      "---\n",
      "Predict: \t12 POI 12 RE contains EOP\n",
      "Ground-Truth: \t12 POI 11 RE contains EOP\n",
      "---\n",
      "Predict: \t2 POI 2 RE contains EOP\n",
      "Ground-Truth: \t2 POI 7 RE contains EOP\n",
      "---\n",
      "Predict: \t14 POI 17 RE place_lived EOP\n",
      "Ground-Truth: \t15 POI 18 RE place_lived EOP\n",
      "---\n",
      "Predict: \t8 POI 8 RE contains EOP\n",
      "Ground-Truth: \t3 POI 6 RE place_lived EOP\n",
      "---\n",
      "Predict: \t4 POI 4 RE contains POI RE EOP\n",
      "Ground-Truth: \t4 POI 16 RE country EOP\n",
      "---\n",
      "Predict: \t18 POI 20 RE company EOP\n",
      "Ground-Truth: \t23 POI 26 RE company EOP\n",
      "---\n",
      "Predict: \t6 POI 4 RE contains EOP\n",
      "Ground-Truth: \t4 POI 6 RE country EOP\n",
      "---\n",
      "Predict: \t18 POI 18 RE country EOP POI RE POI RE RE EOP\n",
      "Ground-Truth: \t20 POI 21 RE country EOP\n",
      "---\n",
      "Predict: \t29 POI 32 RE place_lived EOP\n",
      "Ground-Truth: \t28 POI 37 RE company EOP\n",
      "---\n",
      "Predict: \t1 POI 1 RE contains EOP\n",
      "Ground-Truth: \t13 POI 16 RE nationality EOP\n",
      "---\n",
      "Predict: \t12 POI 12 RE company EOP\n",
      "Ground-Truth: \t9 POI 15 RE company EOP\n",
      "---\n",
      "Predict: \t14 POI 14 RE company EOP\n",
      "Ground-Truth: \t20 POI 12 RE founders EOP\n",
      "---\n",
      "Predict: \t2 POI 2 RE place_lived EOP\n",
      "Ground-Truth: \t2 POI 5 RE place_lived EOP\n",
      "---\n",
      "Predict: \t23 POI 23 RE place_lived EOP\n",
      "Ground-Truth: \t20 POI 31 RE contains EOP\n",
      "---\n",
      "Predict: \t13 POI 14 RE contains EOP\n",
      "Ground-Truth: \t18 POI 13 RE country EOP\n",
      "---\n",
      "Predict: \t12 POI 12 RE contains EOP\n",
      "Ground-Truth: \t14 POI 34 RE country EOP\n",
      "---\n",
      "Predict: \t9 POI 12 RE company EOP\n",
      "Ground-Truth: \t8 POI 15 RE place_lived EOP\n",
      "---\n",
      "Predict: \t15 POI 15 RE contains EOP\n",
      "Ground-Truth: \t6 POI 15 RE company EOP\n",
      "---\n",
      "Predict: \t1 POI 1 RE contains EOP\n",
      "Ground-Truth: \t29 POI 1 RE country EOP\n",
      "---\n",
      "Predict: \t18 POI 18 RE contains EOP\n",
      "Ground-Truth: \t18 POI 16 RE contains EOP\n",
      "---\n",
      "Predict: \t18 POI 18 RE nationality EOP\n",
      "Ground-Truth: \t21 POI 17 RE nationality EOP\n",
      "---\n",
      "Predict: \t23 POI 29 RE contains EOP\n",
      "Ground-Truth: \t30 POI 34 RE country EOP\n",
      "---\n",
      "Predict: \t12 POI 12 RE company EOP\n",
      "Ground-Truth: \t11 POI 17 RE company EOP\n",
      "---\n",
      "Predict: \t23 POI 23 RE contains EOP\n",
      "Ground-Truth: \t26 POI 24 RE contains EOP\n",
      "---\n",
      "Predict: \t8 POI 44 RE contains EOP\n",
      "Ground-Truth: \t38 POI 43 RE company EOP\n",
      "---\n",
      "Predict: \t1 POI 8 RE company EOP\n",
      "Ground-Truth: \t8 POI 15 RE company EOP\n",
      "---\n",
      "Predict: \t32 POI 32 RE contains EOP\n",
      "Ground-Truth: \t14 POI 12 RE contains EOP\n",
      "---\n",
      "Predict: \t18 POI 18 RE contains EOP\n",
      "Ground-Truth: \t29 POI 34 RE contains POI 34 RE contains EOP 29 POI 34 RE contains POI 34 RE contains EOP\n",
      "---\n",
      "Predict: \t12 POI 12 RE country EOP RE capital EOP POI POI RE\n",
      "Ground-Truth: \t15 POI 14 RE contains EOP\n",
      "---\n",
      "Predict: \t26 POI 32 RE contains EOP\n",
      "Ground-Truth: \t29 POI 31 RE country EOP\n",
      "---\n",
      "Predict: \t2 POI 8 RE place_lived EOP\n",
      "Ground-Truth: \t3 POI 6 RE place_lived EOP\n",
      "---\n",
      "Predict: \t18 POI 18 RE contains EOP\n",
      "Ground-Truth: \t19 POI 21 RE country EOP\n",
      "---\n",
      "Predict: \t2 POI 24 RE contains EOP\n",
      "Ground-Truth: \t42 POI 31 RE country EOP\n",
      "---\n",
      "Predict: \t20 POI 18 RE contains EOP\n",
      "Ground-Truth: \t38 POI 43 RE place_of_birth EOP\n",
      "---\n",
      "Predict: \t1 POI 2 RE nationality EOP\n",
      "Ground-Truth: \t38 POI 41 RE nationality EOP\n",
      "---\n",
      "Predict: \t18 POI 18 RE nationality EOP\n",
      "Ground-Truth: \t16 POI 21 RE nationality EOP\n",
      "---\n",
      "Predict: \t23 POI 24 RE nationality EOP\n",
      "Ground-Truth: \t28 POI 30 RE country EOP\n",
      "---\n",
      "Predict: \t2 POI 6 RE place_lived EOP\n",
      "Ground-Truth: \t3 POI 7 RE place_lived EOP\n",
      "---\n",
      "Predict: \t12 POI 12 RE contains EOP\n",
      "Ground-Truth: \t17 POI 11 RE contains EOP\n",
      "---\n",
      "Predict: \t12 POI 12 RE place_lived EOP\n",
      "Ground-Truth: \t10 POI 14 RE place_lived EOP\n",
      "---\n",
      "Predict: \t9 POI 8 RE contains EOP\n",
      "Ground-Truth: \t11 POI 21 RE country EOP\n",
      "---\n",
      "Predict: \t28 POI 32 RE contains EOP\n",
      "Ground-Truth: \t21 POI 16 RE company POI 16 RE company EOP\n",
      "---\n",
      "Predict: \t6 POI 6 RE contains EOP\n",
      "Ground-Truth: \t5 POI 8 RE contains EOP\n",
      "---\n",
      "Predict: \t9 POI 12 RE contains EOP\n",
      "Ground-Truth: \t12 POI 7 RE contains EOP\n",
      "---\n",
      "Predict: \t6 POI 6 RE country EOP\n",
      "Ground-Truth: \t6 POI 7 RE country EOP\n",
      "---\n",
      "Predict: \t16 POI 18 RE contains POI RE capital EOP\n",
      "Ground-Truth: \t7 POI 6 RE contains EOP\n",
      "---\n",
      "Predict: \t8 POI 8 RE contains EOP\n",
      "Ground-Truth: \t8 POI 10 RE contains EOP\n",
      "---\n",
      "Predict: \t13 POI 15 RE contains EOP\n",
      "Ground-Truth: \t14 POI 22 RE country EOP\n",
      "---\n",
      "Predict: \t26 POI 28 RE neighborhood_of EOP POI RE contains EOP\n",
      "Ground-Truth: \t37 POI 34 RE contains EOP\n",
      "---\n",
      "Predict: \t8 POI 8 RE contains EOP\n",
      "Ground-Truth: \t11 POI 7 RE contains EOP\n",
      "---\n",
      "Predict: \t2 POI 32 RE contains EOP\n",
      "Ground-Truth: \t38 POI 39 RE country EOP\n",
      "---\n",
      "Predict: \t1 POI 8 RE company EOP\n",
      "Ground-Truth: \t1 POI 9 RE company EOP\n",
      "---\n",
      "Predict: \t12 POI 12 RE contains EOP\n",
      "Ground-Truth: \t13 POI 10 RE contains EOP\n",
      "---\n",
      "Predict: \t13 POI 12 RE place_lived EOP\n",
      "Ground-Truth: \t14 POI 18 RE place_lived EOP\n",
      "---\n",
      "Predict: \t26 POI 24 RE contains EOP\n",
      "Ground-Truth: \t29 POI 27 RE contains EOP\n",
      "---\n",
      "Predict: \t8 POI 8 RE contains EOP\n",
      "Ground-Truth: \t8 POI 5 RE contains EOP\n",
      "---\n",
      "Predict: \t9 POI 8 RE company EOP\n",
      "Ground-Truth: \t5 POI 8 RE company EOP\n",
      "---\n",
      "Predict: \t6 POI 8 RE contains EOP\n",
      "Ground-Truth: \t8 POI 6 RE contains EOP\n",
      "---\n",
      "Predict: \t16 POI 15 RE contains EOP\n",
      "Ground-Truth: \t18 POI 16 RE contains EOP\n",
      "---\n",
      "Predict: \t9 POI 12 RE contains EOP\n",
      "Ground-Truth: \t9 POI 13 RE country EOP\n",
      "---\n",
      "Predict: \t15 POI 15 RE contains EOP\n",
      "Ground-Truth: \t18 POI 14 RE contains EOP\n",
      "---\n",
      "Predict: \t12 POI 12 RE nationality EOP\n",
      "Ground-Truth: \t4 POI 15 RE place_lived EOP\n",
      "---\n",
      "Predict: \t12 POI 12 RE company EOP\n",
      "Ground-Truth: \t10 POI 19 RE company EOP\n",
      "---\n",
      "Predict: \t16 POI 18 RE place_lived EOP\n",
      "Ground-Truth: \t17 POI 20 RE place_lived EOP\n",
      "---\n",
      "Predict: \t16 POI 16 RE nationality EOP\n",
      "Ground-Truth: \t19 POI 14 RE nationality EOP\n",
      "---\n",
      "Predict: \t7 POI 6 RE nationality EOP\n",
      "Ground-Truth: \t19 POI 7 RE nationality EOP\n",
      "---\n",
      "Predict: \t23 POI 23 RE contains EOP\n",
      "Ground-Truth: \t32 POI 35 RE contains EOP\n",
      "---\n",
      "Predict: \t1 POI 1 RE contains EOP\n",
      "Ground-Truth: \t19 POI 1 RE contains EOP\n",
      "---\n",
      "Predict: \t9 POI 32 RE contains EOP\n",
      "Ground-Truth: \t31 POI 33 RE country EOP\n",
      "---\n",
      "Predict: \t12 POI 12 RE contains EOP\n",
      "Ground-Truth: \t13 POI 12 RE contains EOP\n",
      "---\n",
      "Predict: \t9 POI 8 RE contains EOP\n",
      "Ground-Truth: \t9 POI 30 RE country EOP\n",
      "---\n",
      "Predict: \t8 POI 8 RE contains EOP\n",
      "Ground-Truth: \t8 POI 11 RE contains EOP\n",
      "---\n",
      "Predict: \t13 POI 23 RE contains POI RE contains EOP\n",
      "Ground-Truth: \t12 POI 25 RE contains EOP\n",
      "---\n",
      "Predict: \t10 POI 12 RE place_lived EOP\n",
      "Ground-Truth: \t9 POI 12 RE place_lived EOP\n",
      "---\n",
      "Predict: \t12 POI 10 RE contains EOP\n",
      "Ground-Truth: \t12 POI 10 RE contains EOP\n",
      "---\n",
      "Predict: \t1 POI 1 RE company EOP\n",
      "Ground-Truth: \t1 POI 8 RE company EOP\n",
      "---\n",
      "Predict: \t1 POI 23 RE contains POI RE EOP\n",
      "Ground-Truth: \t27 POI 18 RE country EOP\n",
      "---\n",
      "Predict: \t33 POI 44 RE contains EOP\n",
      "Ground-Truth: \t39 POI 40 RE contains EOP\n",
      "---\n",
      "Predict: \t24 POI 32 RE nationality EOP\n",
      "Ground-Truth: \t36 POI 33 RE contains EOP\n",
      "---\n",
      "Predict: \t12 POI 12 RE contains EOP\n",
      "Ground-Truth: \t13 POI 4 RE contains EOP\n",
      "---\n",
      "Predict: \t1 POI 1 RE contains EOP\n",
      "Ground-Truth: \t12 POI 1 RE country EOP\n",
      "---\n",
      "Predict: \t8 POI 8 RE contains EOP\n",
      "Ground-Truth: \t6 POI 10 RE contains EOP\n",
      "---\n",
      "Predict: \t2 POI 5 RE place_lived EOP\n",
      "Ground-Truth: \t2 POI 5 RE place_lived EOP\n",
      "---\n",
      "Predict: \t12 POI 12 RE contains EOP\n",
      "Ground-Truth: \t12 POI 8 RE contains EOP\n",
      "---\n",
      "Predict: \t13 POI 12 RE contains EOP\n",
      "Ground-Truth: \t17 POI 34 RE country EOP\n",
      "---\n",
      "Predict: \t6 POI 6 RE nationality EOP\n",
      "Ground-Truth: \t5 POI 8 RE nationality EOP\n",
      "---\n",
      "Predict: \t23 POI 23 RE contains EOP\n",
      "Ground-Truth: \t29 POI 23 RE contains EOP\n",
      "---\n",
      "Predict: \t8 POI 8 RE contains EOP\n",
      "Ground-Truth: \t10 POI 9 RE contains EOP 10 POI 9 RE contains EOP\n",
      "---\n",
      "Predict: \t11 POI 44 RE contains EOP\n",
      "Ground-Truth: \t51 POI 49 RE contains EOP\n",
      "---\n",
      "Predict: \t12 POI 12 RE contains EOP\n",
      "Ground-Truth: \t11 POI 10 RE contains EOP\n",
      "---\n",
      "Predict: \t1 POI 1 RE contains EOP\n",
      "Ground-Truth: \t9 POI 1 RE contains EOP\n",
      "---\n",
      "Predict: \t33 POI 37 RE contains EOP\n",
      "Ground-Truth: \t49 POI 47 RE contains EOP\n",
      "---\n",
      "Predict: \t11 POI 44 RE contains EOP\n",
      "Ground-Truth: \t40 POI 14 RE country EOP\n",
      "---\n",
      "Predict: \t19 POI 23 RE contains EOP\n",
      "Ground-Truth: \t18 POI 26 RE country EOP\n",
      "---\n",
      "Predict: \t20 POI 20 RE company EOP\n",
      "Ground-Truth: \t15 POI 26 RE company EOP\n",
      "---\n",
      "Predict: \t12 POI 12 RE nationality EOP\n",
      "Ground-Truth: \t12 POI 11 RE nationality POI 11 RE nationality EOP\n",
      "---\n",
      "Predict: \t10 POI 12 RE place_lived EOP\n",
      "Ground-Truth: \t7 POI 13 RE place_lived EOP\n",
      "---\n",
      "Predict: \t12 POI 12 RE nationality EOP\n",
      "Ground-Truth: \t7 POI 13 RE nationality EOP\n",
      "---\n",
      "Predict: \t6 POI 6 RE place_lived EOP\n",
      "Ground-Truth: \t5 POI 8 RE place_lived EOP\n",
      "---\n",
      "Predict: \t8 POI 8 RE contains EOP\n",
      "Ground-Truth: \t8 POI 10 RE country EOP\n",
      "---\n",
      "Predict: \t6 POI 6 RE contains EOP\n",
      "Ground-Truth: \t9 POI 5 RE administrative_divisions EOP\n",
      "---\n",
      "Predict: \t23 POI 23 RE contains EOP\n",
      "Ground-Truth: \t29 POI 27 RE contains EOP\n",
      "---\n",
      "Predict: \t13 POI 18 RE contains POI RE capital EOP\n",
      "Ground-Truth: \t17 POI 12 RE country EOP\n",
      "---\n",
      "Predict: \t26 POI 24 RE contains EOP\n",
      "Ground-Truth: \t29 POI 31 RE contains EOP\n",
      "---\n",
      "Predict: \t18 POI 20 RE contains EOP\n",
      "Ground-Truth: \t17 POI 20 RE country EOP\n",
      "---\n",
      "Predict: \t19 POI 18 RE contains EOP\n",
      "Ground-Truth: \t38 POI 24 RE country EOP\n",
      "---\n",
      "Predict: \t26 POI 27 RE contains EOP\n",
      "Ground-Truth: \t30 POI 28 RE contains EOP\n",
      "---\n",
      "Predict: \t10 POI 12 RE contains EOP\n",
      "Ground-Truth: \t11 POI 13 RE country EOP\n",
      "---\n",
      "Predict: \t1 POI 35 RE contains POI RE capital EOP\n",
      "Ground-Truth: \t38 POI 19 RE country EOP\n",
      "---\n",
      "Predict: \t32 POI 32 RE contains EOP\n",
      "Ground-Truth: \t37 POI 34 RE contains EOP\n",
      "---\n",
      "Predict: \t18 POI 20 RE contains EOP\n",
      "Ground-Truth: \t23 POI 20 RE contains EOP\n",
      "---\n",
      "Predict: \t33 POI 32 RE contains EOP\n",
      "Ground-Truth: \t29 POI 34 RE contains EOP\n",
      "---\n",
      "Predict: \t32 POI 37 RE contains EOP\n",
      "Ground-Truth: \t41 POI 40 RE contains EOP\n",
      "---\n",
      "Predict: \t23 POI 27 RE contains EOP\n",
      "Ground-Truth: \t29 POI 26 RE contains EOP\n",
      "---\n",
      "Predict: \t20 POI 20 RE neighborhood_of EOP 23 POI 20 RE contains EOP\n",
      "Ground-Truth: \t23 POI 20 RE contains EOP\n",
      "---\n",
      "Predict: \t9 POI 12 RE nationality EOP\n",
      "Ground-Truth: \t11 POI 10 RE contains EOP\n",
      "---\n",
      "Predict: \t12 POI 18 RE contains EOP\n",
      "Ground-Truth: \t13 POI 19 RE place_lived EOP\n",
      "---\n",
      "Predict: \t18 POI 18 RE contains POI RE EOP\n",
      "Ground-Truth: \t17 POI 20 RE administrative_divisions EOP\n",
      "---\n",
      "Predict: \t32 POI 32 RE contains EOP\n",
      "Ground-Truth: \t1 POI 33 RE place_of_death EOP\n",
      "---\n",
      "Predict: \t6 POI 35 RE contains POI RE contains EOP\n",
      "Ground-Truth: \t21 POI 7 RE contains POI 7 RE contains EOP\n",
      "---\n",
      "Predict: \t4 POI 31 RE contains POI RE contains EOP\n",
      "Ground-Truth: \t20 POI 4 RE country POI 4 RE country EOP\n",
      "---\n",
      "Predict: \t12 POI 12 RE contains EOP\n",
      "Ground-Truth: \t13 POI 10 RE contains EOP\n",
      "---\n",
      "Predict: \t9 POI 9 RE nationality EOP\n",
      "Ground-Truth: \t6 POI 9 RE nationality EOP\n",
      "---\n",
      "Predict: \t24 POI 24 RE company EOP\n",
      "Ground-Truth: \t22 POI 30 RE company EOP\n",
      "---\n",
      "Predict: \t9 POI 8 RE contains EOP\n",
      "Ground-Truth: \t13 POI 2 RE contains EOP\n",
      "---\n",
      "Predict: \t10 POI 12 RE contains POI RE capital EOP\n",
      "Ground-Truth: \t7 POI 13 RE contains EOP\n",
      "---\n",
      "Predict: \t26 POI 32 RE neighborhood_of EOP POI RE contains EOP\n",
      "Ground-Truth: \t33 POI 30 RE contains EOP\n",
      "---\n",
      "Predict: \t8 POI 9 RE country EOP RE EOP POI RE\n",
      "Ground-Truth: \t10 POI 11 RE country EOP\n",
      "---\n",
      "Predict: \t6 POI 6 RE contains EOP\n",
      "Ground-Truth: \t6 POI 6 RE country EOP 6 POI 6 RE country EOP\n",
      "---\n",
      "Predict: \t18 POI 20 RE nationality EOP\n",
      "Ground-Truth: \t11 POI 20 RE nationality EOP\n",
      "---\n",
      "Predict: \t6 POI 6 RE country EOP RE\n",
      "Ground-Truth: \t6 POI 5 RE contains EOP\n",
      "---\n",
      "Predict: \t27 POI 27 RE place_lived EOP\n",
      "Ground-Truth: \t21 POI 29 RE place_lived EOP 21 POI 29 RE place_lived EOP\n",
      "---\n",
      "Predict: \t18 POI 22 RE capital POI RE capital EOP\n",
      "Ground-Truth: \t14 POI 12 RE country EOP 14 POI 12 RE country EOP\n",
      "---\n",
      "Predict: \t9 POI 8 RE contains EOP\n",
      "Ground-Truth: \t10 POI 6 RE country EOP\n",
      "---\n",
      "Predict: \t1 POI 31 RE company EOP\n",
      "Ground-Truth: \t1 POI 7 RE company EOP\n",
      "---\n",
      "Predict: \t6 POI 4 RE contains EOP\n",
      "Ground-Truth: \t12 POI 4 RE contains EOP\n",
      "---\n",
      "Predict: \t9 POI 8 RE company EOP\n",
      "Ground-Truth: \t5 POI 11 RE company EOP\n",
      "---\n",
      "Predict: \t24 POI 24 RE nationality EOP\n",
      "Ground-Truth: \t16 POI 12 RE contains EOP\n",
      "---\n",
      "Predict: \t20 POI 20 RE nationality EOP\n",
      "Ground-Truth: \t21 POI 33 RE administrative_divisions EOP\n",
      "---\n",
      "Predict: \t2 POI 2 RE contains EOP\n",
      "Ground-Truth: \t18 POI 16 RE contains EOP\n",
      "---\n",
      "Predict: \t28 POI 32 RE contains POI RE contains EOP\n",
      "Ground-Truth: \t35 POI 33 RE country EOP\n",
      "---\n",
      "Predict: \t13 POI 15 RE contains EOP\n",
      "Ground-Truth: \t10 POI 16 RE place_lived EOP\n",
      "---\n",
      "Predict: \t6 POI 4 RE contains EOP\n",
      "Ground-Truth: \t5 POI 5 RE country EOP 5 POI 5 RE country EOP\n",
      "---\n",
      "Predict: \t6 POI 6 RE contains EOP\n",
      "Ground-Truth: \t6 POI 5 RE contains EOP\n",
      "---\n",
      "Predict: \t23 POI 23 RE contains EOP\n",
      "Ground-Truth: \t1 POI 24 RE place_of_death EOP\n",
      "---\n",
      "Predict: \t8 POI 8 RE contains EOP\n",
      "Ground-Truth: \t8 POI 6 RE contains EOP\n",
      "---\n",
      "Predict: \t18 POI 18 RE contains POI RE EOP\n",
      "Ground-Truth: \t18 POI 41 RE country EOP\n",
      "---\n",
      "Predict: \t8 POI 8 RE contains EOP\n",
      "Ground-Truth: \t9 POI 8 RE contains EOP\n",
      "---\n",
      "Predict: \t9 POI 8 RE company EOP\n",
      "Ground-Truth: \t4 POI 11 RE company EOP\n",
      "---\n",
      "Predict: \t12 POI 12 RE contains EOP\n",
      "Ground-Truth: \t13 POI 12 RE contains EOP\n",
      "---\n",
      "Predict: \t2 POI 20 RE place_lived EOP\n",
      "Ground-Truth: \t19 POI 22 RE place_lived EOP\n",
      "---\n",
      "Predict: \t23 POI 23 RE contains EOP\n",
      "Ground-Truth: \t29 POI 26 RE country EOP\n",
      "---\n",
      "Predict: \t13 POI 18 RE country EOP RE capital EOP RE\n",
      "Ground-Truth: \t16 POI 17 RE country EOP\n",
      "---\n",
      "Predict: \t18 POI 18 RE contains EOP\n",
      "Ground-Truth: \t20 POI 19 RE contains EOP\n",
      "---\n",
      "Predict: \t18 POI 18 RE contains EOP\n",
      "Ground-Truth: \t20 POI 19 RE contains EOP\n",
      "---\n",
      "Predict: \t10 POI 11 RE contains POI RE EOP\n",
      "Ground-Truth: \t14 POI 13 RE contains EOP\n",
      "---\n",
      "Predict: \t35 POI 37 RE company EOP\n",
      "Ground-Truth: \t47 POI 42 RE founders EOP\n",
      "---\n",
      "Predict: \t1 POI 12 RE contains EOP\n",
      "Ground-Truth: \t46 POI 38 RE contains EOP\n",
      "---\n",
      "Predict: \t12 POI 12 RE contains EOP\n",
      "Ground-Truth: \t6 POI 12 RE country EOP\n",
      "---\n",
      "Predict: \t20 POI 30 RE nationality EOP\n",
      "Ground-Truth: \t24 POI 44 RE country EOP\n",
      "---\n",
      "Predict: \t18 POI 18 RE neighborhood_of EOP 18 POI 18 RE contains EOP\n",
      "Ground-Truth: \t19 POI 10 RE contains EOP\n",
      "---\n",
      "Predict: \t6 POI 4 RE contains EOP\n",
      "Ground-Truth: \t6 POI 4 RE contains EOP\n",
      "---\n",
      "Predict: \t2 POI 5 RE place_lived EOP\n",
      "Ground-Truth: \t2 POI 6 RE place_lived EOP\n",
      "---\n",
      "Predict: \t2 POI 2 RE contains EOP\n",
      "Ground-Truth: \t3 POI 30 RE country EOP\n",
      "---\n",
      "Predict: \t9 POI 8 RE contains EOP\n",
      "Ground-Truth: \t11 POI 6 RE contains EOP\n",
      "---\n",
      "Predict: \t10 POI 12 RE contains POI RE capital EOP\n",
      "Ground-Truth: \t12 POI 7 RE country EOP\n",
      "---\n",
      "Predict: \t23 POI 23 RE contains EOP\n",
      "Ground-Truth: \t24 POI 23 RE contains EOP\n",
      "---\n",
      "Predict: \t2 POI 20 RE contains EOP\n",
      "Ground-Truth: \t19 POI 3 RE neighborhood_of EOP\n",
      "---\n",
      "Predict: \t28 POI 32 RE contains POI RE contains EOP\n",
      "Ground-Truth: \t36 POI 42 RE country EOP\n",
      "---\n",
      "Predict: \t18 POI 18 RE contains EOP\n",
      "Ground-Truth: \t20 POI 17 RE contains EOP\n",
      "---\n",
      "Predict: \t14 POI 14 RE company EOP\n",
      "Ground-Truth: \t13 POI 9 RE company EOP\n",
      "---\n",
      "Predict: \t13 POI 12 RE contains EOP\n",
      "Ground-Truth: \t14 POI 13 RE contains EOP\n",
      "---\n",
      "Predict: \t23 POI 23 RE place_lived EOP\n",
      "Ground-Truth: \t23 POI 26 RE place_lived EOP\n",
      "---\n",
      "Predict: \t2 POI 5 RE place_lived EOP\n",
      "Ground-Truth: \t2 POI 6 RE place_lived EOP\n",
      "---\n",
      "Predict: \t20 POI 23 RE place_lived EOP\n",
      "Ground-Truth: \t28 POI 31 RE place_lived EOP\n",
      "---\n",
      "Predict: \t1 POI 31 RE contains POI RE capital POI EOP\n",
      "Ground-Truth: \t28 POI 6 RE country EOP\n",
      "---\n",
      "Predict: \t2 POI 2 RE contains EOP\n",
      "Ground-Truth: \t2 POI 25 RE contains EOP\n",
      "---\n",
      "Predict: \t7 POI 8 RE place_lived EOP\n",
      "Ground-Truth: \t13 POI 6 RE place_lived EOP\n",
      "---\n",
      "Predict: \t23 POI 23 RE contains EOP\n",
      "Ground-Truth: \t24 POI 22 RE contains EOP\n",
      "---\n",
      "Predict: \t32 POI 37 RE place_lived EOP\n",
      "Ground-Truth: \t38 POI 41 RE place_lived EOP\n",
      "---\n",
      "Predict: \t26 POI 28 RE contains EOP\n",
      "Ground-Truth: \t30 POI 34 RE contains EOP\n",
      "---\n",
      "Predict: \t20 POI 20 RE place_lived EOP\n",
      "Ground-Truth: \t13 POI 16 RE place_lived EOP\n",
      "---\n",
      "Predict: \t13 POI 18 RE contains EOP\n",
      "Ground-Truth: \t16 POI 27 RE country EOP\n",
      "---\n",
      "Predict: \t20 POI 20 RE contains EOP\n",
      "Ground-Truth: \t20 POI 23 RE country EOP\n",
      "---\n",
      "Predict: \t6 POI 4 RE contains EOP\n",
      "Ground-Truth: \t3 POI 10 RE country EOP\n",
      "---\n",
      "Predict: \t10 POI 10 RE contains EOP\n",
      "Ground-Truth: \t12 POI 9 RE contains EOP\n",
      "---\n",
      "Predict: \t23 POI 23 RE contains POI RE contains EOP\n",
      "Ground-Truth: \t11 POI 19 RE country EOP\n",
      "---\n",
      "Predict: \t9 POI 18 RE contains POI RE capital EOP\n",
      "Ground-Truth: \t9 POI 7 RE country EOP\n",
      "---\n",
      "Predict: \t19 POI 20 RE contains EOP\n",
      "Ground-Truth: \t18 POI 24 RE country EOP\n",
      "---\n",
      "Predict: \t42 POI 44 RE contains POI RE contains EOP\n",
      "Ground-Truth: \t27 POI 120 RE country EOP 27 POI 120 RE country EOP 27 POI 120 RE country EOP\n",
      "---\n",
      "Predict: \t23 POI 23 RE contains EOP\n",
      "Ground-Truth: \t26 POI 22 RE contains EOP\n",
      "---\n",
      "Predict: \t23 POI 23 RE contains EOP\n",
      "Ground-Truth: \t29 POI 24 RE country EOP\n",
      "---\n",
      "Predict: \t26 POI 28 RE country EOP\n",
      "Ground-Truth: \t30 POI 28 RE contains EOP\n",
      "---\n",
      "Predict: \t28 POI 28 RE nationality EOP\n",
      "Ground-Truth: \t32 POI 34 RE nationality EOP\n",
      "---\n",
      "Predict: \t18 POI 18 RE contains EOP\n",
      "Ground-Truth: \t17 POI 21 RE country EOP\n",
      "---\n",
      "Predict: \t23 POI 23 RE contains EOP\n",
      "Ground-Truth: \t23 POI 20 RE contains EOP\n",
      "---\n",
      "Predict: \t1 POI 31 RE contains POI RE contains EOP\n",
      "Ground-Truth: \t1 POI 6 RE nationality EOP\n",
      "---\n",
      "Predict: \t6 POI 4 RE contains EOP\n",
      "Ground-Truth: \t6 POI 5 RE contains EOP\n",
      "---\n",
      "Predict: \t12 POI 12 RE contains EOP\n",
      "Ground-Truth: \t34 POI 14 RE contains EOP\n",
      "---\n",
      "Predict: \t26 POI 28 RE nationality EOP\n",
      "Ground-Truth: \t33 POI 29 RE nationality EOP\n",
      "---\n",
      "Predict: \t35 POI 37 RE contains EOP\n",
      "Ground-Truth: \t32 POI 13 RE country POI 13 RE country EOP\n",
      "---\n",
      "Predict: \t2 POI 2 RE contains EOP\n",
      "Ground-Truth: \t1 POI 4 RE contains EOP\n",
      "---\n",
      "Predict: \t15 POI 15 RE contains EOP\n",
      "Ground-Truth: \t17 POI 15 RE contains EOP\n",
      "---\n",
      "Predict: \t20 POI 20 RE contains POI EOP\n",
      "Ground-Truth: \t23 POI 22 RE contains EOP\n",
      "---\n",
      "Predict: \t28 POI 32 RE contains EOP\n",
      "Ground-Truth: \t13 POI 13 RE contains EOP 13 POI 13 RE contains EOP\n",
      "---\n",
      "Predict: \t13 POI 15 RE contains EOP\n",
      "Ground-Truth: \t13 POI 14 RE country EOP\n",
      "---\n",
      "Predict: \t28 POI 32 RE contains POI RE contains EOP\n",
      "Ground-Truth: \t18 POI 36 RE country EOP\n",
      "---\n",
      "Predict: \t28 POI 32 RE contains POI RE contains EOP\n",
      "Ground-Truth: \t39 POI 33 RE contains EOP\n",
      "---\n",
      "Predict: \t13 POI 15 RE nationality EOP\n",
      "Ground-Truth: \t11 POI 14 RE nationality EOP\n",
      "---\n",
      "Predict: \t23 POI 23 RE contains EOP\n",
      "Ground-Truth: \t24 POI 23 RE contains EOP\n",
      "---\n",
      "Predict: \t19 POI 18 RE contains EOP\n",
      "Ground-Truth: \t17 POI 27 RE country EOP\n",
      "---\n",
      "Predict: \t13 POI 14 RE place_lived EOP\n",
      "Ground-Truth: \t12 POI 16 RE place_lived EOP\n",
      "---\n",
      "Predict: \t23 POI 23 RE contains EOP\n",
      "Ground-Truth: \t26 POI 28 RE contains EOP\n",
      "---\n",
      "Predict: \t30 POI 37 RE nationality EOP\n",
      "Ground-Truth: \t45 POI 42 RE nationality EOP\n",
      "---\n",
      "Predict: \t8 POI 6 RE contains EOP\n",
      "Ground-Truth: \t5 POI 8 RE contains EOP\n",
      "---\n",
      "Predict: \t6 POI 4 RE contains EOP\n",
      "Ground-Truth: \t9 POI 3 RE country EOP\n",
      "---\n",
      "Predict: \t1 POI 31 RE company EOP\n",
      "Ground-Truth: \t1 POI 37 RE founders POI 37 RE founders EOP 1 POI 37 RE founders POI 37 RE founders EOP\n",
      "---\n",
      "Predict: \t24 POI 24 RE nationality EOP\n",
      "Ground-Truth: \t25 POI 28 RE nationality EOP\n",
      "---\n",
      "Predict: \t13 POI 15 RE contains EOP\n",
      "Ground-Truth: \t13 POI 14 RE country EOP\n",
      "---\n",
      "Predict: \t27 POI 28 RE contains EOP\n",
      "Ground-Truth: \t31 POI 28 RE contains EOP\n",
      "---\n",
      "Predict: \t15 POI 15 RE contains EOP\n",
      "Ground-Truth: \t16 POI 14 RE contains EOP\n",
      "---\n",
      "Predict: \t23 POI 23 RE contains EOP\n",
      "Ground-Truth: \t30 POI 29 RE contains EOP\n",
      "---\n",
      "Predict: \t14 POI 15 RE company EOP\n",
      "Ground-Truth: \t10 POI 22 RE company EOP\n",
      "---\n",
      "Predict: \t13 POI 15 RE contains EOP\n",
      "Ground-Truth: \t15 POI 17 RE contains EOP\n",
      "---\n",
      "Predict: \t16 POI 18 RE contains EOP\n",
      "Ground-Truth: \t19 POI 17 RE contains EOP\n",
      "---\n",
      "Predict: \t8 POI 8 RE place_lived EOP\n",
      "Ground-Truth: \t6 POI 8 RE place_lived EOP\n",
      "---\n",
      "Predict: \t9 POI 8 RE contains EOP\n",
      "Ground-Truth: \t10 POI 9 RE contains EOP\n",
      "---\n",
      "Predict: \t2 POI 2 RE contains EOP\n",
      "Ground-Truth: \t4 POI 2 RE country EOP\n",
      "---\n",
      "Predict: \t13 POI 12 RE contains EOP\n",
      "Ground-Truth: \t12 POI 16 RE contains EOP\n",
      "---\n",
      "Predict: \t6 POI 4 RE contains EOP\n",
      "Ground-Truth: \t19 POI 17 RE contains EOP\n",
      "---\n",
      "Predict: \t2 POI 2 RE contains EOP\n",
      "Ground-Truth: \t3 POI 3 RE country EOP 3 POI 3 RE country EOP\n",
      "---\n",
      "Predict: \t2 POI 5 RE place_lived EOP\n",
      "Ground-Truth: \t2 POI 5 RE place_lived EOP\n",
      "---\n",
      "Predict: \t1 POI 24 RE company EOP\n",
      "Ground-Truth: \t18 POI 3 RE company EOP\n",
      "---\n",
      "Predict: \t6 POI 32 RE contains POI RE capital EOP\n",
      "Ground-Truth: \t31 POI 10 RE country EOP\n",
      "---\n",
      "Predict: \t6 POI 6 RE contains EOP\n",
      "Ground-Truth: \t5 POI 6 RE country EOP\n",
      "---\n",
      "Predict: \t1 POI 4 RE nationality EOP\n",
      "Ground-Truth: \t3 POI 6 RE nationality EOP\n",
      "---\n",
      "Predict: \t12 POI 12 RE contains EOP\n",
      "Ground-Truth: \t13 POI 12 RE contains EOP\n",
      "---\n",
      "Predict: \t24 POI 30 RE place_lived EOP\n",
      "Ground-Truth: \t32 POI 39 RE company EOP\n",
      "---\n",
      "Predict: \t13 POI 15 RE contains EOP\n",
      "Ground-Truth: \t10 POI 18 RE company EOP\n",
      "---\n",
      "Predict: \t13 POI 12 RE contains EOP\n",
      "Ground-Truth: \t17 POI 24 RE country EOP\n",
      "---\n",
      "Predict: \t17 POI 18 RE place_lived EOP\n",
      "Ground-Truth: \t13 POI 17 RE place_lived EOP\n",
      "---\n",
      "Predict: \t4 POI 4 RE contains EOP\n",
      "Ground-Truth: \t6 POI 5 RE contains EOP\n",
      "---\n",
      "Predict: \t13 POI 12 RE contains EOP\n",
      "Ground-Truth: \t14 POI 13 RE contains EOP\n",
      "---\n",
      "Predict: \t9 POI 10 RE place_lived EOP\n",
      "Ground-Truth: \t8 POI 11 RE place_lived EOP\n",
      "---\n",
      "Predict: \t1 POI 1 RE contains EOP\n",
      "Ground-Truth: \t1 POI 22 RE country EOP 1 POI 22 RE country EOP\n",
      "---\n",
      "Predict: \t12 POI 9 RE contains EOP\n",
      "Ground-Truth: \t13 POI 9 RE contains EOP\n",
      "---\n",
      "Predict: \t28 POI 32 RE contains POI RE contains EOP\n",
      "Ground-Truth: \t32 POI 11 RE country EOP\n",
      "---\n",
      "Predict: \t6 POI 2 RE contains EOP\n",
      "Ground-Truth: \t4 POI 3 RE contains EOP\n",
      "---\n",
      "Predict: \t42 POI 44 RE contains EOP\n",
      "Ground-Truth: \t79 POI 47 RE country EOP\n",
      "---\n",
      "Predict: \t1 POI 1 RE nationality EOP\n",
      "Ground-Truth: \t1 POI 4 RE nationality EOP\n",
      "---\n",
      "Predict: \t1 POI 15 RE company EOP\n",
      "Ground-Truth: \t1 POI 11 RE company EOP\n",
      "---\n",
      "Predict: \t13 POI 15 RE contains EOP\n",
      "Ground-Truth: \t15 POI 16 RE country EOP\n",
      "---\n",
      "Predict: \t1 POI 18 RE contains POI RE capital EOP\n",
      "Ground-Truth: \t3 POI 13 RE contains POI 13 RE contains EOP 3 POI 13 RE contains POI 13 RE contains EOP\n",
      "---\n",
      "Predict: \t10 POI 12 RE contains POI RE contains EOP\n",
      "Ground-Truth: \t13 POI 12 RE contains EOP\n",
      "---\n",
      "Predict: \t9 POI 32 RE contains EOP\n",
      "Ground-Truth: \t31 POI 8 RE capital EOP\n",
      "---\n",
      "Predict: \t35 POI 44 RE contains EOP\n",
      "Ground-Truth: \t54 POI 45 RE contains EOP\n",
      "---\n",
      "Predict: \t32 POI 35 RE contains EOP\n",
      "Ground-Truth: \t42 POI 40 RE contains EOP\n",
      "---\n",
      "Predict: \t12 POI 12 RE company EOP\n",
      "Ground-Truth: \t10 POI 16 RE company EOP\n",
      "---\n",
      "Predict: \t8 POI 8 RE contains EOP\n",
      "Ground-Truth: \t5 POI 6 RE country EOP\n",
      "---\n",
      "Predict: \t8 POI 8 RE place_lived EOP\n",
      "Ground-Truth: \t5 POI 9 RE place_lived EOP\n",
      "---\n",
      "Predict: \t6 POI 4 RE contains EOP\n",
      "Ground-Truth: \t5 POI 4 RE contains EOP\n",
      "---\n",
      "Predict: \t26 POI 28 RE nationality EOP\n",
      "Ground-Truth: \t23 POI 29 RE company EOP\n",
      "---\n",
      "Predict: \t8 POI 8 RE neighborhood_of EOP 8 POI 6 RE contains EOP\n",
      "Ground-Truth: \t10 POI 7 RE contains EOP\n",
      "---\n",
      "Predict: \t28 POI 30 RE company EOP\n",
      "Ground-Truth: \t36 POI 37 RE children EOP 37 EOP\n",
      "---\n",
      "Predict: \t18 POI 18 RE contains EOP\n",
      "Ground-Truth: \t23 POI 22 RE contains EOP\n",
      "---\n",
      "Predict: \t1 POI 23 RE contains POI RE capital POI RE EOP\n",
      "Ground-Truth: \t23 POI 8 RE country EOP\n",
      "---\n",
      "Predict: \t4 POI 4 RE country EOP RE EOP RE\n",
      "Ground-Truth: \t5 POI 4 RE contains EOP\n",
      "---\n",
      "Predict: \t16 POI 15 RE contains EOP\n",
      "Ground-Truth: \t17 POI 14 RE contains EOP\n",
      "---\n",
      "Predict: \t1 POI 1 RE contains EOP\n",
      "Ground-Truth: \t20 POI 2 RE country EOP\n",
      "---\n",
      "Predict: \t8 POI 8 RE neighborhood_of EOP POI 8 RE contains EOP\n",
      "Ground-Truth: \t8 POI 7 RE contains EOP\n",
      "---\n",
      "Predict: \t13 POI 15 RE contains POI RE capital POI RE EOP\n",
      "Ground-Truth: \t16 POI 11 RE country EOP\n",
      "---\n",
      "Predict: \t28 POI 32 RE contains EOP\n",
      "Ground-Truth: \t34 POI 36 RE country EOP\n",
      "---\n",
      "Predict: \t18 POI 18 RE contains EOP\n",
      "Ground-Truth: \t21 POI 20 RE contains EOP\n",
      "---\n",
      "Predict: \t9 POI 8 RE nationality EOP\n",
      "Ground-Truth: \t11 POI 6 RE nationality EOP\n",
      "---\n",
      "Predict: \t26 POI 32 RE contains EOP\n",
      "Ground-Truth: \t33 POI 19 RE contains EOP\n",
      "---\n",
      "Predict: \t4 POI 2 RE contains EOP\n",
      "Ground-Truth: \t6 POI 12 RE contains EOP\n",
      "---\n",
      "Predict: \t23 POI 24 RE contains EOP\n",
      "Ground-Truth: \t7 POI 21 RE country POI 21 RE country EOP 7 POI 21 RE country POI 21 RE country EOP\n",
      "---\n",
      "Predict: \t23 POI 23 RE contains EOP\n",
      "Ground-Truth: \t23 POI 22 RE contains EOP\n",
      "---\n",
      "Predict: \t27 POI 32 RE contains EOP\n",
      "Ground-Truth: \t34 POI 30 RE contains EOP\n",
      "---\n",
      "Predict: \t29 POI 32 RE contains EOP\n",
      "Ground-Truth: \t34 POI 31 RE contains EOP\n",
      "---\n",
      "Predict: \t6 POI 6 RE nationality EOP\n",
      "Ground-Truth: \t4 POI 7 RE nationality EOP\n",
      "---\n",
      "Predict: \t1 POI 15 RE nationality EOP\n",
      "Ground-Truth: \t14 POI 17 RE nationality EOP\n",
      "---\n",
      "Predict: \t9 POI 9 RE country EOP RE capital EOP\n",
      "Ground-Truth: \t8 POI 21 RE country EOP\n",
      "---\n",
      "Predict: \t1 POI 8 RE company EOP\n",
      "Ground-Truth: \t1 POI 10 RE company EOP\n",
      "---\n",
      "Predict: \t9 POI 12 RE contains EOP\n",
      "Ground-Truth: \t11 POI 6 RE contains EOP\n",
      "---\n",
      "Predict: \t1 POI 35 RE contains POI RE contains EOP\n",
      "Ground-Truth: \t44 POI 19 RE country EOP\n",
      "---\n",
      "Predict: \t12 POI 12 RE contains EOP\n",
      "Ground-Truth: \t11 POI 15 RE contains EOP\n",
      "---\n",
      "Predict: \t28 POI 32 RE contains POI RE contains EOP\n",
      "Ground-Truth: \t36 POI 9 RE country EOP\n",
      "---\n",
      "Predict: \t1 POI 12 RE company EOP\n",
      "Ground-Truth: \t1 POI 3 RE company EOP\n",
      "---\n",
      "Predict: \t23 POI 23 RE contains EOP\n",
      "Ground-Truth: \t23 POI 29 RE country EOP\n",
      "---\n",
      "Predict: \t12 POI 12 RE contains EOP\n",
      "Ground-Truth: \t9 POI 6 RE contains EOP\n",
      "---\n",
      "Predict: \t2 POI 4 RE contains EOP\n",
      "Ground-Truth: \t2 POI 6 RE company EOP\n",
      "---\n",
      "Predict: \t27 POI 26 RE contains EOP\n",
      "Ground-Truth: \t30 POI 26 RE contains EOP\n",
      "---\n",
      "Predict: \t8 POI 8 RE contains EOP\n",
      "Ground-Truth: \t8 POI 7 RE contains EOP\n",
      "---\n",
      "Predict: \t13 POI 14 RE contains EOP\n",
      "Ground-Truth: \t16 POI 13 RE contains EOP\n",
      "---\n",
      "Predict: \t15 POI 18 RE contains EOP\n",
      "Ground-Truth: \t19 POI 15 RE contains EOP\n",
      "---\n",
      "Predict: \t2 POI 26 RE contains EOP\n",
      "Ground-Truth: \t3 POI 28 RE contains EOP\n",
      "---\n",
      "Predict: \t1 POI 29 RE company EOP\n",
      "Ground-Truth: \t36 POI 41 RE company EOP\n",
      "---\n",
      "Predict: \t8 POI 8 RE contains EOP\n",
      "Ground-Truth: \t11 POI 10 RE contains EOP\n",
      "---\n",
      "Predict: \t9 POI 8 RE contains EOP\n",
      "Ground-Truth: \t10 POI 7 RE contains EOP\n",
      "---\n",
      "Predict: \t13 POI 12 RE contains EOP\n",
      "Ground-Truth: \t16 POI 11 RE contains EOP\n",
      "---\n",
      "Predict: \t32 POI 35 RE contains EOP\n",
      "Ground-Truth: \t41 POI 35 RE contains EOP\n",
      "---\n",
      "Predict: \t13 POI 15 RE contains EOP\n",
      "Ground-Truth: \t15 POI 16 RE country EOP\n",
      "---\n",
      "Predict: \t13 POI 18 RE contains EOP\n",
      "Ground-Truth: \t21 POI 15 RE country POI 15 RE country EOP\n",
      "---\n",
      "Predict: \t18 POI 20 RE nationality EOP\n",
      "Ground-Truth: \t27 POI 20 RE country EOP\n",
      "---\n",
      "Predict: \t16 POI 18 RE contains EOP\n",
      "Ground-Truth: \t11 POI 18 RE company EOP\n",
      "---\n",
      "Predict: \t15 POI 14 RE contains EOP\n",
      "Ground-Truth: \t18 POI 17 RE contains EOP\n",
      "---\n",
      "Predict: \t13 POI 12 RE contains EOP\n",
      "Ground-Truth: \t21 POI 11 RE contains EOP\n",
      "---\n",
      "Predict: \t8 POI 8 RE contains EOP\n",
      "Ground-Truth: \t29 POI 1 RE contains EOP\n",
      "---\n",
      "Predict: \t18 POI 18 RE contains EOP\n",
      "Ground-Truth: \t21 POI 19 RE contains EOP\n",
      "---\n",
      "Predict: \t24 POI 23 RE contains EOP\n",
      "Ground-Truth: \t27 POI 21 RE contains EOP\n",
      "---\n",
      "Predict: \t6 POI 6 RE contains EOP\n",
      "Ground-Truth: \t7 POI 5 RE contains EOP\n",
      "---\n",
      "Predict: \t8 POI 8 RE contains EOP\n",
      "Ground-Truth: \t10 POI 4 RE country EOP\n",
      "---\n",
      "Predict: \t1 POI 12 RE company EOP\n",
      "Ground-Truth: \t1 POI 4 RE company EOP\n",
      "---\n",
      "Predict: \t32 POI 37 RE contains EOP\n",
      "Ground-Truth: \t44 POI 42 RE contains EOP\n",
      "---\n",
      "Predict: \t2 POI 5 RE place_lived EOP\n",
      "Ground-Truth: \t2 POI 5 RE place_lived EOP\n",
      "---\n",
      "Predict: \t23 POI 23 RE contains EOP\n",
      "Ground-Truth: \t27 POI 24 RE contains EOP\n",
      "---\n",
      "Predict: \t18 POI 18 RE country POI RE capital POI EOP RE\n",
      "Ground-Truth: \t19 POI 20 RE country EOP\n",
      "---\n",
      "Predict: \t6 POI 6 RE contains EOP\n",
      "Ground-Truth: \t7 POI 5 RE contains EOP\n",
      "---\n",
      "Predict: \t27 POI 27 RE contains EOP\n",
      "Ground-Truth: \t27 POI 24 RE contains EOP\n",
      "---\n",
      "Predict: \t12 POI 12 RE contains EOP\n",
      "Ground-Truth: \t13 POI 12 RE contains EOP\n",
      "---\n",
      "Predict: \t13 POI 15 RE company EOP\n",
      "Ground-Truth: \t22 POI 19 RE contains EOP\n",
      "---\n",
      "Predict: \t23 POI 23 RE contains EOP\n",
      "Ground-Truth: \t28 POI 29 RE capital EOP\n",
      "---\n",
      "Predict: \t6 POI 4 RE contains EOP\n",
      "Ground-Truth: \t5 POI 4 RE contains EOP\n",
      "---\n",
      "Predict: \t35 POI 44 RE contains EOP\n",
      "Ground-Truth: \t50 POI 47 RE contains EOP\n",
      "---\n",
      "Predict: \t42 POI 44 RE contains EOP\n",
      "Ground-Truth: \t95 POI 64 RE country EOP\n",
      "---\n",
      "Predict: \t2 POI 1 RE contains EOP\n",
      "Ground-Truth: \t13 POI 11 RE contains EOP\n",
      "---\n",
      "Predict: \t12 POI 12 RE company EOP\n",
      "Ground-Truth: \t8 POI 14 RE company EOP\n",
      "---\n",
      "Predict: \t15 POI 18 RE contains EOP\n",
      "Ground-Truth: \t17 POI 15 RE contains EOP\n",
      "---\n",
      "Predict: \t23 POI 30 RE contains EOP\n",
      "Ground-Truth: \t40 POI 37 RE contains EOP\n",
      "---\n",
      "Predict: \t6 POI 18 RE contains EOP\n",
      "Ground-Truth: \t4 POI 6 RE country POI 6 RE country EOP 4 POI 6 RE country POI 6 RE country EOP\n",
      "---\n",
      "Predict: \t7 POI 8 RE place_lived EOP\n",
      "Ground-Truth: \t4 POI 8 RE place_lived EOP\n",
      "---\n",
      "Predict: \t10 POI 10 RE contains EOP\n",
      "Ground-Truth: \t5 POI 11 RE company EOP\n",
      "---\n",
      "Predict: \t13 POI 15 RE contains EOP\n",
      "Ground-Truth: \t13 POI 14 RE country EOP\n",
      "---\n",
      "Predict: \t8 POI 8 RE contains EOP\n",
      "Ground-Truth: \t6 POI 5 RE contains EOP\n",
      "---\n",
      "Predict: \t16 POI 18 RE contains EOP\n",
      "Ground-Truth: \t21 POI 15 RE contains EOP\n",
      "---\n",
      "Predict: \t28 POI 32 RE contains POI RE EOP\n",
      "Ground-Truth: \t34 POI 33 RE contains EOP\n",
      "---\n",
      "Predict: \t13 POI 12 RE contains EOP\n",
      "Ground-Truth: \t15 POI 11 RE contains EOP\n",
      "---\n",
      "Predict: \t8 POI 8 RE contains EOP\n",
      "Ground-Truth: \t8 POI 17 RE contains EOP\n",
      "---\n",
      "Predict: \t32 POI 32 RE contains EOP\n",
      "Ground-Truth: \t41 POI 42 RE country EOP\n",
      "---\n",
      "Predict: \t1 POI 20 RE contains POI RE capital EOP\n",
      "Ground-Truth: \t21 POI 20 RE contains EOP\n",
      "---\n",
      "Predict: \t13 POI 18 RE contains POI EOP\n",
      "Ground-Truth: \t1 POI 19 RE country EOP\n",
      "---\n",
      "Predict: \t6 POI 6 RE contains EOP\n",
      "Ground-Truth: \t9 POI 7 RE country EOP\n",
      "---\n",
      "Predict: \t6 POI 6 RE country EOP RE capital EOP POI RE POI RE POI\n",
      "Ground-Truth: \t8 POI 7 RE contains EOP\n",
      "---\n",
      "Predict: \t20 POI 20 RE nationality EOP\n",
      "Ground-Truth: \t14 POI 22 RE nationality EOP\n",
      "---\n",
      "Predict: \t23 POI 23 RE nationality EOP\n",
      "Ground-Truth: \t25 POI 24 RE contains EOP\n",
      "---\n",
      "Predict: \t18 POI 18 RE contains EOP\n",
      "Ground-Truth: \t19 POI 18 RE contains EOP\n",
      "---\n",
      "Predict: \t13 POI 14 RE contains EOP\n",
      "Ground-Truth: \t17 POI 11 RE contains EOP\n",
      "---\n",
      "Predict: \t23 POI 23 RE place_lived EOP\n",
      "Ground-Truth: \t15 POI 18 RE place_lived EOP\n",
      "---\n",
      "Predict: \t12 POI 12 RE contains EOP\n",
      "Ground-Truth: \t13 POI 11 RE contains EOP\n",
      "---\n",
      "Predict: \t1 POI 24 RE company EOP\n",
      "Ground-Truth: \t1 POI 14 RE company EOP\n",
      "---\n",
      "Predict: \t1 POI 8 RE company EOP\n",
      "Ground-Truth: \t10 POI 7 RE contains EOP\n",
      "---\n",
      "Predict: \t18 POI 18 RE contains EOP\n",
      "Ground-Truth: \t6 POI 20 RE country EOP 6 POI 20 RE country EOP\n",
      "---\n",
      "Predict: \t12 POI 12 RE contains EOP\n",
      "Ground-Truth: \t16 POI 14 RE contains EOP\n",
      "---\n",
      "Predict: \t16 POI 18 RE contains EOP\n",
      "Ground-Truth: \t20 POI 17 RE contains EOP\n",
      "---\n",
      "Predict: \t1 POI 37 RE contains EOP\n",
      "Ground-Truth: \t17 POI 11 RE contains EOP\n",
      "---\n",
      "Predict: \t28 POI 32 RE contains EOP\n",
      "Ground-Truth: \t34 POI 32 RE contains EOP\n",
      "---\n",
      "Predict: \t13 POI 14 RE contains POI EOP\n",
      "Ground-Truth: \t58 POI 16 RE country EOP\n",
      "---\n",
      "Predict: \t23 POI 23 RE contains EOP\n",
      "Ground-Truth: \t24 POI 13 RE country POI 13 RE country EOP\n",
      "---\n",
      "Predict: \t6 POI 4 RE contains EOP\n",
      "Ground-Truth: \t6 POI 4 RE contains EOP\n",
      "---\n",
      "Predict: \t13 POI 12 RE contains EOP\n",
      "Ground-Truth: \t16 POI 11 RE contains EOP\n",
      "---\n",
      "Predict: \t13 POI 12 RE contains EOP\n",
      "Ground-Truth: \t21 POI 37 RE country EOP 23 POI 37 RE country EOP 21 POI 37 RE country EOP\n",
      "---\n",
      "Predict: \t12 POI 12 RE contains POI RE contains EOP\n",
      "Ground-Truth: \t10 POI 12 RE country POI 12 RE country EOP 10 POI 12 RE country POI 12 RE country EOP\n",
      "---\n",
      "Predict: \t12 POI 12 RE place_lived EOP\n",
      "Ground-Truth: \t9 POI 12 RE place_lived EOP\n",
      "---\n",
      "Predict: \t6 POI 4 RE place_lived EOP\n",
      "Ground-Truth: \t21 POI 25 RE place_lived EOP\n",
      "---\n",
      "Predict: \t6 POI 6 RE contains EOP\n",
      "Ground-Truth: \t7 POI 6 RE contains EOP\n",
      "---\n",
      "Predict: \t10 POI 10 RE contains EOP\n",
      "Ground-Truth: \t14 POI 13 RE contains EOP\n",
      "---\n",
      "Predict: \t1 POI 1 RE nationality EOP\n",
      "Ground-Truth: \t7 POI 1 RE nationality EOP\n",
      "---\n",
      "Predict: \t6 POI 32 RE contains EOP\n",
      "Ground-Truth: \t29 POI 26 RE contains EOP\n",
      "---\n",
      "Predict: \t13 POI 12 RE contains EOP\n",
      "Ground-Truth: \t17 POI 6 RE country EOP\n",
      "---\n",
      "Predict: \t32 POI 32 RE contains EOP\n",
      "Ground-Truth: \t38 POI 13 RE contains EOP\n",
      "---\n",
      "Predict: \t12 POI 12 RE contains EOP\n",
      "Ground-Truth: \t1 POI 14 RE place_lived EOP\n",
      "---\n",
      "Predict: \t2 POI 2 RE contains EOP\n",
      "Ground-Truth: \t18 POI 15 RE contains EOP\n",
      "---\n",
      "Predict: \t1 POI 1 RE nationality POI RE EOP\n",
      "Ground-Truth: \t3 POI 1 RE country EOP\n",
      "---\n",
      "Predict: \t13 POI 12 RE contains EOP\n",
      "Ground-Truth: \t9 POI 5 RE contains EOP\n",
      "---\n",
      "Predict: \t26 POI 24 RE contains EOP\n",
      "Ground-Truth: \t20 POI 28 RE company EOP\n",
      "---\n",
      "Predict: \t20 POI 20 RE nationality EOP\n",
      "Ground-Truth: \t15 POI 23 RE nationality EOP\n",
      "---\n",
      "Predict: \t23 POI 23 RE contains EOP\n",
      "Ground-Truth: \t12 POI 1 RE children EOP\n",
      "---\n",
      "Predict: \t8 POI 8 RE contains EOP\n",
      "Ground-Truth: \t9 POI 8 RE contains EOP\n",
      "---\n",
      "Predict: \t28 POI 32 RE contains POI RE EOP\n",
      "Ground-Truth: \t35 POI 31 RE contains EOP\n",
      "---\n",
      "Predict: \t20 POI 22 RE company EOP\n",
      "Ground-Truth: \t15 POI 23 RE company EOP\n",
      "---\n",
      "Predict: \t8 POI 8 RE contains EOP\n",
      "Ground-Truth: \t8 POI 5 RE contains EOP\n",
      "---\n",
      "Predict: \t2 POI 5 RE place_lived EOP\n",
      "Ground-Truth: \t2 POI 5 RE place_lived EOP\n",
      "---\n",
      "Predict: \t2 POI 8 RE place_lived EOP\n",
      "Ground-Truth: \t1 POI 5 RE place_lived EOP\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "preResult = [' '.join([index2token.get(i, str(i)) for i in sent if i != 0]) for sent in result]\n",
    "actResult = [' '.join([index2token.get(i, str(i)) for i in sent if i != 0]) for sent in y_test]\n",
    "for i in range(395):\n",
    "    print('Predict: \\t%s' % preResult[i])\n",
    "    print('Ground-Truth: \\t%s' % actResult[i])\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{6, 8, 10, 12, 18, 20}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([len(i.split()) for i in actResult])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "84px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
