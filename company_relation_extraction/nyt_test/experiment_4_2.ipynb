{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#GPU\" data-toc-modified-id=\"GPU-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>GPU</a></div><div class=\"lev1 toc-item\"><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Preprocessing</a></div><div class=\"lev2 toc-item\"><a href=\"#Load-Data\" data-toc-modified-id=\"Load-Data-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Load Data</a></div><div class=\"lev3 toc-item\"><a href=\"#Load-Train-Data\" data-toc-modified-id=\"Load-Train-Data-211\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Load Train Data</a></div><div class=\"lev3 toc-item\"><a href=\"#Load-Test-Data\" data-toc-modified-id=\"Load-Test-Data-212\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Load Test Data</a></div><div class=\"lev3 toc-item\"><a href=\"#Relation-Types\" data-toc-modified-id=\"Relation-Types-213\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Relation Types</a></div><div class=\"lev2 toc-item\"><a href=\"#Participle\" data-toc-modified-id=\"Participle-22\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Participle</a></div><div class=\"lev2 toc-item\"><a href=\"#Make-Adjacency-List\" data-toc-modified-id=\"Make-Adjacency-List-23\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Make Adjacency List</a></div><div class=\"lev3 toc-item\"><a href=\"#Make-Adjacency-List-of-Train-Data\" data-toc-modified-id=\"Make-Adjacency-List-of-Train-Data-231\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Make Adjacency List of Train Data</a></div><div class=\"lev3 toc-item\"><a href=\"#Make-Adjacency-List-of-Test-Data\" data-toc-modified-id=\"Make-Adjacency-List-of-Test-Data-232\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Make Adjacency List of Test Data</a></div><div class=\"lev2 toc-item\"><a href=\"#Make-Position-List\" data-toc-modified-id=\"Make-Position-List-24\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Make Position List</a></div><div class=\"lev3 toc-item\"><a href=\"#Make-Position-List-of-Train-Data\" data-toc-modified-id=\"Make-Position-List-of-Train-Data-241\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Make Position List of Train Data</a></div><div class=\"lev3 toc-item\"><a href=\"#Make-Position-List-of-Test-Data\" data-toc-modified-id=\"Make-Position-List-of-Test-Data-242\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>Make Position List of Test Data</a></div><div class=\"lev1 toc-item\"><a href=\"#Word-to-Vector\" data-toc-modified-id=\"Word-to-Vector-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Word to Vector</a></div><div class=\"lev2 toc-item\"><a href=\"#Tokenize-Text\" data-toc-modified-id=\"Tokenize-Text-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Tokenize Text</a></div><div class=\"lev2 toc-item\"><a href=\"#Create-Word-Embeddings-with-GloVe\" data-toc-modified-id=\"Create-Word-Embeddings-with-GloVe-32\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Create Word Embeddings with GloVe</a></div><div class=\"lev3 toc-item\"><a href=\"#Read-GloVe\" data-toc-modified-id=\"Read-GloVe-321\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Read GloVe</a></div><div class=\"lev3 toc-item\"><a href=\"#Use-Glove-to-Initialize-Embedding-Matrix\" data-toc-modified-id=\"Use-Glove-to-Initialize-Embedding-Matrix-322\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Use Glove to Initialize Embedding Matrix</a></div><div class=\"lev1 toc-item\"><a href=\"#Build-Dateset\" data-toc-modified-id=\"Build-Dateset-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Build Dateset</a></div><div class=\"lev1 toc-item\"><a href=\"#Save-Dataset\" data-toc-modified-id=\"Save-Dataset-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Save Dataset</a></div><div class=\"lev1 toc-item\"><a href=\"#Checkpoint\" data-toc-modified-id=\"Checkpoint-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Checkpoint</a></div><div class=\"lev1 toc-item\"><a href=\"#Build-Model\" data-toc-modified-id=\"Build-Model-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Build Model</a></div><div class=\"lev2 toc-item\"><a href=\"#Set-Hyperparameters\" data-toc-modified-id=\"Set-Hyperparameters-71\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Set Hyperparameters</a></div><div class=\"lev2 toc-item\"><a href=\"#Import-Libraries\" data-toc-modified-id=\"Import-Libraries-72\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Import Libraries</a></div><div class=\"lev2 toc-item\"><a href=\"#Build-Graph\" data-toc-modified-id=\"Build-Graph-73\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Build Graph</a></div><div class=\"lev2 toc-item\"><a href=\"#Model-Visualization\" data-toc-modified-id=\"Model-Visualization-74\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Model Visualization</a></div><div class=\"lev2 toc-item\"><a href=\"#Train\" data-toc-modified-id=\"Train-75\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;</span>Train</a></div><div class=\"lev1 toc-item\"><a href=\"#Evaluate\" data-toc-modified-id=\"Evaluate-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Evaluate</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Load Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentText = []\n",
    "relationMentions = []\n",
    "relationLabels = []\n",
    "entityMentions = []\n",
    "entityLabels = []\n",
    "em1Text = []\n",
    "em2Text = []\n",
    "\n",
    "with open(\"/Users/lizhn7/Downloads/DATA/nyt/train.json\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    item = json.loads(line)\n",
    "    # Exclude \"None\" label\n",
    "    if not all(i['label'] == 'None' for i in item['relationMentions']):\n",
    "        sentText.append(item['sentText'])\n",
    "        relationMentions.append(item['relationMentions'])\n",
    "        entityMentions.append(item['entityMentions'])\n",
    "    \n",
    "relationLabels = [[i['label'].split('/')[-1] for i in rM] for rM in relationMentions]\n",
    "entityLabels = [[i['text'] for i in eM] for eM in entityMentions]\n",
    "em1Text = [[i['em1Text'] for i in rM] for rM in relationMentions]\n",
    "em2Text = [[i['em2Text'] for i in rM] for rM in relationMentions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Clean words\n",
    "replaceDict = {\n",
    "               'ā': 'a', 'á': 'a', 'ǎ': 'a', 'à': 'a',\n",
    "               'ō': 'o', 'ó': 'o', 'ǒ': 'o', 'ò': 'o', 'ô': 'o', 'ö': 'o',\n",
    "               'ē': 'e', 'é': 'e', 'ě': 'e', 'è': 'e', \n",
    "               'ī': 'i', 'í': 'i', 'ǐ': 'i', 'ì': 'i',\n",
    "               'ū': 'u', 'ú': 'u', 'ǔ': 'u', 'ù': 'u', 'ü': 'u',\n",
    "               'ñ': 'n',\n",
    "               'É': 'E'\n",
    "              }\n",
    "em1Text = [[''.join([replaceDict.get(i, i) for i in e]) for e in eT] for eT in em1Text]\n",
    "em2Text = [[''.join([replaceDict.get(i, i) for i in e]) for e in eT] for eT in em2Text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t_sentText = []\n",
    "t_relationMentions = []\n",
    "t_relationLabels = []\n",
    "t_entityMentions = []\n",
    "t_entityLabels = []\n",
    "t_em1Text = []\n",
    "t_em2Text = []\n",
    "\n",
    "with open(\"/Users/lizhn7/Downloads/DATA/nyt/test.json\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    item = json.loads(line)\n",
    "    t_sentText.append(item['sentText'])\n",
    "    t_relationMentions.append(item['relationMentions'])\n",
    "    t_entityMentions.append(item['entityMentions'])\n",
    "    \n",
    "t_relationLabels = [[i['label'].split('/')[-1] for i in rM] for rM in t_relationMentions]\n",
    "t_entityLabels = [[i['text'] for i in eM] for eM in t_entityMentions]\n",
    "t_em1Text = [[i['em1Text'] for i in rM] for rM in t_relationMentions]\n",
    "t_em2Text = [[i['em2Text'] for i in rM] for rM in t_relationMentions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Clean words\n",
    "t_replaceDict = {\n",
    "               'ā': 'a', 'á': 'a', 'ǎ': 'a', 'à': 'a',\n",
    "               'ō': 'o', 'ó': 'o', 'ǒ': 'o', 'ò': 'o', 'ô': 'o', 'ö': 'o',\n",
    "               'ē': 'e', 'é': 'e', 'ě': 'e', 'è': 'e', \n",
    "               'ī': 'i', 'í': 'i', 'ǐ': 'i', 'ì': 'i',\n",
    "               'ū': 'u', 'ú': 'u', 'ǔ': 'u', 'ù': 'u', 'ü': 'u',\n",
    "               'ñ': 'n',\n",
    "               'É': 'E'\n",
    "              }\n",
    "t_em1Text = [[''.join([t_replaceDict.get(i, i) for i in e]) for e in eT] for eT in t_em1Text]\n",
    "t_em2Text = [[''.join([t_replaceDict.get(i, i) for i in e]) for e in eT] for eT in t_em2Text]\n",
    "t_entityLabels = [[''.join([t_replaceDict.get(i, i) for i in e]) for e in eT] for eT in t_entityLabels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###  Relation Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['place_of_birth',\n",
       " 'advisors',\n",
       " 'contains',\n",
       " 'location',\n",
       " 'industry',\n",
       " 'place_founded',\n",
       " 'major_shareholder_of',\n",
       " 'geographic_distribution',\n",
       " 'founders',\n",
       " 'company',\n",
       " 'nationality',\n",
       " 'major_shareholders',\n",
       " 'teams',\n",
       " 'religion',\n",
       " 'country',\n",
       " 'profession',\n",
       " 'ethnicity',\n",
       " 'capital',\n",
       " 'administrative_divisions',\n",
       " 'people',\n",
       " 'place_lived',\n",
       " 'children',\n",
       " 'place_of_death',\n",
       " 'neighborhood_of']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relationTypes = list(set([r for rl in relationLabels for r in rl]))\n",
    "relationTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['founders',\n",
       " 'place_of_birth',\n",
       " 'capital',\n",
       " 'company',\n",
       " 'contains',\n",
       " 'nationality',\n",
       " 'administrative_divisions',\n",
       " 'country',\n",
       " 'place_lived',\n",
       " 'children',\n",
       " 'place_of_death',\n",
       " 'neighborhood_of']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_relationTypes = list(set([r for rl in t_relationLabels for r in rl if r != 'None']))\n",
    "t_relationTypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Participle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cut(s):\n",
    "    \"\"\"\n",
    "    Participle\n",
    "    \"\"\"\n",
    "    pattern = r'''\n",
    "              (?x)                   # set flag to allow verbose regexps \n",
    "              (?:[A-Z]\\.)+           # abbreviations, e.g. U.S.A. \n",
    "              |\\d+(?:\\.\\d+)?%?       # numbers, incl. currency and percentages \n",
    "              |\\w+(?:[-&']\\w+)*       # words w/ optional internal hyphens/apostrophe  \n",
    "           '''  \n",
    "    return regexp_tokenize(s, pattern=pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentWords = [cut(s) for s in sentText]\n",
    "entlabWords = [[cut(s) for s in eL] for eL in entityLabels]\n",
    "em1Words = [[cut(s) for s in eL] for eL in em1Text]\n",
    "em2Words = [[cut(s) for s in eL] for eL in em2Text]\n",
    "t_sentWords = [cut(s) for s in t_sentText]\n",
    "t_entlabWords = [[cut(s) for s in eL] for eL in t_entityLabels]\n",
    "t_em1Words = [[cut(s) for s in eL] for eL in t_em1Text]\n",
    "t_em2Words = [[cut(s) for s in eL] for eL in t_em2Text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MAX_SENT_LEN = 120\n",
    "MAX_ADJL_LEN = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Adjacency List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Adjacency List of Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i_entityLabels = []\n",
    "for i in range(len(sentWords)):\n",
    "    eL = []\n",
    "    sDict = list(enumerate(sentWords[i]))\n",
    "    j = 0\n",
    "    for item in entlabWords[i]:\n",
    "        el = []\n",
    "        for e in item:\n",
    "            while j < len(sDict):\n",
    "                if e == sDict[j][1]:\n",
    "                    el.append(sDict[j][0])\n",
    "                    j += 1\n",
    "                    break\n",
    "                j += 1\n",
    "        eL.append(el)\n",
    "    i_entityLabels.append(eL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmSet = []\n",
    "for item in range(len(relationMentions)):\n",
    "    rms = [(em1Words[item][i][0], em2Words[item][i][0], relationLabels[item][i]) for i in range(len(relationLabels[item]))]\n",
    "    rms = [' '.join(list(i)) for i in rms]\n",
    "    rmSet.append(rms)\n",
    "rmSet = [set(i) for i in rmSet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i_adjacencyList = []\n",
    "new_sentWords = []\n",
    "pad = ['POI', 'RE', 'EOP']\n",
    "for n in range(len(sentWords)):\n",
    "    aL = []\n",
    "    for l in range(len(relationLabels[n])):\n",
    "        e1 = []\n",
    "        e2 = []\n",
    "        for item in i_entityLabels[n]:\n",
    "            if ' '.join([sentWords[n][i] for i in item]) == ' '.join(em1Words[n][l]):\n",
    "                e1.append(item[0])\n",
    "            if ' '.join([sentWords[n][i] for i in item]) == ' '.join(em2Words[n][l]):\n",
    "                e2.append(item[0])\n",
    "        c1 = [(a, b) for a in e1 for b in e2 if ' '.join([sentWords[n][a], sentWords[n][b], relationLabels[n][l]]) in rmSet[n]]\n",
    "        c2 = [(a, b) for a in e2 for b in e1 if ' '.join([sentWords[n][b], sentWords[n][a], relationLabels[n][l]]) in rmSet[n]]\n",
    "        c = c1 + c2\n",
    "        m = min([(i, abs(j[0]-j[1])) for i, j in enumerate(c)], key=lambda x: x[-1])[-1]\n",
    "        r = [i[0] for i in [(i, abs(j[0]-j[1])) for i, j in enumerate(c)] if i[-1] == m]\n",
    "        for i in r:\n",
    "            if ' '.join([sentWords[n][c[i][0]], sentWords[n][c[i][1]], relationLabels[n][l]]) in rmSet[n]:\n",
    "                al = (c[i][0], pad[0], c[i][1], pad[1], relationLabels[n][l], pad[2])\n",
    "                if al[4] in t_relationTypes and al[0] < MAX_SENT_LEN and al[2] < MAX_SENT_LEN: \n",
    "                    aL.append(al)\n",
    "    aL = sorted([list(i) for i in set(aL)])\n",
    "    if len(aL) == 1 or len(aL) == 2:\n",
    "        i_adjacencyList.append(aL)\n",
    "        new_sentWords.append(sentWords[n])\n",
    "i_adjacencyList = [sum(i, []) for i in i_adjacencyList]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Make Adjacency List of Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Clean words\n",
    "t_sentWords = [[''.join([t_replaceDict.get(i, i) for i in e]) for e in eT] for eT in t_sentWords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ti_entityLabels = []\n",
    "for i in range(len(t_sentWords)):\n",
    "    eL = []\n",
    "    t_sDict = list(enumerate(t_sentWords[i]))\n",
    "    j = 0\n",
    "    for item in t_entlabWords[i]:\n",
    "        el = []\n",
    "        for e in item:\n",
    "            while j < len(t_sDict):\n",
    "                if e == t_sDict[j][1]:\n",
    "                    el.append(t_sDict[j][0])\n",
    "                    j += 1\n",
    "                    break\n",
    "                j += 1\n",
    "        eL.append(el)\n",
    "    ti_entityLabels.append(eL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ti_trueLables = [[i for i, j in enumerate(rl) if j != 'None'] for rl in t_relationLabels]\n",
    "t_trueMentions = [[j for i, j in enumerate(rl) if j['label'] != 'None'] for rl in t_relationMentions]\n",
    "pad = ['POI', 'RE', 'EOP']\n",
    "ti_adjacencyList = []\n",
    "for n in range(len(t_sentWords)): \n",
    "    e1 = []\n",
    "    e2 = []\n",
    "    aL = []\n",
    "    for l in ti_trueLables[n]:\n",
    "        for item in ti_entityLabels[n]:\n",
    "            if ' '.join([t_sentWords[n][i] for i in item]) == ' '.join(t_em1Words[n][l]):\n",
    "                e1.append(item[0])\n",
    "            if ' '.join([t_sentWords[n][i] for i in item]) == ' '.join(t_em2Words[n][l]):\n",
    "                e2.append(item[0])\n",
    "    c = [(a, b) for a in e1 for b in e2]\n",
    "    r = c[min([(i, abs(j[0]-j[1])) for i, j in enumerate(c)], key=lambda x: x[-1])[0]]\n",
    "    aL = aL + [r[0], pad[0], r[1], pad[1], t_relationLabels[n][l], pad[2]]\n",
    "    ti_adjacencyList.append(aL)\n",
    "\n",
    "# Modify \n",
    "ti_adjacencyList[26] = [34, 'POI', 36, 'RE', 'founders', 'EOP']\n",
    "ti_adjacencyList[191] = [22, 'POI', 36, 'RE', 'country', 'EOP', 33, 'POI', 36, 'RE', 'country', 'EOP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Position List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Position List of Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "postionList = [[i+1 for i, j in enumerate(item)]for item in new_sentWords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Position List of Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_postionList = [[i+1 for i, j in enumerate(item)]for item in t_sentWords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word to Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "sentWords = [[wnl.lemmatize(t.lower()) for t in toks] for toks in new_sentWords]\n",
    "t_sentWords = [[wnl.lemmatize(t.lower()) for t in toks] for toks in t_sentWords]\n",
    "tok_sentWords = sentWords.copy()\n",
    "tok_sentWords.extend(t_sentWords)\n",
    "tokTexts = [' '.join(i) for i in tok_sentWords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 65574 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(filters='')\n",
    "tokenizer.fit_on_texts(tokTexts)\n",
    "word2index = tokenizer.word_index\n",
    "index2word = {i: w for w, i in word2index.items()}\n",
    "print('Found %s unique tokens.' % len(word2index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Word Embeddings with GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 65575\n",
    "EMBEDDING_SIZE = 300\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1917495"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_n_symbols = !wc -l /Users/lizhn7/Downloads/DATA/Glove/glove.42B.300d.txt\n",
    "glove_n_symbols = int(glove_n_symbols[0].split()[0])\n",
    "glove_n_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_index_dict = {}\n",
    "glove_embedding_weights = np.empty((glove_n_symbols, EMBEDDING_SIZE))\n",
    "globale_scale = 0.1\n",
    "with open('/Users/lizhn7/Downloads/DATA/Glove/glove.42B.300d.txt', 'r') as fp:\n",
    "    index = 0\n",
    "    for l in fp:\n",
    "        l = l.strip().split()\n",
    "        word = l[0]\n",
    "        glove_index_dict[word] = index\n",
    "        glove_embedding_weights[index, :] = [float(n) for n in l[1:]]\n",
    "        index += 1\n",
    "glove_embedding_weights *= globale_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Glove to Initialize Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer, LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate random embedding with same scale as glove\n",
    "np.random.seed(SEED)\n",
    "shape = (VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "scale = glove_embedding_weights.std() * np.sqrt(12) / 2 \n",
    "embedding = np.random.uniform(low=-scale, high=scale, size=shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61085-93.15% tokens in vocab found in glove and copied to embedding.\n"
     ]
    }
   ],
   "source": [
    "# Copy from glove weights of words that appear in index2word\n",
    "count = 0 \n",
    "for i in range(1, VOCAB_SIZE):\n",
    "    w = index2word[i]\n",
    "    g = glove_index_dict.get(w)\n",
    "    if g is None:\n",
    "        w = porter.stem(w)\n",
    "        g = glove_index_dict.get(w)\n",
    "    if g is None:\n",
    "        w = lancaster.stem(w)\n",
    "        g = glove_index_dict.get(w)\n",
    "    if g is not None:\n",
    "        embedding[i, :] = glove_embedding_weights[g, :]\n",
    "        count += 1\n",
    "print('{num_tokens}-{per:.2f}% tokens in vocab found in glove and copied to embedding.'.format(num_tokens=count, per=count/float(VOCAB_SIZE)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Dateset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_sentText = [' '.join(i) for i in sentWords]\n",
    "sentSeq = tokenizer.texts_to_sequences(new_sentText)\n",
    "sentData = pad_sequences(sentSeq, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
    "\n",
    "t_new_sentText = [' '.join(i) for i in t_sentWords]\n",
    "t_sentSeq = tokenizer.texts_to_sequences(t_new_sentText)\n",
    "t_sentData = pad_sequences(t_sentSeq, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
    "\n",
    "token2index = dict((j, i+120) for i, j in enumerate(['PAD']+pad+t_relationTypes))\n",
    "token2index['PAD'] = 0\n",
    "index2token = {i: w for w, i in token2index.items()}\n",
    "newi_adjacencyList = [[token2index[i] if i in token2index else i+1 for i in aL] for aL in i_adjacencyList]\n",
    "newi_adjacencyList = pad_sequences(newi_adjacencyList, maxlen=MAX_ADJL_LEN, padding='post', truncating='post')\n",
    "\n",
    "newti_adjacencyList = [[token2index[i] if i in token2index else i+1 for i in aL] for aL in ti_adjacencyList]\n",
    "newti_adjacencyList = pad_sequences(newti_adjacencyList, maxlen=MAX_ADJL_LEN, padding='post', truncating='post')\n",
    "\n",
    "new_postionList = pad_sequences(postionList, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
    "newt_postionList = pad_sequences(t_postionList, maxlen=MAX_SENT_LEN, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data into a training set, a validation set and a test set\n",
    "x_train_all = sentData\n",
    "x_train_p_all = new_postionList\n",
    "y_train_all = newi_adjacencyList\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_all, y_train_all, test_size=0.15, random_state=SEED)\n",
    "x_train_p, x_val_p, _, _ = train_test_split(x_train_p_all, y_train_all, test_size=0.15, random_state=SEED)\n",
    "x_train_all, _, y_train_all, _  = train_test_split(x_train_all, y_train_all, test_size=0., random_state=SEED)\n",
    "x_train_p_all, _, _, _  = train_test_split(x_train_p_all, y_train_all, test_size=0., random_state=SEED)\n",
    " \n",
    "x_test = t_sentData\n",
    "x_test_p = newt_postionList\n",
    "y_test = newti_adjacencyList\n",
    "\n",
    "x_test, _, y_test, _ = train_test_split(x_test, y_test, test_size=0., random_state=SEED)\n",
    "x_test_p, _, _, _ = train_test_split(x_test_p, y_test, test_size=0., random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fh = h5py.File('/Users/lizhn7/Downloads/DATA/nyt/experiment_4_2/allData.h5', 'w')\n",
    "fh['x_train'] = x_train\n",
    "fh['y_train'] = y_train\n",
    "fh['x_train_p'] = x_train_p\n",
    "fh['x_val'] = x_val\n",
    "fh['y_val'] = y_val\n",
    "fh['x_val_p'] = x_val_p\n",
    "fh['x_train_all'] = x_train_all\n",
    "fh['y_train_all'] = y_train_all\n",
    "fh['x_train_p_all'] = x_train_p_all\n",
    "fh['x_test'] = x_test\n",
    "fh['y_test'] = y_test\n",
    "fh['x_test_p'] = x_test_p\n",
    "fh['embedding'] = embedding\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/Users/lizhn7/Downloads/DATA/nyt/experiment_4_2/index.pkl', 'wb') as fp:\n",
    "    pickle.dump((t_sentWords, word2index, index2word, token2index, index2token), fp, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File('/Users/lizhn7/Downloads/DATA/nyt/experiment_4_2/allData.h5', 'r') as fh:\n",
    "    x_train = fh['x_train'][:]\n",
    "    y_train = fh['y_train'][:]\n",
    "    x_train_p = fh['x_train_p'][:]\n",
    "    x_val = fh['x_val'][:]\n",
    "    y_val = fh['y_val'][:]\n",
    "    x_val_p = fh['x_val_p'][:]\n",
    "    x_train_all = fh['x_train_all'][:]\n",
    "    y_train_all = fh['y_train_all'][:]\n",
    "    x_train_p_all = fh['x_train_p_all'][:]\n",
    "    x_test = fh['x_test'][:]\n",
    "    y_test = fh['y_test'][:]\n",
    "    x_test_p = fh['x_test_p'][:]\n",
    "    embedding = fh['embedding'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/Users/lizhn7/Downloads/DATA/nyt/experiment_4_2/index.pkl', 'rb') as fp:\n",
    "    t_sentWords, word2index, index2word, token2index, index2token = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#300-600-0.5-0.2-256-79-20-5\n",
    "MAX_SENT_LEN = 120\n",
    "MAX_ADJL_LEN = 12\n",
    "VOCAB_SIZE = len(word2index)+1\n",
    "POSITION_SIZE = 121\n",
    "NUM_CLASSES = 136\n",
    "EMBEDDING_SIZE = 300\n",
    "POSI_EMBEDDING_SIZE = 15\n",
    "\n",
    "ENC_RNN_SIZE = 300\n",
    "DEC_RNN_SIZE = 600\n",
    "DROPOUT_RATE = 0.5\n",
    "RNN_DROPOUT_RATE = 0.2\n",
    "NUM_EPOCHS = 256\n",
    "BATCH_SIZE = 79\n",
    "STEPS_PER_EPOCH = 20\n",
    "TEST_STEPS = len(x_test)//BATCH_SIZE\n",
    "\n",
    "VALIDATION_STEPS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_EPOCHS: \t\t256\n",
      "STEPS_PER_EPOCH: \t20\n",
      "TEST_STEPS: \t\t5\n",
      "VALIDATION_STEPS: \t3\n",
      "TRAIN_BATCHES: \t\t5120\n",
      "NUM_BATCHES \t\t612\n"
     ]
    }
   ],
   "source": [
    "print('NUM_EPOCHS: \\t\\t%d' % NUM_EPOCHS)\n",
    "print('STEPS_PER_EPOCH: \\t%d' % STEPS_PER_EPOCH)\n",
    "print('TEST_STEPS: \\t\\t%d' % TEST_STEPS)\n",
    "print('VALIDATION_STEPS: \\t%d' % VALIDATION_STEPS)\n",
    "print('TRAIN_BATCHES: \\t\\t%d' % (NUM_EPOCHS * STEPS_PER_EPOCH))\n",
    "print('NUM_BATCHES \\t\\t%d' % (len(x_train)//BATCH_SIZE+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Embedding, Dropout, TimeDistributed, concatenate, Bidirectional, LSTM, RepeatVector, Dense\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.callbacks import*\n",
    "from keras.utils import to_categorical\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "sequence = Input(shape=(MAX_SENT_LEN,), name='INPUT') \n",
    "position = Input(shape=(MAX_SENT_LEN,), name='POSI_INPUT')\n",
    "emb_seq = Embedding(VOCAB_SIZE, EMBEDDING_SIZE, weights=[embedding], mask_zero=True, input_length=MAX_SENT_LEN, trainable=True, name='EMBEDDING')(sequence)\n",
    "emb_posi = Embedding(POSITION_SIZE, POSI_EMBEDDING_SIZE, mask_zero=True, input_length=MAX_SENT_LEN, name='POSI_EMBEDDING')(position)\n",
    "emb_seq = Dropout(DROPOUT_RATE)(emb_seq)\n",
    "emb_posi = Dropout(DROPOUT_RATE)(emb_posi)\n",
    "emb_seq = concatenate([emb_seq, emb_posi], axis=-1)\n",
    "blstm = Bidirectional(LSTM(ENC_RNN_SIZE, return_sequences=True, implementation=0, dropout=RNN_DROPOUT_RATE, recurrent_dropout=RNN_DROPOUT_RATE), merge_mode='concat', name='ENC_BLSTM_1')(emb_seq)\n",
    "blstm = Dropout(DROPOUT_RATE)(blstm)\n",
    "blstm = Bidirectional(LSTM(ENC_RNN_SIZE, return_sequences=False, implementation=0, dropout=RNN_DROPOUT_RATE, recurrent_dropout=RNN_DROPOUT_RATE), merge_mode='concat', name='ENC_BLSTM_2')(blstm)\n",
    "blstm = Dropout(DROPOUT_RATE)(blstm)\n",
    "context = RepeatVector(MAX_ADJL_LEN, name='CONTEXT')(blstm)\n",
    "lstm = LSTM(DEC_RNN_SIZE, return_sequences=True, implementation=0, dropout=RNN_DROPOUT_RATE, recurrent_dropout=RNN_DROPOUT_RATE, name='DEC_LSTM')(context)\n",
    "lstm = Dropout(DROPOUT_RATE)(lstm)\n",
    "output = TimeDistributed(Dense(NUM_CLASSES, activation='softmax'), name='OUTPUT')(lstm)\n",
    "model = Model(inputs=[sequence, position], outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "INPUT (InputLayer)               (None, 120)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "POSI_INPUT (InputLayer)          (None, 120)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "EMBEDDING (Embedding)            (None, 120, 300)      19672500    INPUT[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "POSI_EMBEDDING (Embedding)       (None, 120, 15)       1815        POSI_INPUT[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 120, 300)      0           EMBEDDING[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 120, 15)       0           POSI_EMBEDDING[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 120, 315)      0           dropout_1[0][0]                  \n",
      "                                                                   dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "ENC_BLSTM_1 (Bidirectional)      (None, 120, 600)      1478400     concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 120, 600)      0           ENC_BLSTM_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "ENC_BLSTM_2 (Bidirectional)      (None, 600)           2162400     dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 600)           0           ENC_BLSTM_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "CONTEXT (RepeatVector)           (None, 12, 600)       0           dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "DEC_LSTM (LSTM)                  (None, 12, 600)       2882400     CONTEXT[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 12, 600)       0           DEC_LSTM[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "OUTPUT (TimeDistributed)         (None, 12, 136)       81736       dropout_5[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 26,279,251\n",
      "Trainable params: 26,279,251\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"848pt\" viewBox=\"0.00 0.00 407.60 848.00\" width=\"408pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 844)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-844 403.6001,-844 403.6001,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 5019097520 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>5019097520</title>\n",
       "<polygon fill=\"none\" points=\"22.9106,-803.5 22.9106,-839.5 149.0171,-839.5 149.0171,-803.5 22.9106,-803.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.9639\" y=\"-817.3\">INPUT: InputLayer</text>\n",
       "</g>\n",
       "<!-- 5019098304 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>5019098304</title>\n",
       "<polygon fill=\"none\" points=\"0,-730.5 0,-766.5 171.9277,-766.5 171.9277,-730.5 0,-730.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.9639\" y=\"-744.3\">EMBEDDING: Embedding</text>\n",
       "</g>\n",
       "<!-- 5019097520&#45;&gt;5019098304 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>5019097520-&gt;5019098304</title>\n",
       "<path d=\"M85.9639,-803.4551C85.9639,-795.3828 85.9639,-785.6764 85.9639,-776.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"89.464,-776.5903 85.9639,-766.5904 82.464,-776.5904 89.464,-776.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5019098192 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>5019098192</title>\n",
       "<polygon fill=\"none\" points=\"213.2383,-803.5 213.2383,-839.5 376.6895,-839.5 376.6895,-803.5 213.2383,-803.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"294.9639\" y=\"-817.3\">POSI_INPUT: InputLayer</text>\n",
       "</g>\n",
       "<!-- 5019098136 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>5019098136</title>\n",
       "<polygon fill=\"none\" points=\"190.3276,-730.5 190.3276,-766.5 399.6001,-766.5 399.6001,-730.5 190.3276,-730.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"294.9639\" y=\"-744.3\">POSI_EMBEDDING: Embedding</text>\n",
       "</g>\n",
       "<!-- 5019098192&#45;&gt;5019098136 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>5019098192-&gt;5019098136</title>\n",
       "<path d=\"M294.9639,-803.4551C294.9639,-795.3828 294.9639,-785.6764 294.9639,-776.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"298.464,-776.5903 294.9639,-766.5904 291.464,-776.5904 298.464,-776.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5019098584 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>5019098584</title>\n",
       "<polygon fill=\"none\" points=\"53.1621,-657.5 53.1621,-693.5 180.7656,-693.5 180.7656,-657.5 53.1621,-657.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"116.9639\" y=\"-671.3\">dropout_1: Dropout</text>\n",
       "</g>\n",
       "<!-- 5019098304&#45;&gt;5019098584 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>5019098304-&gt;5019098584</title>\n",
       "<path d=\"M93.6268,-730.4551C97.1665,-722.1196 101.4462,-712.0416 105.3689,-702.8042\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"108.5944,-704.1629 109.2817,-693.5904 102.1513,-701.4267 108.5944,-704.1629\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5019098472 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>5019098472</title>\n",
       "<polygon fill=\"none\" points=\"215.1621,-657.5 215.1621,-693.5 342.7656,-693.5 342.7656,-657.5 215.1621,-657.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"278.9639\" y=\"-671.3\">dropout_2: Dropout</text>\n",
       "</g>\n",
       "<!-- 5019098136&#45;&gt;5019098472 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>5019098136-&gt;5019098472</title>\n",
       "<path d=\"M291.0088,-730.4551C289.2203,-722.2951 287.0658,-712.4652 285.0765,-703.3887\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"288.4888,-702.6091 282.9289,-693.5904 281.6511,-704.1079 288.4888,-702.6091\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5019921488 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>5019921488</title>\n",
       "<polygon fill=\"none\" points=\"103.6445,-584.5 103.6445,-620.5 276.2832,-620.5 276.2832,-584.5 103.6445,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"189.9639\" y=\"-598.3\">concatenate_1: Concatenate</text>\n",
       "</g>\n",
       "<!-- 5019098584&#45;&gt;5019921488 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>5019098584-&gt;5019921488</title>\n",
       "<path d=\"M135.0088,-657.4551C143.9584,-648.5054 154.9169,-637.547 164.6869,-627.7769\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"167.2773,-630.1363 171.8735,-620.5904 162.3275,-625.1866 167.2773,-630.1363\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5019098472&#45;&gt;5019921488 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>5019098472-&gt;5019921488</title>\n",
       "<path d=\"M256.9639,-657.4551C245.8387,-648.3299 232.1675,-637.1165 220.082,-627.2036\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"221.9707,-624.2261 212.0192,-620.5904 217.5314,-629.6384 221.9707,-624.2261\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5335312816 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>5335312816</title>\n",
       "<polygon fill=\"none\" points=\"46.8345,-511.5 46.8345,-547.5 333.0933,-547.5 333.0933,-511.5 46.8345,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"189.9639\" y=\"-525.3\">ENC_BLSTM_1(lstm_1): Bidirectional(LSTM)</text>\n",
       "</g>\n",
       "<!-- 5019921488&#45;&gt;5335312816 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>5019921488-&gt;5335312816</title>\n",
       "<path d=\"M189.9639,-584.4551C189.9639,-576.3828 189.9639,-566.6764 189.9639,-557.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"193.464,-557.5903 189.9639,-547.5904 186.464,-557.5904 193.464,-557.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5019851128 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>5019851128</title>\n",
       "<polygon fill=\"none\" points=\"126.1621,-438.5 126.1621,-474.5 253.7656,-474.5 253.7656,-438.5 126.1621,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"189.9639\" y=\"-452.3\">dropout_3: Dropout</text>\n",
       "</g>\n",
       "<!-- 5335312816&#45;&gt;5019851128 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>5335312816-&gt;5019851128</title>\n",
       "<path d=\"M189.9639,-511.4551C189.9639,-503.3828 189.9639,-493.6764 189.9639,-484.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"193.464,-484.5903 189.9639,-474.5904 186.464,-484.5904 193.464,-484.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5449337152 -->\n",
       "<g class=\"node\" id=\"node10\">\n",
       "<title>5449337152</title>\n",
       "<polygon fill=\"none\" points=\"46.8345,-365.5 46.8345,-401.5 333.0933,-401.5 333.0933,-365.5 46.8345,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"189.9639\" y=\"-379.3\">ENC_BLSTM_2(lstm_2): Bidirectional(LSTM)</text>\n",
       "</g>\n",
       "<!-- 5019851128&#45;&gt;5449337152 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>5019851128-&gt;5449337152</title>\n",
       "<path d=\"M189.9639,-438.4551C189.9639,-430.3828 189.9639,-420.6764 189.9639,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"193.464,-411.5903 189.9639,-401.5904 186.464,-411.5904 193.464,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5335424136 -->\n",
       "<g class=\"node\" id=\"node11\">\n",
       "<title>5335424136</title>\n",
       "<polygon fill=\"none\" points=\"126.1621,-292.5 126.1621,-328.5 253.7656,-328.5 253.7656,-292.5 126.1621,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"189.9639\" y=\"-306.3\">dropout_4: Dropout</text>\n",
       "</g>\n",
       "<!-- 5449337152&#45;&gt;5335424136 -->\n",
       "<g class=\"edge\" id=\"edge10\">\n",
       "<title>5449337152-&gt;5335424136</title>\n",
       "<path d=\"M189.9639,-365.4551C189.9639,-357.3828 189.9639,-347.6764 189.9639,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"193.464,-338.5903 189.9639,-328.5904 186.464,-338.5904 193.464,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5449679928 -->\n",
       "<g class=\"node\" id=\"node12\">\n",
       "<title>5449679928</title>\n",
       "<polygon fill=\"none\" points=\"108.252,-219.5 108.252,-255.5 271.6758,-255.5 271.6758,-219.5 108.252,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"189.9639\" y=\"-233.3\">CONTEXT: RepeatVector</text>\n",
       "</g>\n",
       "<!-- 5335424136&#45;&gt;5449679928 -->\n",
       "<g class=\"edge\" id=\"edge11\">\n",
       "<title>5335424136-&gt;5449679928</title>\n",
       "<path d=\"M189.9639,-292.4551C189.9639,-284.3828 189.9639,-274.6764 189.9639,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"193.464,-265.5903 189.9639,-255.5904 186.464,-265.5904 193.464,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5465308240 -->\n",
       "<g class=\"node\" id=\"node13\">\n",
       "<title>5465308240</title>\n",
       "<polygon fill=\"none\" points=\"123.4312,-146.5 123.4312,-182.5 256.4966,-182.5 256.4966,-146.5 123.4312,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"189.9639\" y=\"-160.3\">DEC_LSTM: LSTM</text>\n",
       "</g>\n",
       "<!-- 5449679928&#45;&gt;5465308240 -->\n",
       "<g class=\"edge\" id=\"edge12\">\n",
       "<title>5449679928-&gt;5465308240</title>\n",
       "<path d=\"M189.9639,-219.4551C189.9639,-211.3828 189.9639,-201.6764 189.9639,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"193.464,-192.5903 189.9639,-182.5904 186.464,-192.5904 193.464,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5465307568 -->\n",
       "<g class=\"node\" id=\"node14\">\n",
       "<title>5465307568</title>\n",
       "<polygon fill=\"none\" points=\"126.1621,-73.5 126.1621,-109.5 253.7656,-109.5 253.7656,-73.5 126.1621,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"189.9639\" y=\"-87.3\">dropout_5: Dropout</text>\n",
       "</g>\n",
       "<!-- 5465308240&#45;&gt;5465307568 -->\n",
       "<g class=\"edge\" id=\"edge13\">\n",
       "<title>5465308240-&gt;5465307568</title>\n",
       "<path d=\"M189.9639,-146.4551C189.9639,-138.3828 189.9639,-128.6764 189.9639,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"193.464,-119.5903 189.9639,-109.5904 186.464,-119.5904 193.464,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 5183412096 -->\n",
       "<g class=\"node\" id=\"node15\">\n",
       "<title>5183412096</title>\n",
       "<polygon fill=\"none\" points=\"55,-.5 55,-36.5 324.9277,-36.5 324.9277,-.5 55,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"189.9639\" y=\"-14.3\">OUTPUT(dense_1): TimeDistributed(Dense)</text>\n",
       "</g>\n",
       "<!-- 5465307568&#45;&gt;5183412096 -->\n",
       "<g class=\"edge\" id=\"edge14\">\n",
       "<title>5465307568-&gt;5183412096</title>\n",
       "<path d=\"M189.9639,-73.4551C189.9639,-65.3828 189.9639,-55.6764 189.9639,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"193.464,-46.5903 189.9639,-36.5904 186.464,-46.5904 193.464,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_label(s):\n",
    "    \"\"\"\n",
    "    One-hot encoding\n",
    "    \"\"\"\n",
    "    gen = to_categorical(s, num_classes=NUM_CLASSES)\n",
    "    return gen\n",
    "\n",
    "def data_generator_all(data, label, batch_size):\n",
    "    \"\"\"\n",
    "    Yield batches of all data\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    while True:\n",
    "        if count >= len(data[0]): \n",
    "            count = 0\n",
    "        x_1 = np.zeros((batch_size, MAX_SENT_LEN))\n",
    "        x_2 = np.zeros((batch_size, MAX_SENT_LEN))\n",
    "        y = np.zeros((batch_size, MAX_ADJL_LEN, NUM_CLASSES))\n",
    "        for i in range(batch_size):\n",
    "            n = i + count\n",
    "            if n > len(data[0])-1:\n",
    "                break\n",
    "            x_1[i, :] = data[0][n]\n",
    "            x_2[i, :] = data[1][n]\n",
    "            y[i, :, :] = gen_label(label[n])\n",
    "        count += batch_size\n",
    "        yield ([x_1, x_2], y)\n",
    "        \n",
    "def data_generator(data, label, batch_size): \n",
    "    \"\"\"\n",
    "    Yield batches \n",
    "    \"\"\"\n",
    "    index = np.arange(len(data[0]))\n",
    "    np.random.shuffle(index)    \n",
    "    batches = [index[range(batch_size*i, min(len(data[0]), batch_size*(i+1)))] for i in range(len(data[0])//batch_size)]\n",
    "    while True:\n",
    "        for i in batches:\n",
    "            x_1 = data[0][i]\n",
    "            x_2 = data[1][i]\n",
    "            y = np.array(list(map(gen_label, label[i])))\n",
    "            yield ([x_1, x_2], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_train_all = data_generator([x_train_all, x_train_p_all], y_train_all, BATCH_SIZE)\n",
    "gen_test = data_generator_all([x_test, x_test_p], y_test, BATCH_SIZE)\n",
    "#gen_train = data_generator(x_train, y_train, BATCH_SIZE)\n",
    "#gen_val = data_generator(x_val, y_val, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Continue Trian\n",
    "# filename = 'cp_logs/.hdf5'\n",
    "# model.load_weights(filename)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = '/Users/lizhn7/Downloads/DATA/nyt/experiment_4_2/cp_logs/weights.{epoch:03d}-{val_loss:.6f}.hdf5'\n",
    "log_string = '/Users/lizhn7/Downloads/DATA/nyt/experiment_4_2/tb_logs/300-600-0.5-0.2-256-79-20-5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "tensorboard = TensorBoard(log_dir=log_string, \n",
    "                          histogram_freq=1, \n",
    "                          write_graph=False, \n",
    "                          write_grads=False, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          write_images=True, \n",
    "                          embeddings_freq=1, \n",
    "                          embeddings_layer_names=None,\n",
    "                          embeddings_metadata=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(gen_train_all, \n",
    "                              steps_per_epoch=STEPS_PER_EPOCH, \n",
    "                              epochs=NUM_EPOCHS, \n",
    "                              verbose=1,\n",
    "                              #callbacks=[checkpoint, tensorboard],\n",
    "                              validation_data=gen_test, \n",
    "                              validation_steps=TEST_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = '/Users/lizhn7/Downloads/DATA/nyt/experiment_4_2/cp_logs/weights.128-0.795735.hdf5'\n",
    "model.load_weights(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395/395 [==============================] - 16s    \n"
     ]
    }
   ],
   "source": [
    "result = np.argmax(model.predict([x_test, x_test_p], batch_size=BATCH_SIZE, verbose=1), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1 Accuracy: \t\t0.275949\n",
      "E2 Accuracy: \t\t0.192405\n",
      "En Accuracy: \t\t0.065823\n",
      "Triple Accuracy: \t0.060759\n"
     ]
    }
   ],
   "source": [
    "preResult = []\n",
    "actResult = []\n",
    "for n in range(395):\n",
    "    pR = [index2token.get(i, str(i)) if i > len(t_sentWords[n]) else t_sentWords[n][i-1] for i in result[n] if i != 0]\n",
    "    aR = [index2token.get(i, str(i)) if i > len(t_sentWords[n]) else t_sentWords[n][i-1] for i in y_test[n] if i != 0]\n",
    "    preResult.append(pR)\n",
    "    actResult.append(aR)\n",
    "e1Count = sum([1 for i in range(395) if actResult[i][0] == preResult[i][0]])\n",
    "e2Count = sum([1 for i in range(395) if actResult[i][2] == preResult[i][2]])\n",
    "enCount = sum([1 for i in range(395) if actResult[i][0] == preResult[i][0] and actResult[i][2] == preResult[i][2]])\n",
    "tripleCount = sum([1 for i in range(395) if actResult[i][0] == preResult[i][0] and actResult[i][2] == preResult[i][2] and actResult[i][4] == preResult[i][4]])\n",
    "print('E1 Accuracy: \\t\\t%.6f' % (e1Count/395))\n",
    "print('E2 Accuracy: \\t\\t%.6f' % (e2Count/395))\n",
    "print('En Accuracy: \\t\\t%.6f' % (enCount/395))\n",
    "print('Triple Accuracy: \\t%.6f' % (tripleCount/395))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1 Accuracy: \t\t0.245570\n",
      "E2 Accuracy: \t\t0.232911\n",
      "En Accuracy: \t\t0.063291\n",
      "Triple Accuracy: \t0.055696\n"
     ]
    }
   ],
   "source": [
    "preResult = []\n",
    "actResult = []\n",
    "for n in range(395):\n",
    "    pR = [index2token.get(i, str(i)) if i > len(t_sentWords[n]) else t_sentWords[n][i-1] for i in result[n] if i != 0]\n",
    "    aR = [index2token.get(i, str(i)) if i > len(t_sentWords[n]) else t_sentWords[n][i-1] for i in y_test[n] if i != 0]\n",
    "    preResult.append(pR)\n",
    "    actResult.append(aR)\n",
    "e1Count = sum([1 for i in range(395) if actResult[i][0] == preResult[i][0]])\n",
    "e2Count = sum([1 for i in range(395) if actResult[i][2] == preResult[i][2]])\n",
    "enCount = sum([1 for i in range(395) if actResult[i][0] == preResult[i][0] and actResult[i][2] == preResult[i][2]])\n",
    "tripleCount = sum([1 for i in range(395) if actResult[i][0] == preResult[i][0] and actResult[i][2] == preResult[i][2] and actResult[i][4] == preResult[i][4]])\n",
    "print('E1 Accuracy: \\t\\t%.6f' % (e1Count/395))\n",
    "print('E2 Accuracy: \\t\\t%.6f' % (e2Count/395))\n",
    "print('En Accuracy: \\t\\t%.6f' % (enCount/395))\n",
    "print('Triple Accuracy: \\t%.6f' % (tripleCount/395))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict: \tdepartment POI department RE contains EOP\n",
      "Ground-Truth: \thealth POI department RE country EOP\n",
      "---\n",
      "Predict: \thad POI had RE contains EOP\n",
      "Ground-Truth: \thad POI wounded RE country EOP\n",
      "---\n",
      "Predict: \there POI senator RE capital EOP POI RE contains EOP\n",
      "Ground-Truth: \tsenator POI nelson RE nationality EOP\n",
      "---\n",
      "Predict: \thome POI into RE contains EOP\n",
      "Ground-Truth: \tinto POI coping RE contains EOP\n",
      "---\n",
      "Predict: \tdeeper POI engagement RE place_lived EOP\n",
      "Ground-Truth: \tofficial POI deeper RE place_lived EOP\n",
      "---\n",
      "Predict: \tmalcom POI eli RE contains EOP\n",
      "Ground-Truth: \tmalcom POI eli RE contains EOP\n",
      "---\n",
      "Predict: \tit POI bragdon RE company EOP\n",
      "Ground-Truth: \tgeneral POI it RE founders EOP\n",
      "---\n",
      "Predict: \tofficial POI official RE contains EOP\n",
      "Ground-Truth: \ta POI official RE country EOP\n",
      "---\n",
      "Predict: \tromer POI rochester RE contains EOP\n",
      "Ground-Truth: \tresidency POI grant RE country EOP\n",
      "---\n",
      "Predict: \tare POI by RE nationality EOP\n",
      "Ground-Truth: \tthe POI appearing RE contains EOP\n",
      "---\n",
      "Predict: \tsunni POI it RE place_lived EOP\n",
      "Ground-Truth: \tit POI ally RE company EOP\n",
      "---\n",
      "Predict: \tstool POI three-legged RE contains EOP\n",
      "Ground-Truth: \tstool POI three-legged RE contains EOP\n",
      "---\n",
      "Predict: \tindian POI indian RE contains EOP\n",
      "Ground-Truth: \tairline POI largest RE contains EOP\n",
      "---\n",
      "Predict: \tand POI indonesia RE place_lived EOP\n",
      "Ground-Truth: \tand POI 18 RE place_lived EOP\n",
      "---\n",
      "Predict: \tbut POI in RE company EOP\n",
      "Ground-Truth: \tsaid POI take RE place_lived EOP\n",
      "---\n",
      "Predict: \tof POI australia RE contains EOP\n",
      "Ground-Truth: \tthe POI australia RE country EOP\n",
      "---\n",
      "Predict: \tirish POI irish RE company EOP\n",
      "Ground-Truth: \tis POI it RE company EOP\n",
      "---\n",
      "Predict: \tbritish POI british RE contains EOP\n",
      "Ground-Truth: \ta POI british RE country EOP\n",
      "---\n",
      "Predict: \tflorida POI florida RE contains EOP\n",
      "Ground-Truth: \twhere POI sale RE country EOP\n",
      "---\n",
      "Predict: \teuropean POI france RE place_lived EOP\n",
      "Ground-Truth: \t10 POI by RE company EOP\n",
      "---\n",
      "Predict: \tdy POI died RE nationality EOP\n",
      "Ground-Truth: \tharrelson POI died RE nationality EOP\n",
      "---\n",
      "Predict: \tsunni POI who RE company EOP\n",
      "Ground-Truth: \ttroop POI on RE company EOP\n",
      "---\n",
      "Predict: \tparis POI the RE company EOP\n",
      "Ground-Truth: \tin POI northwest RE founders EOP\n",
      "---\n",
      "Predict: \tnorthern POI laden RE contains EOP\n",
      "Ground-Truth: \tnorthern POI monday RE place_lived EOP\n",
      "---\n",
      "Predict: \t24 POI 28 RE place_lived EOP\n",
      "Ground-Truth: \tcounty POI 31 RE contains EOP\n",
      "---\n",
      "Predict: \t18 POI korea RE contains EOP\n",
      "Ground-Truth: \t18 POI in RE country EOP\n",
      "---\n",
      "Predict: \tand POI and RE contains EOP\n",
      "Ground-Truth: \tfamily POI and RE country EOP\n",
      "---\n",
      "Predict: \tfrom POI from RE nationality EOP\n",
      "Ground-Truth: \trob POI calderon RE place_lived EOP\n",
      "---\n",
      "Predict: \tbirmingham POI birmingham RE contains EOP\n",
      "Ground-Truth: \tamerican POI in RE company EOP\n",
      "---\n",
      "Predict: \tin POI in RE capital EOP POI RE contains EOP\n",
      "Ground-Truth: \t29 POI in RE country EOP\n",
      "---\n",
      "Predict: \tmerger POI merger RE contains EOP\n",
      "Ground-Truth: \tmerger POI at&t RE contains EOP\n",
      "---\n",
      "Predict: \tzone POI flood RE nationality EOP\n",
      "Ground-Truth: \ton POI slightly RE nationality EOP\n",
      "---\n",
      "Predict: \ttime POI parent RE contains EOP\n",
      "Ground-Truth: \tgive POI time RE country EOP\n",
      "---\n",
      "Predict: \twa POI joy RE company EOP\n",
      "Ground-Truth: \twa POI 17 RE company EOP\n",
      "---\n",
      "Predict: \t27 POI 26 RE contains EOP\n",
      "Ground-Truth: \t26 POI 24 RE contains EOP\n",
      "---\n",
      "Predict: \t42 POI 44 RE contains EOP\n",
      "Ground-Truth: \t38 POI 43 RE company EOP\n",
      "---\n",
      "Predict: \tm POI syria RE company EOP\n",
      "Ground-Truth: \t1971 POI birthday RE company EOP\n",
      "---\n",
      "Predict: \tis POI is RE contains EOP\n",
      "Ground-Truth: \tstart POI is RE contains EOP\n",
      "---\n",
      "Predict: \tseeing POI dramatic RE contains EOP\n",
      "Ground-Truth: \truling POI new RE contains EOP\n",
      "---\n",
      "Predict: \tsaid POI of RE contains EOP\n",
      "Ground-Truth: \tsaid POI nebraska RE contains EOP\n",
      "---\n",
      "Predict: \tof POI investor RE contains EOP\n",
      "Ground-Truth: \tlead POI in RE country EOP\n",
      "---\n",
      "Predict: \tcame POI a RE place_lived EOP\n",
      "Ground-Truth: \tfrom POI a RE place_lived EOP\n",
      "---\n",
      "Predict: \tlast POI last RE nationality EOP\n",
      "Ground-Truth: \t400 POI last RE country EOP\n",
      "---\n",
      "Predict: \tsigned POI after RE contains EOP\n",
      "Ground-Truth: \t42 POI the RE country EOP\n",
      "---\n",
      "Predict: \tcrossword POI york RE contains EOP\n",
      "Ground-Truth: \tof POI employ RE place_of_birth EOP\n",
      "---\n",
      "Predict: \thad POI had RE contains EOP\n",
      "Ground-Truth: \t38 POI 41 RE nationality EOP\n",
      "---\n",
      "Predict: \tof POI zapatero RE nationality EOP\n",
      "Ground-Truth: \tjose POI spain RE nationality EOP\n",
      "---\n",
      "Predict: \t29 POI 30 RE contains EOP\n",
      "Ground-Truth: \tmichoacan POI 30 RE country EOP\n",
      "---\n",
      "Predict: \tcompany POI 15 RE place_lived EOP\n",
      "Ground-Truth: \thave POI mile RE place_lived EOP\n",
      "---\n",
      "Predict: \t16 POI 16 RE contains EOP\n",
      "Ground-Truth: \t17 POI 11 RE contains EOP\n",
      "---\n",
      "Predict: \tnonprofit POI founded RE place_lived EOP\n",
      "Ground-Truth: \tproject POI founded RE place_lived EOP\n",
      "---\n",
      "Predict: \tdocumentarians POI the RE contains EOP\n",
      "Ground-Truth: \tsarin POI named RE country EOP\n",
      "---\n",
      "Predict: \t29 POI 30 RE contains EOP\n",
      "Ground-Truth: \tisrael POI the RE company EOP\n",
      "---\n",
      "Predict: \tjewish POI jewish RE contains EOP\n",
      "Ground-Truth: \tworld POI asked RE contains EOP\n",
      "---\n",
      "Predict: \tmiikka POI message RE nationality EOP\n",
      "Ground-Truth: \tmessage POI kizor RE contains EOP\n",
      "---\n",
      "Predict: \tsanskrit POI in RE contains EOP\n",
      "Ground-Truth: \tin POI sanskrit RE country EOP\n",
      "---\n",
      "Predict: \tthe POI israel RE capital EOP POI RE contains EOP\n",
      "Ground-Truth: \tthe POI of RE contains EOP\n",
      "---\n",
      "Predict: \tred POI terry RE contains EOP\n",
      "Ground-Truth: \tterry POI evans RE contains EOP\n",
      "---\n",
      "Predict: \ts POI s RE contains EOP\n",
      "Ground-Truth: \t400 POI northern RE country EOP\n",
      "---\n",
      "Predict: \tpotential POI for RE contains EOP\n",
      "Ground-Truth: \tboth POI insoluble RE contains EOP\n",
      "---\n",
      "Predict: \trockville POI rockville RE contains EOP POI RE contains EOP\n",
      "Ground-Truth: \tn.y. POI pub RE contains EOP\n",
      "---\n",
      "Predict: \tin POI 40 RE contains EOP\n",
      "Ground-Truth: \t38 POI 39 RE country EOP\n",
      "---\n",
      "Predict: \tmr POI trying RE company EOP mr POI mr RE founders EOP\n",
      "Ground-Truth: \tmr POI to RE company EOP\n",
      "---\n",
      "Predict: \tand POI city RE contains EOP\n",
      "Ground-Truth: \tand POI includes RE contains EOP\n",
      "---\n",
      "Predict: \ton POI the RE place_lived EOP\n",
      "Ground-Truth: \tthe POI 18 RE place_lived EOP\n",
      "---\n",
      "Predict: \tceltic POI in RE contains EOP\n",
      "Ground-Truth: \tceltic POI 1 RE contains EOP\n",
      "---\n",
      "Predict: \ttourist POI a RE contains EOP\n",
      "Ground-Truth: \ttourist POI marlene RE contains EOP\n",
      "---\n",
      "Predict: \ttheir POI unbuilding RE nationality EOP\n",
      "Ground-Truth: \tgrow POI unbuilding RE company EOP\n",
      "---\n",
      "Predict: \talso POI also RE contains EOP\n",
      "Ground-Truth: \tvoted POI iowa RE contains EOP\n",
      "---\n",
      "Predict: \tmeet POI will RE contains EOP\n",
      "Ground-Truth: \tmeet POI he RE contains EOP\n",
      "---\n",
      "Predict: \tisrael POI israel RE contains EOP\n",
      "Ground-Truth: \tstrictly POI of RE country EOP\n",
      "---\n",
      "Predict: \teight-hour POI after RE contains EOP\n",
      "Ground-Truth: \teight-hour POI last RE contains EOP\n",
      "---\n",
      "Predict: \tthen POI and RE contains EOP\n",
      "Ground-Truth: \tto POI began RE place_lived EOP\n",
      "---\n",
      "Predict: \tbarbara POI lrb RE company EOP\n",
      "Ground-Truth: \tny POI height RE company EOP\n",
      "---\n",
      "Predict: \tfor POI preschool RE place_lived EOP\n",
      "Ground-Truth: \tfor POI for RE place_lived EOP\n",
      "---\n",
      "Predict: \tthough POI though RE nationality EOP\n",
      "Ground-Truth: \the POI need RE nationality EOP\n",
      "---\n",
      "Predict: \tparliament POI parliament RE nationality EOP\n",
      "Ground-Truth: \tto POI of RE nationality EOP\n",
      "---\n",
      "Predict: \tvillage POI many RE contains EOP\n",
      "Ground-Truth: \tfloodplain POI 35 RE contains EOP\n",
      "---\n",
      "Predict: \tsurvey POI 19 RE contains EOP\n",
      "Ground-Truth: \t19 POI it RE contains EOP\n",
      "---\n",
      "Predict: \tlive POI 32 RE contains EOP\n",
      "Ground-Truth: \t31 POI 33 RE country EOP\n",
      "---\n",
      "Predict: \teach POI bed RE contains EOP\n",
      "Ground-Truth: \teach POI bed RE contains EOP\n",
      "---\n",
      "Predict: \t9 POI 10 RE contains EOP\n",
      "Ground-Truth: \t9 POI 30 RE country EOP\n",
      "---\n",
      "Predict: \tthe POI by RE contains EOP\n",
      "Ground-Truth: \tby POI nation RE contains EOP\n",
      "---\n",
      "Predict: \tall POI 26 RE capital EOP POI RE contains EOP\n",
      "Ground-Truth: \tcook POI 25 RE contains EOP\n",
      "---\n",
      "Predict: \tclinical POI clinical RE place_lived EOP\n",
      "Ground-Truth: \tanatomic POI pathology RE place_lived EOP\n",
      "---\n",
      "Predict: \tcom POI mexicoboutiquehotels RE contains EOP\n",
      "Ground-Truth: \tcom POI www RE contains EOP\n",
      "---\n",
      "Predict: \tin POI foundation RE company EOP\n",
      "Ground-Truth: \tin POI foundation RE company EOP\n",
      "---\n",
      "Predict: \tlower-level POI lower-level RE nationality EOP\n",
      "Ground-Truth: \t27 POI romania RE country EOP\n",
      "---\n",
      "Predict: \tgrant POI when RE contains EOP\n",
      "Ground-Truth: \tcharitable POI purpose RE contains EOP\n",
      "---\n",
      "Predict: \tby POI 34 RE nationality EOP\n",
      "Ground-Truth: \t36 POI 33 RE contains EOP\n",
      "---\n",
      "Predict: \tbegin POI is RE contains EOP\n",
      "Ground-Truth: \tto POI at RE contains EOP\n",
      "---\n",
      "Predict: \tlast POI last RE contains EOP\n",
      "Ground-Truth: \tbeen POI last RE country EOP\n",
      "---\n",
      "Predict: \tlinda POI and RE contains EOP\n",
      "Ground-Truth: \twife POI sharon RE contains EOP\n",
      "---\n",
      "Predict: \tnew POI spur RE place_lived EOP\n",
      "Ground-Truth: \tnew POI spur RE place_lived EOP\n",
      "---\n",
      "Predict: \tsan POI francisco RE contains EOP POI RE contains EOP\n",
      "Ground-Truth: \tfrancisco POI beach RE contains EOP\n",
      "---\n",
      "Predict: \ta POI than RE contains EOP\n",
      "Ground-Truth: \ta POI a RE country EOP\n",
      "---\n",
      "Predict: \thad POI had RE nationality EOP\n",
      "Ground-Truth: \twhich POI like RE nationality EOP\n",
      "---\n",
      "Predict: \t27 POI 26 RE contains EOP\n",
      "Ground-Truth: \t29 POI 265 RE contains EOP\n",
      "---\n",
      "Predict: \tprime POI minister RE contains EOP\n",
      "Ground-Truth: \tminister POI prime RE contains EOP\n",
      "---\n",
      "Predict: \told POI old RE contains EOP\n",
      "Ground-Truth: \t51 POI 49 RE contains EOP\n",
      "---\n",
      "Predict: \twhen POI when RE contains EOP\n",
      "Ground-Truth: \twhen POI summer RE contains EOP\n",
      "---\n",
      "Predict: \the POI he RE contains EOP\n",
      "Ground-Truth: \tof POI he RE contains EOP\n",
      "---\n",
      "Predict: \t36 POI 36 RE contains EOP\n",
      "Ground-Truth: \t49 POI 47 RE contains EOP\n",
      "---\n",
      "Predict: \t34 POI 38 RE contains EOP\n",
      "Ground-Truth: \t40 POI like RE country EOP\n",
      "---\n",
      "Predict: \tcollege POI ramapo RE contains EOP\n",
      "Ground-Truth: \tand POI ramapo RE country EOP\n",
      "---\n",
      "Predict: \tin POI in RE company EOP\n",
      "Ground-Truth: \tthat POI on RE company EOP\n",
      "---\n",
      "Predict: \tout POI out RE nationality EOP\n",
      "Ground-Truth: \tof POI out RE nationality EOP\n",
      "---\n",
      "Predict: \tbreaking POI independent RE place_lived EOP\n",
      "Ground-Truth: \tradio POI and RE place_lived EOP\n",
      "---\n",
      "Predict: \t11 POI 11 RE nationality EOP\n",
      "Ground-Truth: \tnebraska POI 13 RE nationality EOP\n",
      "---\n",
      "Predict: \tis POI blossoming RE place_lived EOP\n",
      "Ground-Truth: \twind POI blossoming RE place_lived EOP\n",
      "---\n",
      "Predict: \tclient POI client RE nationality EOP\n",
      "Ground-Truth: \tclient POI david RE country EOP\n",
      "---\n",
      "Predict: \tis POI experience RE contains EOP\n",
      "Ground-Truth: \tthat POI the RE administrative_divisions EOP\n",
      "---\n",
      "Predict: \t29 POI 28 RE contains EOP\n",
      "Ground-Truth: \t29 POI 27 RE contains EOP\n",
      "---\n",
      "Predict: \trussia POI russia RE contains EOP\n",
      "Ground-Truth: \ton POI for RE country EOP\n",
      "---\n",
      "Predict: \ts POI party RE contains EOP\n",
      "Ground-Truth: \ts POI to RE contains EOP\n",
      "---\n",
      "Predict: \tduring POI three RE contains EOP\n",
      "Ground-Truth: \tto POI question RE country EOP\n",
      "---\n",
      "Predict: \ta POI nearly RE contains EOP\n",
      "Ground-Truth: \t38 POI 24 RE country EOP\n",
      "---\n",
      "Predict: \tvulnerable POI vulnerable RE contains EOP\n",
      "Ground-Truth: \tmost POI of RE contains EOP\n",
      "---\n",
      "Predict: \tdemocrat POI democrat RE contains EOP\n",
      "Ground-Truth: \tdemocrat POI minnesota RE country EOP\n",
      "---\n",
      "Predict: \t20 POI 19 RE contains EOP\n",
      "Ground-Truth: \t38 POI 19 RE country EOP\n",
      "---\n",
      "Predict: \twa POI 38 RE contains EOP\n",
      "Ground-Truth: \t37 POI wa RE contains EOP\n",
      "---\n",
      "Predict: \tmoore POI moore RE contains EOP\n",
      "Ground-Truth: \tprofessor POI robin RE contains EOP\n",
      "---\n",
      "Predict: \t29 POI 30 RE contains EOP\n",
      "Ground-Truth: \t29 POI 34 RE contains EOP\n",
      "---\n",
      "Predict: \thusband POI husband RE contains EOP\n",
      "Ground-Truth: \tlike POI supporter RE contains EOP\n",
      "---\n",
      "Predict: \tseemed POI others RE contains EOP\n",
      "Ground-Truth: \tseemed POI lazard RE contains EOP\n",
      "---\n",
      "Predict: \t23 POI 23 RE neighborhood_of EOP 23 POI 23 RE contains EOP\n",
      "Ground-Truth: \t23 POI bill RE contains EOP\n",
      "---\n",
      "Predict: \tschool POI school RE nationality EOP\n",
      "Ground-Truth: \tschool POI and RE contains EOP\n",
      "---\n",
      "Predict: \tscott POI scott RE contains EOP\n",
      "Ground-Truth: \toffice POI virginia RE place_lived EOP\n",
      "---\n",
      "Predict: \tthe POI military RE nationality EOP\n",
      "Ground-Truth: \tit POI the RE administrative_divisions EOP\n",
      "---\n",
      "Predict: \taccording POI with RE contains EOP\n",
      "Ground-Truth: \taccording POI a RE place_of_death EOP\n",
      "---\n",
      "Predict: \tforeign POI of RE capital EOP POI RE contains EOP\n",
      "Ground-Truth: \tthe POI aziz RE contains EOP\n",
      "---\n",
      "Predict: \tthe POI for RE contains EOP\n",
      "Ground-Truth: \tand POI began RE country EOP\n",
      "---\n",
      "Predict: \the POI before RE contains EOP\n",
      "Ground-Truth: \the POI concentration RE contains EOP\n",
      "---\n",
      "Predict: \tfrom POI them RE nationality EOP\n",
      "Ground-Truth: \t15 POI from RE nationality EOP\n",
      "---\n",
      "Predict: \t29 POI 30 RE company EOP\n",
      "Ground-Truth: \tthe POI 30 RE company EOP\n",
      "---\n",
      "Predict: \tthe POI the RE contains EOP\n",
      "Ground-Truth: \tnew POI p. RE contains EOP\n",
      "---\n",
      "Predict: \tto POI to RE contains EOP\n",
      "Ground-Truth: \tphilosophy POI favor RE contains EOP\n",
      "---\n",
      "Predict: \t34 POI 32 RE contains EOP\n",
      "Ground-Truth: \t33 POI 30 RE contains EOP\n",
      "---\n",
      "Predict: \tthree POI new RE contains EOP\n",
      "Ground-Truth: \tthree POI new RE country EOP\n",
      "---\n",
      "Predict: \tfour POI the RE contains EOP\n",
      "Ground-Truth: \toperates POI the RE country EOP\n",
      "---\n",
      "Predict: \tnative POI native RE nationality EOP\n",
      "Ground-Truth: \ts POI connecticut RE nationality EOP\n",
      "---\n",
      "Predict: \twhite POI white RE contains EOP\n",
      "Ground-Truth: \twhite POI the RE contains EOP\n",
      "---\n",
      "Predict: \twho POI politician RE place_lived EOP\n",
      "Ground-Truth: \ta POI who RE place_lived EOP\n",
      "---\n",
      "Predict: \tspain POI spain RE contains EOP\n",
      "Ground-Truth: \tsouth POI spain RE country EOP\n",
      "---\n",
      "Predict: \tstudied POI and RE contains EOP\n",
      "Ground-Truth: \tlaw POI in RE country EOP\n",
      "---\n",
      "Predict: \trepublican POI 37 RE company EOP\n",
      "Ground-Truth: \trepublican POI and RE company EOP\n",
      "---\n",
      "Predict: \tchicago POI chicago RE contains EOP\n",
      "Ground-Truth: \tcox POI face RE contains EOP\n",
      "---\n",
      "Predict: \twin POI 1988 RE company EOP\n",
      "Ground-Truth: \thelped POI 1988 RE company EOP\n",
      "---\n",
      "Predict: \ttrans-siberian POI railroad RE nationality EOP\n",
      "Ground-Truth: \ta POI 1971 RE contains EOP\n",
      "---\n",
      "Predict: \this POI 23 RE contains EOP\n",
      "Ground-Truth: \this POI 33 RE administrative_divisions EOP\n",
      "---\n",
      "Predict: \tnew POI in RE contains EOP\n",
      "Ground-Truth: \twestchester POI mamaroneck RE contains EOP\n",
      "---\n",
      "Predict: \tof POI w. RE contains EOP\n",
      "Ground-Truth: \tthe POI caldwell RE country EOP\n",
      "---\n",
      "Predict: \tjean-louis POI who RE company EOP\n",
      "Ground-Truth: \tconservative POI a RE place_lived EOP\n",
      "---\n",
      "Predict: \tspeaker POI the RE contains EOP\n",
      "Ground-Truth: \tthe POI april RE country EOP\n",
      "---\n",
      "Predict: \tthat POI that RE contains EOP\n",
      "Ground-Truth: \tthat POI learned RE contains EOP\n",
      "---\n",
      "Predict: \tand POI and RE contains EOP\n",
      "Ground-Truth: \tmr POI union RE place_of_death EOP\n",
      "---\n",
      "Predict: \tbarely POI barely RE contains EOP\n",
      "Ground-Truth: \tbarely POI s RE contains EOP\n",
      "---\n",
      "Predict: \tchad POI the RE company EOP\n",
      "Ground-Truth: \tfiltering POI 41 RE country EOP\n",
      "---\n",
      "Predict: \tthe POI like RE contains EOP\n",
      "Ground-Truth: \tthe POI like RE contains EOP\n",
      "---\n",
      "Predict: \tmount POI hospital RE company EOP\n",
      "Ground-Truth: \ta POI hospital RE company EOP\n",
      "---\n",
      "Predict: \tdevelopment POI development RE nationality EOP\n",
      "Ground-Truth: \tis POI development RE contains EOP\n",
      "---\n",
      "Predict: \twedding POI coolest RE place_lived EOP\n",
      "Ground-Truth: \tthe POI and RE place_lived EOP\n",
      "---\n",
      "Predict: \t29 POI 28 RE contains EOP\n",
      "Ground-Truth: \t29 POI innocent RE country EOP\n",
      "---\n",
      "Predict: \tvoiced POI voiced RE contains EOP\n",
      "Ground-Truth: \tvoiced POI his RE country EOP\n",
      "---\n",
      "Predict: \tyear POI visiting RE contains EOP\n",
      "Ground-Truth: \tyear POI 30 RE contains EOP\n",
      "---\n",
      "Predict: \ttrained POI in RE contains EOP\n",
      "Ground-Truth: \ttrained POI had RE contains EOP\n",
      "---\n",
      "Predict: \tin POI in RE capital EOP mamaroneck POI church RE contains EOP\n",
      "Ground-Truth: \tin POI church RE contains EOP\n",
      "---\n",
      "Predict: \tto POI controlling RE company EOP\n",
      "Ground-Truth: \t47 POI stake RE founders EOP\n",
      "---\n",
      "Predict: \t42 POI 41 RE contains EOP\n",
      "Ground-Truth: \t46 POI 38 RE contains EOP\n",
      "---\n",
      "Predict: \ttoronto POI cause RE contains EOP\n",
      "Ground-Truth: \tof POI warned RE country EOP\n",
      "---\n",
      "Predict: \tlast POI 41 RE contains EOP\n",
      "Ground-Truth: \thenin POI 44 RE country EOP\n",
      "---\n",
      "Predict: \tontario POI ontario RE neighborhood_of EOP ontario POI ontario RE contains EOP\n",
      "Ground-Truth: \tin POI may RE contains EOP\n",
      "---\n",
      "Predict: \tled POI led RE contains EOP\n",
      "Ground-Truth: \tled POI syria RE contains EOP\n",
      "---\n",
      "Predict: \tthe POI xwb RE place_lived EOP\n",
      "Ground-Truth: \tthe POI a RE place_lived EOP\n",
      "---\n",
      "Predict: \tthe POI 30 RE contains EOP\n",
      "Ground-Truth: \tis POI 30 RE country EOP\n",
      "---\n",
      "Predict: \tover POI soldier RE contains EOP\n",
      "Ground-Truth: \tthe POI of RE contains EOP\n",
      "---\n",
      "Predict: \tbeen POI had RE contains EOP\n",
      "Ground-Truth: \ta POI anyone RE country EOP\n",
      "---\n",
      "Predict: \tsix POI is RE contains EOP\n",
      "Ground-Truth: \tsix POI is RE contains EOP\n",
      "---\n",
      "Predict: \tcommittee POI 21 RE contains EOP\n",
      "Ground-Truth: \tclimate POI 22 RE neighborhood_of EOP\n",
      "---\n",
      "Predict: \tat POI 41 RE contains EOP POI RE contains EOP\n",
      "Ground-Truth: \t36 POI 42 RE country EOP\n",
      "---\n",
      "Predict: \teight POI 10 RE contains EOP\n",
      "Ground-Truth: \teight POI in RE contains EOP\n",
      "---\n",
      "Predict: \tit POI result RE company EOP\n",
      "Ground-Truth: \tthat POI iraq RE company EOP\n",
      "---\n",
      "Predict: \ta POI fizie RE contains EOP\n",
      "Ground-Truth: \ta POI fizie RE contains EOP\n",
      "---\n",
      "Predict: \t25 POI tanzania RE place_lived EOP\n",
      "Ground-Truth: \ttanzania POI 26 RE place_lived EOP\n",
      "---\n",
      "Predict: \tminister POI of RE place_lived EOP\n",
      "Ground-Truth: \tminister POI israel RE place_lived EOP\n",
      "---\n",
      "Predict: \tsouth POI south RE place_lived EOP\n",
      "Ground-Truth: \tport POI 31 RE place_lived EOP\n",
      "---\n",
      "Predict: \tdemocrat POI kyl RE nationality EOP\n",
      "Ground-Truth: \tskeptical POI and RE country EOP\n",
      "---\n",
      "Predict: \tfeldstein POI feldstein RE contains EOP\n",
      "Ground-Truth: \tfeldstein POI harvard RE contains EOP\n",
      "---\n",
      "Predict: \theld POI held RE contains EOP\n",
      "Ground-Truth: \tto POI held RE place_lived EOP\n",
      "---\n",
      "Predict: \tgovernor POI national RE contains EOP\n",
      "Ground-Truth: \tgovernor POI the RE contains EOP\n",
      "---\n",
      "Predict: \t38 POI 41 RE place_lived EOP\n",
      "Ground-Truth: \t38 POI 41 RE place_lived EOP\n",
      "---\n",
      "Predict: \tnot POI built RE contains EOP\n",
      "Ground-Truth: \tbuilt POI russia RE contains EOP\n",
      "---\n",
      "Predict: \tseminole POI seminole RE place_lived EOP\n",
      "Ground-Truth: \t21 POI seminole RE place_lived EOP\n",
      "---\n",
      "Predict: \tto POI to RE contains EOP\n",
      "Ground-Truth: \tplanned POI word RE country EOP\n",
      "---\n",
      "Predict: \t21 POI 21 RE contains EOP\n",
      "Ground-Truth: \t20 POI 23 RE country EOP\n",
      "---\n",
      "Predict: \tit POI it RE contains EOP\n",
      "Ground-Truth: \tdifficult POI image RE country EOP\n",
      "---\n",
      "Predict: \tstaten POI linking RE contains EOP\n",
      "Ground-Truth: \tisland POI 1964 RE contains EOP\n",
      "---\n",
      "Predict: \tthose POI maranello RE capital EOP POI RE contains EOP\n",
      "Ground-Truth: \tthose POI sport RE country EOP\n",
      "---\n",
      "Predict: \teither POI more RE contains EOP\n",
      "Ground-Truth: \tor POI either RE country EOP\n",
      "---\n",
      "Predict: \tin POI in RE contains EOP\n",
      "Ground-Truth: \tof POI 24 RE country EOP\n",
      "---\n",
      "Predict: \t52 POI 52 RE contains EOP\n",
      "Ground-Truth: \t106 POI 120 RE country EOP\n",
      "---\n",
      "Predict: \t24 POI 23 RE contains EOP\n",
      "Ground-Truth: \t26 POI 22 RE contains EOP\n",
      "---\n",
      "Predict: \the POI he RE contains EOP\n",
      "Ground-Truth: \tgaragiola POI said RE country EOP\n",
      "---\n",
      "Predict: \tsouth POI and RE contains EOP\n",
      "Ground-Truth: \tcarolina POI and RE contains EOP\n",
      "---\n",
      "Predict: \t34 POI 32 RE nationality EOP\n",
      "Ground-Truth: \t32 POI 34 RE nationality EOP\n",
      "---\n",
      "Predict: \tlrb POI lrb RE contains EOP\n",
      "Ground-Truth: \tthe POI the RE country EOP\n",
      "---\n",
      "Predict: \t24 POI 23 RE contains EOP\n",
      "Ground-Truth: \t23 POI 20 RE contains EOP\n",
      "---\n",
      "Predict: \tcohen POI 30 RE contains EOP\n",
      "Ground-Truth: \tcohen POI story RE nationality EOP\n",
      "---\n",
      "Predict: \t48 POI radcliffe RE contains EOP\n",
      "Ground-Truth: \t48 POI radcliffe RE contains EOP\n",
      "---\n",
      "Predict: \t34 POI 34 RE contains EOP\n",
      "Ground-Truth: \t34 POI 14 RE contains EOP\n",
      "---\n",
      "Predict: \thelp POI help RE nationality EOP\n",
      "Ground-Truth: \tthe POI to RE nationality EOP\n",
      "---\n",
      "Predict: \tin POI photography RE contains EOP\n",
      "Ground-Truth: \t32 POI 44 RE country EOP\n",
      "---\n",
      "Predict: \tve POI they RE contains EOP\n",
      "Ground-Truth: \tthey POI on RE contains EOP\n",
      "---\n",
      "Predict: \twith POI membership RE contains EOP\n",
      "Ground-Truth: \ttalk POI start RE contains EOP\n",
      "---\n",
      "Predict: \t23 POI 23 RE neighborhood_of EOP 23 POI 23 RE contains EOP\n",
      "Ground-Truth: \t23 POI 22 RE contains EOP\n",
      "---\n",
      "Predict: \tan POI race RE contains EOP\n",
      "Ground-Truth: \tand POI off-road RE contains EOP\n",
      "---\n",
      "Predict: \tbaltimore POI stadium RE contains EOP\n",
      "Ground-Truth: \tstadium POI in RE country EOP\n",
      "---\n",
      "Predict: \tcancer POI 38 RE contains EOP POI RE contains EOP\n",
      "Ground-Truth: \tfebruary POI 36 RE country EOP\n",
      "---\n",
      "Predict: \tlucas POI redwood RE contains EOP POI RE EOP\n",
      "Ground-Truth: \tcity POI amy RE contains EOP\n",
      "---\n",
      "Predict: \tman POI who RE nationality EOP\n",
      "Ground-Truth: \tsame POI they RE nationality EOP\n",
      "---\n",
      "Predict: \t24 POI 23 RE contains EOP\n",
      "Ground-Truth: \t24 POI 23 RE contains EOP\n",
      "---\n",
      "Predict: \t23 POI to RE contains EOP\n",
      "Ground-Truth: \t23 POI attack RE country EOP\n",
      "---\n",
      "Predict: \tand POI sandy RE place_lived EOP\n",
      "Ground-Truth: \tof POI sandy RE place_lived EOP\n",
      "---\n",
      "Predict: \tover POI more RE contains EOP\n",
      "Ground-Truth: \tmore POI the RE contains EOP\n",
      "---\n",
      "Predict: \t38 POI 41 RE nationality EOP\n",
      "Ground-Truth: \t45 POI 42 RE nationality EOP\n",
      "---\n",
      "Predict: \tby POI by RE contains EOP\n",
      "Ground-Truth: \twa POI gov RE contains EOP\n",
      "---\n",
      "Predict: \ttaipei POI lucifer RE contains EOP\n",
      "Ground-Truth: \tis POI a RE country EOP\n",
      "---\n",
      "Predict: \thenry POI 37 RE company EOP\n",
      "Ground-Truth: \t35 POI 37 RE founders EOP\n",
      "---\n",
      "Predict: \tconventional POI 28 RE nationality EOP\n",
      "Ground-Truth: \ta POI 28 RE nationality EOP\n",
      "---\n",
      "Predict: \twho POI the RE contains EOP\n",
      "Ground-Truth: \tthe POI general RE country EOP\n",
      "---\n",
      "Predict: \tpast POI past RE contains EOP\n",
      "Ground-Truth: \t31 POI not RE contains EOP\n",
      "---\n",
      "Predict: \town POI own RE contains EOP\n",
      "Ground-Truth: \town POI seeking RE contains EOP\n",
      "---\n",
      "Predict: \tan POI swimming RE contains EOP\n",
      "Ground-Truth: \t30 POI 29 RE contains EOP\n",
      "---\n",
      "Predict: \taraoz POI mayor RE company EOP\n",
      "Ground-Truth: \tthe POI movement RE company EOP\n",
      "---\n",
      "Predict: \titaly POI italy RE contains EOP\n",
      "Ground-Truth: \tspain POI mexico RE contains EOP\n",
      "---\n",
      "Predict: \tmade POI made RE contains EOP\n",
      "Ground-Truth: \tmade POI washington RE contains EOP\n",
      "---\n",
      "Predict: \twere POI reported RE place_lived EOP\n",
      "Ground-Truth: \tpeople POI reported RE place_lived EOP\n",
      "---\n",
      "Predict: \tfebruary POI tuesday RE contains EOP\n",
      "Ground-Truth: \tfebruary POI tuesday RE contains EOP\n",
      "---\n",
      "Predict: \tneglected POI neglected RE contains EOP\n",
      "Ground-Truth: \tone POI neglected RE country EOP\n",
      "---\n",
      "Predict: \tblake POI james RE contains EOP\n",
      "Ground-Truth: \tcup POI at RE contains EOP\n",
      "---\n",
      "Predict: \tknown POI mgm RE contains EOP\n",
      "Ground-Truth: \tmgm POI star RE contains EOP\n",
      "---\n",
      "Predict: \tfor POI award RE contains EOP\n",
      "Ground-Truth: \ttime POI the RE country EOP\n",
      "---\n",
      "Predict: \tsisario POI pele RE place_lived EOP\n",
      "Ground-Truth: \tsisario POI pele RE place_lived EOP\n",
      "---\n",
      "Predict: \tat POI is RE company EOP\n",
      "Ground-Truth: \tbollywood POI center RE company EOP\n",
      "---\n",
      "Predict: \tof POI of RE contains EOP\n",
      "Ground-Truth: \tmedical POI of RE country EOP\n",
      "---\n",
      "Predict: \tlack POI lack RE contains EOP\n",
      "Ground-Truth: \tnavarre POI lack RE country EOP\n",
      "---\n",
      "Predict: \tthird-generation POI third-generation RE nationality EOP\n",
      "Ground-Truth: \tabove POI chef RE nationality EOP\n",
      "---\n",
      "Predict: \titaly POI to RE contains EOP\n",
      "Ground-Truth: \titaly POI to RE contains EOP\n",
      "---\n",
      "Predict: \tnegotiation POI negotiation RE capital EOP POI RE EOP\n",
      "Ground-Truth: \tnegotiation POI 39 RE company EOP\n",
      "---\n",
      "Predict: \t20 POI 20 RE contains EOP\n",
      "Ground-Truth: \tcenter POI org RE company EOP\n",
      "---\n",
      "Predict: \this POI experiment RE contains EOP\n",
      "Ground-Truth: \tin POI middle RE country EOP\n",
      "---\n",
      "Predict: \tthe POI the RE place_lived EOP\n",
      "Ground-Truth: \tsecond-highest POI cascade RE place_lived EOP\n",
      "---\n",
      "Predict: \tgregory POI the RE contains EOP\n",
      "Ground-Truth: \tgregory POI director RE contains EOP\n",
      "---\n",
      "Predict: \tnot POI not RE contains EOP\n",
      "Ground-Truth: \tnot POI ha RE contains EOP\n",
      "---\n",
      "Predict: \tbeen POI on RE place_lived EOP\n",
      "Ground-Truth: \tever POI on RE place_lived EOP\n",
      "---\n",
      "Predict: \tthe POI the RE contains EOP\n",
      "Ground-Truth: \t18 POI 22 RE country EOP\n",
      "---\n",
      "Predict: \tproprietor POI the RE contains EOP\n",
      "Ground-Truth: \ta POI dunne RE contains EOP\n",
      "---\n",
      "Predict: \tthe POI visit RE capital EOP POI visit RE contains EOP\n",
      "Ground-Truth: \tvisit POI war RE country EOP\n",
      "---\n",
      "Predict: \tw. POI w. RE contains EOP\n",
      "Ground-Truth: \tpatterson POI patrick RE contains EOP\n",
      "---\n",
      "Predict: \t42 POI 41 RE contains EOP\n",
      "Ground-Truth: \t79 POI 47 RE country EOP\n",
      "---\n",
      "Predict: \ttraveling POI pompidou RE nationality EOP\n",
      "Ground-Truth: \ttraveling POI centre RE nationality EOP\n",
      "---\n",
      "Predict: \tfound POI found RE contains EOP\n",
      "Ground-Truth: \tmarshall POI batman RE company EOP\n",
      "---\n",
      "Predict: \tand POI and RE contains EOP\n",
      "Ground-Truth: \tbuild POI and RE country EOP\n",
      "---\n",
      "Predict: \ts POI of RE contains EOP\n",
      "Ground-Truth: \ttechnology POI the RE contains EOP\n",
      "---\n",
      "Predict: \tlawmaker POI by RE contains EOP POI RE contains EOP\n",
      "Ground-Truth: \tlawmaker POI by RE contains EOP\n",
      "---\n",
      "Predict: \tsunday POI on RE contains EOP\n",
      "Ground-Truth: \ta POI parliament RE capital EOP\n",
      "---\n",
      "Predict: \t42 POI 41 RE nationality EOP\n",
      "Ground-Truth: \t54 POI 45 RE contains EOP\n",
      "---\n",
      "Predict: \tmany POI 41 RE contains EOP\n",
      "Ground-Truth: \t42 POI 40 RE contains EOP\n",
      "---\n",
      "Predict: \tgraduated POI normale RE company EOP\n",
      "Ground-Truth: \tgennes POI in RE company EOP\n",
      "---\n",
      "Predict: \tspain POI now RE contains EOP\n",
      "Ground-Truth: \tand POI ac RE country EOP\n",
      "---\n",
      "Predict: \tboston POI boston RE contains EOP\n",
      "Ground-Truth: \tfirst POI in RE place_lived EOP\n",
      "---\n",
      "Predict: \tpartner POI partner RE contains EOP\n",
      "Ground-Truth: \tpartner POI minority RE contains EOP\n",
      "---\n",
      "Predict: \tthe POI the RE place_lived EOP\n",
      "Ground-Truth: \tpopular POI joined RE company EOP\n",
      "---\n",
      "Predict: \tcondition POI condition RE neighborhood_of EOP condition POI condition RE contains EOP\n",
      "Ground-Truth: \tincluding POI s RE contains EOP\n",
      "---\n",
      "Predict: \t34 POI 37 RE company EOP\n",
      "Ground-Truth: \t36 POI 42 RE children EOP\n",
      "---\n",
      "Predict: \twashington POI washington RE contains EOP\n",
      "Ground-Truth: \tupper POI in RE contains EOP\n",
      "---\n",
      "Predict: \t32 POI 32 RE contains EOP\n",
      "Ground-Truth: \t23 POI the RE country EOP\n",
      "---\n",
      "Predict: \tra'anana POI ra'anana RE nationality EOP\n",
      "Ground-Truth: \tra'anana POI in RE contains EOP\n",
      "---\n",
      "Predict: \tmoscow POI new RE contains EOP\n",
      "Ground-Truth: \tyork POI vienna RE contains EOP\n",
      "---\n",
      "Predict: \twoodruff POI who RE contains EOP\n",
      "Ground-Truth: \tdovetailed POI woodruff RE country EOP\n",
      "---\n",
      "Predict: \ton POI any RE capital EOP POI RE contains EOP\n",
      "Ground-Truth: \tany POI on RE contains EOP\n",
      "---\n",
      "Predict: \tcasa POI casa RE contains EOP\n",
      "Ground-Truth: \tstone POI home RE country EOP\n",
      "---\n",
      "Predict: \tvariety POI regulatory RE contains EOP\n",
      "Ground-Truth: \tvariety POI regulatory RE country EOP\n",
      "---\n",
      "Predict: \t21 POI 21 RE contains EOP\n",
      "Ground-Truth: \t21 POI 20 RE contains EOP\n",
      "---\n",
      "Predict: \tadmission POI the RE nationality EOP\n",
      "Ground-Truth: \tthe POI she RE nationality EOP\n",
      "---\n",
      "Predict: \t32 POI 32 RE contains EOP\n",
      "Ground-Truth: \t33 POI tehran RE contains EOP\n",
      "---\n",
      "Predict: \tweizmann POI the RE contains EOP\n",
      "Ground-Truth: \tthe POI rehovot RE contains EOP\n",
      "---\n",
      "Predict: \tarte POI mexico RE contains EOP\n",
      "Ground-Truth: \tcontemporary POI april RE country EOP\n",
      "---\n",
      "Predict: \tnazi POI escape RE contains EOP\n",
      "Ground-Truth: \tescape POI to RE contains EOP\n",
      "---\n",
      "Predict: \t34 POI 32 RE contains EOP\n",
      "Ground-Truth: \t34 POI 30 RE contains EOP\n",
      "---\n",
      "Predict: \t35 POI 35 RE contains EOP\n",
      "Ground-Truth: \t34 POI home RE contains EOP\n",
      "---\n",
      "Predict: \tand POI reign RE nationality EOP\n",
      "Ground-Truth: \tof POI and RE nationality EOP\n",
      "---\n",
      "Predict: \tkentucky POI of RE nationality EOP\n",
      "Ground-Truth: \tmcconnell POI kentucky RE nationality EOP\n",
      "---\n",
      "Predict: \tnearly POI nearly RE contains EOP\n",
      "Ground-Truth: \tprimarily POI nearly RE country EOP\n",
      "---\n",
      "Predict: \tthe POI e. RE company EOP\n",
      "Ground-Truth: \tthe POI donald RE company EOP\n",
      "---\n",
      "Predict: \tpm POI pm RE contains EOP\n",
      "Ground-Truth: \teast POI 5 RE contains EOP\n",
      "---\n",
      "Predict: \t19 POI 19 RE contains EOP\n",
      "Ground-Truth: \t44 POI 19 RE country EOP\n",
      "---\n",
      "Predict: \tpoland POI poland RE contains EOP\n",
      "Ground-Truth: \tin POI research RE contains EOP\n",
      "---\n",
      "Predict: \tadministration POI administration RE contains EOP\n",
      "Ground-Truth: \tthe POI could RE country EOP\n",
      "---\n",
      "Predict: \tgrant POI national RE company EOP\n",
      "Ground-Truth: \tgrant POI a RE company EOP\n",
      "---\n",
      "Predict: \t25 POI 28 RE contains EOP\n",
      "Ground-Truth: \t23 POI 29 RE country EOP\n",
      "---\n",
      "Predict: \tgeographical POI wa RE contains EOP\n",
      "Ground-Truth: \tgeographical POI made RE contains EOP\n",
      "---\n",
      "Predict: \ta POI 21 RE place_lived EOP\n",
      "Ground-Truth: \twriter POI in RE company EOP\n",
      "---\n",
      "Predict: \t30 POI 30 RE contains EOP\n",
      "Ground-Truth: \t30 POI 26 RE contains EOP\n",
      "---\n",
      "Predict: \tinclude POI up RE contains EOP\n",
      "Ground-Truth: \tinclude POI up RE contains EOP\n",
      "---\n",
      "Predict: \tmoving POI moving RE contains EOP\n",
      "Ground-Truth: \tmoving POI spain RE contains EOP\n",
      "---\n",
      "Predict: \tiran POI iran RE contains EOP\n",
      "Ground-Truth: \tiran POI neighbor RE contains EOP\n",
      "---\n",
      "Predict: \tjohnson POI 28 RE contains EOP\n",
      "Ground-Truth: \tretired POI 28 RE contains EOP\n",
      "---\n",
      "Predict: \twaverly POI waverly RE place_lived EOP\n",
      "Ground-Truth: \t36 POI 41 RE company EOP\n",
      "---\n",
      "Predict: \tat POI away RE contains EOP\n",
      "Ground-Truth: \tat POI peacefully RE contains EOP\n",
      "---\n",
      "Predict: \tgeorge POI officer RE contains EOP\n",
      "Ground-Truth: \treyes POI financial RE contains EOP\n",
      "---\n",
      "Predict: \tpresident POI group RE contains EOP\n",
      "Ground-Truth: \tpresident POI m. RE contains EOP\n",
      "---\n",
      "Predict: \t38 POI 41 RE contains EOP\n",
      "Ground-Truth: \t41 POI 35 RE contains EOP\n",
      "---\n",
      "Predict: \tit POI it RE contains EOP\n",
      "Ground-Truth: \tunless POI it RE country EOP\n",
      "---\n",
      "Predict: \t21 POI 23 RE contains EOP\n",
      "Ground-Truth: \t21 POI 24 RE country EOP\n",
      "---\n",
      "Predict: \t1983 POI 23 RE nationality EOP\n",
      "Ground-Truth: \t27 POI in RE country EOP\n",
      "---\n",
      "Predict: \ton POI iran RE contains EOP\n",
      "Ground-Truth: \tputin POI to RE company EOP\n",
      "---\n",
      "Predict: \tantonio POI in RE contains EOP\n",
      "Ground-Truth: \tantonio POI san RE contains EOP\n",
      "---\n",
      "Predict: \tberkeley POI 21 RE contains EOP\n",
      "Ground-Truth: \t21 POI 11 RE contains EOP\n",
      "---\n",
      "Predict: \twhere POI where RE contains EOP\n",
      "Ground-Truth: \t29 POI 26 RE contains EOP\n",
      "---\n",
      "Predict: \t21 POI 20 RE contains EOP\n",
      "Ground-Truth: \t21 POI 19 RE contains EOP\n",
      "---\n",
      "Predict: \tthe POI by RE contains EOP\n",
      "Ground-Truth: \tcharged POI hick RE contains EOP\n",
      "---\n",
      "Predict: \troute POI the RE contains EOP\n",
      "Ground-Truth: \troute POI along RE contains EOP\n",
      "---\n",
      "Predict: \ttemporarily POI temporarily RE contains EOP\n",
      "Ground-Truth: \tclosed POI temporarily RE country EOP\n",
      "---\n",
      "Predict: \ta POI the RE company EOP\n",
      "Ground-Truth: \ta POI national RE company EOP\n",
      "---\n",
      "Predict: \tthe POI for RE contains EOP\n",
      "Ground-Truth: \tand POI the RE contains EOP\n",
      "---\n",
      "Predict: \ton POI suffolk RE place_lived EOP\n",
      "Ground-Truth: \ton POI suffolk RE place_lived EOP\n",
      "---\n",
      "Predict: \t25 POI 28 RE contains EOP\n",
      "Ground-Truth: \t27 POI 24 RE contains EOP\n",
      "---\n",
      "Predict: \tin POI center RE contains EOP\n",
      "Ground-Truth: \tcenter POI of RE country EOP\n",
      "---\n",
      "Predict: \tapproved POI communique RE contains EOP\n",
      "Ground-Truth: \tapproved POI final RE contains EOP\n",
      "---\n",
      "Predict: \t13 POI 30 RE contains EOP\n",
      "Ground-Truth: \t27 POI 24 RE contains EOP\n",
      "---\n",
      "Predict: \t13 POI florida RE contains EOP\n",
      "Ground-Truth: \t13 POI florida RE contains EOP\n",
      "---\n",
      "Predict: \this POI two RE company EOP\n",
      "Ground-Truth: \tand POI his RE contains EOP\n",
      "---\n",
      "Predict: \tresponse POI hurricane RE contains EOP\n",
      "Ground-Truth: \thurricane POI response RE capital EOP\n",
      "---\n",
      "Predict: \tfor POI hypocritical RE contains EOP\n",
      "Ground-Truth: \tfor POI hypocritical RE contains EOP\n",
      "---\n",
      "Predict: \t42 POI 44 RE contains EOP\n",
      "Ground-Truth: \t50 POI 47 RE contains EOP\n",
      "---\n",
      "Predict: \ttehran POI 52 RE contains EOP\n",
      "Ground-Truth: \t95 POI 64 RE country EOP\n",
      "---\n",
      "Predict: \ta3-11 POI damascus RE contains EOP\n",
      "Ground-Truth: \tanyone POI syria RE contains EOP\n",
      "---\n",
      "Predict: \tfrom POI the RE place_lived EOP\n",
      "Ground-Truth: \thave POI state RE company EOP\n",
      "---\n",
      "Predict: \tyork POI new RE contains EOP\n",
      "Ground-Truth: \tyork POI in RE contains EOP\n",
      "---\n",
      "Predict: \ts POI s RE contains EOP\n",
      "Ground-Truth: \t40 POI 37 RE contains EOP\n",
      "---\n",
      "Predict: \tunited POI spend RE contains EOP\n",
      "Ground-Truth: \twanted POI to RE country EOP\n",
      "---\n",
      "Predict: \tin POI in RE place_lived EOP\n",
      "Ground-Truth: \tsuch POI city RE place_lived EOP\n",
      "---\n",
      "Predict: \tconduct POI conduct RE contains EOP\n",
      "Ground-Truth: \tthat POI of RE company EOP\n",
      "---\n",
      "Predict: \tvillage POI the RE contains EOP\n",
      "Ground-Truth: \tthe POI tiny RE country EOP\n",
      "---\n",
      "Predict: \tgermany POI private RE contains EOP\n",
      "Ground-Truth: \tweek POI this RE contains EOP\n",
      "---\n",
      "Predict: \tdecoufle POI turn RE contains EOP\n",
      "Ground-Truth: \tfirst POI surprising RE contains EOP\n",
      "---\n",
      "Predict: \tbuilding POI building RE capital EOP POI gujarat RE contains EOP\n",
      "Ground-Truth: \tbuilding POI and RE contains EOP\n",
      "---\n",
      "Predict: \tpolitical POI toasted RE contains EOP\n",
      "Ground-Truth: \tthe POI republican RE contains EOP\n",
      "---\n",
      "Predict: \t7 POI 7 RE contains EOP\n",
      "Ground-Truth: \t8 POI 17 RE contains EOP\n",
      "---\n",
      "Predict: \t38 POI 38 RE contains EOP\n",
      "Ground-Truth: \t41 POI 42 RE country EOP\n",
      "---\n",
      "Predict: \t4 POI 4 RE contains EOP\n",
      "Ground-Truth: \t4 POI the RE contains EOP\n",
      "---\n",
      "Predict: \tthe POI the RE contains EOP\n",
      "Ground-Truth: \tspeculation POI to RE country EOP\n",
      "---\n",
      "Predict: \t1946 POI 1946 RE contains EOP\n",
      "Ground-Truth: \tbecame POI 1946 RE country EOP\n",
      "---\n",
      "Predict: \tpark POI winslow RE contains EOP\n",
      "Ground-Truth: \tpark POI winslow RE contains EOP\n",
      "---\n",
      "Predict: \tweek POI a RE place_lived EOP\n",
      "Ground-Truth: \twould POI on RE nationality EOP\n",
      "---\n",
      "Predict: \tof POI several RE contains EOP\n",
      "Ground-Truth: \tseveral POI of RE contains EOP\n",
      "---\n",
      "Predict: \ttwo POI two RE contains EOP\n",
      "Ground-Truth: \ttwo POI at RE contains EOP\n",
      "---\n",
      "Predict: \ta POI prodigy RE contains EOP\n",
      "Ground-Truth: \tfield POI a RE contains EOP\n",
      "---\n",
      "Predict: \tin POI an RE place_lived EOP\n",
      "Ground-Truth: \tto POI s RE place_lived EOP\n",
      "---\n",
      "Predict: \tattorney POI the RE contains EOP\n",
      "Ground-Truth: \tassistant POI to RE contains EOP\n",
      "---\n",
      "Predict: \the POI 28 RE company EOP\n",
      "Ground-Truth: \the POI 14 RE company EOP\n",
      "---\n",
      "Predict: \tthe POI b. RE company EOP\n",
      "Ground-Truth: \tjuliet POI not RE contains EOP\n",
      "---\n",
      "Predict: \tnever-ending POI and RE contains EOP\n",
      "Ground-Truth: \ttheir POI never-ending RE country EOP\n",
      "---\n",
      "Predict: \tgov POI of RE contains EOP\n",
      "Ground-Truth: \tgov POI ohio RE contains EOP\n",
      "---\n",
      "Predict: \tabbas POI mahmoud RE contains EOP\n",
      "Ground-Truth: \tha POI president RE contains EOP\n",
      "---\n",
      "Predict: \tthe POI the RE contains EOP\n",
      "Ground-Truth: \tcursory POI ha RE contains EOP\n",
      "---\n",
      "Predict: \t34 POI 32 RE contains EOP\n",
      "Ground-Truth: \t34 POI 32 RE contains EOP\n",
      "---\n",
      "Predict: \tjoin POI leading RE contains EOP\n",
      "Ground-Truth: \t58 POI will RE country EOP\n",
      "---\n",
      "Predict: \t29 POI 28 RE contains EOP\n",
      "Ground-Truth: \t24 POI 31 RE country EOP\n",
      "---\n",
      "Predict: \tand POI and RE contains EOP\n",
      "Ground-Truth: \tand POI mr RE contains EOP\n",
      "---\n",
      "Predict: \tand POI in RE contains EOP\n",
      "Ground-Truth: \twestchester POI have RE contains EOP\n",
      "---\n",
      "Predict: \t17 POI 17 RE contains EOP\n",
      "Ground-Truth: \t23 POI 37 RE country EOP 34 POI 37 RE country EOP\n",
      "---\n",
      "Predict: \tno POI no RE nationality EOP\n",
      "Ground-Truth: \tthere POI no RE country EOP\n",
      "---\n",
      "Predict: \ts POI second-largest RE place_lived EOP\n",
      "Ground-Truth: \tlinden POI second-largest RE place_lived EOP\n",
      "---\n",
      "Predict: \thamas POI said RE place_lived EOP\n",
      "Ground-Truth: \tehud POI continues RE place_lived EOP\n",
      "---\n",
      "Predict: \tbetween POI tension RE contains EOP\n",
      "Ground-Truth: \tbetween POI tension RE contains EOP\n",
      "---\n",
      "Predict: \thaven POI new RE contains EOP\n",
      "Ground-Truth: \tsimilar POI sponsored RE contains EOP\n",
      "---\n",
      "Predict: \ti POI i RE nationality EOP\n",
      "Ground-Truth: \tfrance POI i RE nationality EOP\n",
      "---\n",
      "Predict: \t35 POI 35 RE contains EOP\n",
      "Ground-Truth: \t29 POI 26 RE contains EOP\n",
      "---\n",
      "Predict: \ts POI and RE contains EOP\n",
      "Ground-Truth: \twoman POI fellow RE country EOP\n",
      "---\n",
      "Predict: \t39 POI 38 RE contains EOP\n",
      "Ground-Truth: \t38 POI yet RE contains EOP\n",
      "---\n",
      "Predict: \tdespite POI session RE nationality EOP\n",
      "Ground-Truth: \tdespite POI bigotry RE place_lived EOP\n",
      "---\n",
      "Predict: \tchief POI chief RE contains EOP\n",
      "Ground-Truth: \tnew POI jetblue RE contains EOP\n",
      "---\n",
      "Predict: \ta POI a RE nationality EOP\n",
      "Ground-Truth: \tbook POI a RE country EOP\n",
      "---\n",
      "Predict: \tand POI naval RE contains EOP\n",
      "Ground-Truth: \tvahradian POI in RE contains EOP\n",
      "---\n",
      "Predict: \tdocument POI to RE contains EOP\n",
      "Ground-Truth: \tincluding POI to RE company EOP\n",
      "---\n",
      "Predict: \tisrael POI built RE nationality EOP\n",
      "Ground-Truth: \tbarrier POI of RE nationality EOP\n",
      "---\n",
      "Predict: \tmetaphorical POI their RE contains EOP\n",
      "Ground-Truth: \twhortleberry POI the RE children EOP\n",
      "---\n",
      "Predict: \taustralia POI of RE contains EOP\n",
      "Ground-Truth: \taustralia POI of RE contains EOP\n",
      "---\n",
      "Predict: \tlong-term POI long-term RE contains EOP\n",
      "Ground-Truth: \tfederal POI the RE contains EOP\n",
      "---\n",
      "Predict: \tmartin POI of RE place_lived EOP\n",
      "Ground-Truth: \tprinceton POI of RE company EOP\n",
      "---\n",
      "Predict: \tcamp POI day RE contains EOP\n",
      "Ground-Truth: \tday POI ran RE contains EOP\n",
      "---\n",
      "Predict: \tanother POI tuesday RE place_lived EOP\n",
      "Ground-Truth: \tanother POI tuesday RE place_lived EOP\n",
      "---\n",
      "Predict: \tsong POI multimedia RE place_lived EOP\n",
      "Ground-Truth: \tsong POI love RE place_lived EOP\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "preResult = [' '.join(sent) for sent in preResult]\n",
    "actResult = [' '.join(sent) for sent in actResult]\n",
    "for i in range(395):\n",
    "    print('Predict: \\t%s' % preResult[i])\n",
    "    print('Ground-Truth: \\t%s' % actResult[i])\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#[i for i in enumerate(actResult) if len(i[-1].split()) == 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#actResult[372]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preResult[372]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set([len(i.split()) for i in actResult])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "84px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
