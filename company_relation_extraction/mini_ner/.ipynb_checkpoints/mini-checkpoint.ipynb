{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data Preprocessing</a></div><div class=\"lev2 toc-item\"><a href=\"#Participle\" data-toc-modified-id=\"Participle-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Participle</a></div><div class=\"lev2 toc-item\"><a href=\"#Word-to-Label\" data-toc-modified-id=\"Word-to-Label-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Word to Label</a></div><div class=\"lev2 toc-item\"><a href=\"#Word-to-Vector\" data-toc-modified-id=\"Word-to-Vector-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Word to Vector</a></div><div class=\"lev3 toc-item\"><a href=\"#Train\" data-toc-modified-id=\"Train-131\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Train</a></div><div class=\"lev3 toc-item\"><a href=\"#Test\" data-toc-modified-id=\"Test-132\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Test</a></div><div class=\"lev1 toc-item\"><a href=\"#Model\" data-toc-modified-id=\"Model-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Model</a></div><div class=\"lev2 toc-item\"><a href=\"#Set-Hyperparameters\" data-toc-modified-id=\"Set-Hyperparameters-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Set Hyperparameters</a></div><div class=\"lev2 toc-item\"><a href=\"#Custom-Metrics\" data-toc-modified-id=\"Custom-Metrics-22\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Custom Metrics</a></div><div class=\"lev2 toc-item\"><a href=\"#Builde-Graph\" data-toc-modified-id=\"Builde-Graph-23\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Builde Graph</a></div><div class=\"lev2 toc-item\"><a href=\"#Data-Generator\" data-toc-modified-id=\"Data-Generator-24\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Data Generator</a></div><div class=\"lev2 toc-item\"><a href=\"#Split-Data\" data-toc-modified-id=\"Split-Data-25\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Split Data</a></div><div class=\"lev2 toc-item\"><a href=\"#Train\" data-toc-modified-id=\"Train-26\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Train</a></div><div class=\"lev2 toc-item\"><a href=\"#Predict\" data-toc-modified-id=\"Predict-27\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Predict</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import jieba\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('content.pkl', 'rb') as fp:\n",
    "    content = pickle.load(fp)\n",
    "with open('core-entity.pkl', 'rb') as fp:\n",
    "    core_eneity = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['《醉后一夜》把一对青年男女的相识相爱当悬疑故事来讲是个很好玩的事情，可是导演故事没讲话，玩花哨也没给影片带来多少特别讨喜的内容，只能算是平庸之作。',\n",
       " '《醉后决定爱上你》是由台湾梦田文创出品，是《命中注定我爱你》三部曲中的第二部，由陈铭章执导，杨丞琳、张孝全、王传一、  许玮甯等主演。',\n",
       " '非常漂亮的一个江南园林，乘坐地铁9号线直接到醉白池。',\n",
       " '醉白池乍一听很像最白痴，是江南园林的代表之一。',\n",
       " '《罪恶城市》和《Original  Gangstaz》一样是一款黑帮主题的角色扮演类游戏，虽然主题相同，但《罪恶城市》充分认识到了《Original  Gangstaz》的不足之处，改变其简单枯燥的游戏方式，加入法姆维尔风格地图及更精致的人物造型等等。',\n",
       " '专辑主打歌‘最熟悉的陌生人’是一首带有台式复古风味的抒情歌曲，MTV选在台中一家别具古味的餐厅拍摄，整个餐厅里头就是一条30年代的台湾老街，很符合主打歌既熟悉又陌生的感觉，MTV的剧情安排王介安经由时光隧道回到过去，看着街上的男男女女，彼此都有似曾相识的感觉，唱片公司老板林秋离更是首次粉墨登场，扮一个马夫酒鬼，负责在一旁抽烟喝酒。']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[6:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['醉后一夜'],\n",
       " ['醉后决定爱上你'],\n",
       " [],\n",
       " ['醉白池'],\n",
       " ['罪恶城市', 'Original  Gangstaz'],\n",
       " ['最熟悉的陌生人']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_eneity[6:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评论平均长度为：54\n"
     ]
    }
   ],
   "source": [
    "print('评论平均长度为：%d' % (int(round(np.mean([len(i) for i in content])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAXLEN = 80\n",
    "EMBEDDING_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Participle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "not_cuts = re.compile(u'([\\da-zA-Z \\.]+)|《(.*?)》|“(.{1,10})”')\n",
    "re_replace = re.compile(u'[^\\u4e00-\\u9fa50-9a-zA-Z《》\\(\\)（）“”·\\.]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def newcut(s):\n",
    "    \"\"\"\n",
    "    修改原分词函数:\n",
    "    1: 英文和数字部分不分词 \n",
    "    2: 双书名号中内容不分词\n",
    "    3: 双引号中十字以内内容不分词\n",
    "    4: 超出范围的字符均替换为空格\n",
    "    5: 使用结巴分词(关闭新词发现功能)\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    j = 0\n",
    "    s = re_replace.sub(' ', s)\n",
    "    for i in not_cuts.finditer(s):\n",
    "        result.extend(jieba.lcut(s[j:i.start()], HMM=False))\n",
    "        if s[i.start()] in [u'《', u'“']:\n",
    "            result.extend([s[i.start()], s[i.start()+1:i.end()-1], s[i.end()-1]])\n",
    "        else:\n",
    "            result.append(s[i.start():i.end()])\n",
    "        j = i.end()\n",
    "    result.extend(jieba.lcut(s[j:], HMM=False))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/kz/hqjl_dfx3g3_2vxylxlj1s940000gn/T/jieba.cache\n",
      "Loading model cost 1.312 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "words = [newcut(s) for s in content]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word to Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2label(n, word):\n",
    "    \"\"\"\n",
    "    将输出结果转换为标签序列\n",
    "    5tag:\n",
    "    0: 非核心实体 \n",
    "    1: 单词核心实体\n",
    "    2: 多词核心实体首词\n",
    "    3: 多词核心实体中间部分\n",
    "    4: 多词核心实体末词\n",
    "    \"\"\"\n",
    "    seq_word = word\n",
    "    s = seq_word[n]\n",
    "    r = ['0']*len(s)\n",
    "    for i in range(len(s)):\n",
    "        for j in core_eneity[n]:\n",
    "            if s[i] in j:\n",
    "                r[i] = '1'\n",
    "                break\n",
    "    s = ''.join(r)\n",
    "    r = [0]*len(s)\n",
    "    for i in re.finditer('1+', s):\n",
    "        if i.end() - i.start() > 1:\n",
    "            r[i.start()] = 2\n",
    "            r[i.end()-1] = 4\n",
    "            for j in range(i.start()+1, i.end()-1):\n",
    "                r[j] = 3\n",
    "        else:\n",
    "            r[i.start()] = 1\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = [word2label(i, words) for i in range(len(words))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12445, 12445)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#随机打乱数据\n",
    "from sklearn.model_selection import train_test_split\n",
    "words, _, labels, _ = train_test_split(words, labels, test_size=0., random_state=42)\n",
    "len(words), len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word to Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-31 19:57:37,187 : INFO : collecting all words and their counts\n",
      "2017-07-31 19:57:37,189 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-07-31 19:57:37,275 : INFO : PROGRESS: at sentence #10000, processed 314194 words, keeping 37037 word types\n",
      "2017-07-31 19:57:37,294 : INFO : collected 42029 word types from a corpus of 390502 raw words and 12445 sentences\n",
      "2017-07-31 19:57:37,295 : INFO : Loading a fresh vocabulary\n",
      "2017-07-31 19:57:37,395 : INFO : min_count=1 retains 42029 unique words (100% of original 42029, drops 0)\n",
      "2017-07-31 19:57:37,395 : INFO : min_count=1 leaves 390502 word corpus (100% of original 390502, drops 0)\n",
      "2017-07-31 19:57:37,513 : INFO : deleting the raw counts dictionary of 42029 items\n",
      "2017-07-31 19:57:37,515 : INFO : sample=0.001 downsamples 23 most-common words\n",
      "2017-07-31 19:57:37,516 : INFO : downsampling leaves estimated 303299 word corpus (77.7% of prior 390502)\n",
      "2017-07-31 19:57:37,517 : INFO : estimated required memory for 42029 words and 128 dimensions: 64052196 bytes\n",
      "2017-07-31 19:57:37,652 : INFO : resetting layer weights\n",
      "2017-07-31 19:57:38,241 : INFO : training model with 20 workers on 42029 vocabulary and 128 features, using sg=1 hs=0 sample=0.001 negative=8 window=8\n",
      "2017-07-31 19:57:39,378 : INFO : PROGRESS: at 0.51% examples, 27578 words/s, in_qsize 40, out_qsize 1\n",
      "2017-07-31 19:57:40,409 : INFO : PROGRESS: at 4.08% examples, 115008 words/s, in_qsize 34, out_qsize 5\n",
      "2017-07-31 19:57:41,717 : INFO : PROGRESS: at 7.78% examples, 136506 words/s, in_qsize 40, out_qsize 0\n",
      "2017-07-31 19:57:42,848 : INFO : PROGRESS: at 10.99% examples, 144999 words/s, in_qsize 40, out_qsize 1\n",
      "2017-07-31 19:57:43,970 : INFO : PROGRESS: at 13.78% examples, 146426 words/s, in_qsize 39, out_qsize 0\n",
      "2017-07-31 19:57:44,973 : INFO : PROGRESS: at 17.75% examples, 160310 words/s, in_qsize 39, out_qsize 0\n",
      "2017-07-31 19:57:46,053 : INFO : PROGRESS: at 20.58% examples, 159994 words/s, in_qsize 39, out_qsize 0\n",
      "2017-07-31 19:57:47,067 : INFO : PROGRESS: at 23.62% examples, 162675 words/s, in_qsize 39, out_qsize 0\n",
      "2017-07-31 19:57:48,407 : INFO : PROGRESS: at 26.44% examples, 157968 words/s, in_qsize 39, out_qsize 0\n",
      "2017-07-31 19:57:49,426 : INFO : PROGRESS: at 30.53% examples, 165761 words/s, in_qsize 39, out_qsize 0\n",
      "2017-07-31 19:57:50,453 : INFO : PROGRESS: at 33.46% examples, 166414 words/s, in_qsize 39, out_qsize 0\n",
      "2017-07-31 19:57:51,501 : INFO : PROGRESS: at 36.42% examples, 166730 words/s, in_qsize 38, out_qsize 1\n",
      "2017-07-31 19:57:52,768 : INFO : PROGRESS: at 39.23% examples, 163921 words/s, in_qsize 40, out_qsize 0\n",
      "2017-07-31 19:57:53,787 : INFO : PROGRESS: at 42.28% examples, 165133 words/s, in_qsize 38, out_qsize 1\n",
      "2017-07-31 19:57:54,830 : INFO : PROGRESS: at 45.74% examples, 167368 words/s, in_qsize 39, out_qsize 0\n",
      "2017-07-31 19:57:56,044 : INFO : PROGRESS: at 49.45% examples, 168590 words/s, in_qsize 40, out_qsize 0\n",
      "2017-07-31 19:57:57,168 : INFO : PROGRESS: at 52.38% examples, 168006 words/s, in_qsize 38, out_qsize 1\n",
      "2017-07-31 19:57:58,179 : INFO : PROGRESS: at 55.97% examples, 170363 words/s, in_qsize 39, out_qsize 0\n",
      "2017-07-31 19:57:59,197 : INFO : PROGRESS: at 58.77% examples, 170221 words/s, in_qsize 39, out_qsize 0\n",
      "2017-07-31 19:58:00,202 : INFO : PROGRESS: at 61.97% examples, 171246 words/s, in_qsize 39, out_qsize 0\n",
      "2017-07-31 19:58:01,293 : INFO : PROGRESS: at 64.91% examples, 170880 words/s, in_qsize 40, out_qsize 0\n",
      "2017-07-31 19:58:02,447 : INFO : PROGRESS: at 67.82% examples, 170087 words/s, in_qsize 39, out_qsize 0\n",
      "2017-07-31 19:58:03,458 : INFO : PROGRESS: at 72.30% examples, 174015 words/s, in_qsize 40, out_qsize 0\n",
      "2017-07-31 19:58:04,473 : INFO : PROGRESS: at 74.61% examples, 172609 words/s, in_qsize 39, out_qsize 0\n",
      "2017-07-31 19:58:05,671 : INFO : PROGRESS: at 77.92% examples, 172407 words/s, in_qsize 38, out_qsize 1\n",
      "2017-07-31 19:58:06,844 : INFO : PROGRESS: at 80.75% examples, 171297 words/s, in_qsize 40, out_qsize 0\n",
      "2017-07-31 19:58:07,982 : INFO : PROGRESS: at 83.41% examples, 170225 words/s, in_qsize 39, out_qsize 0\n",
      "2017-07-31 19:58:09,066 : INFO : PROGRESS: at 86.49% examples, 170274 words/s, in_qsize 39, out_qsize 0\n",
      "2017-07-31 19:58:10,076 : INFO : PROGRESS: at 89.44% examples, 170469 words/s, in_qsize 38, out_qsize 1\n",
      "2017-07-31 19:58:11,109 : INFO : PROGRESS: at 92.88% examples, 171487 words/s, in_qsize 40, out_qsize 1\n",
      "2017-07-31 19:58:12,265 : INFO : PROGRESS: at 96.08% examples, 171354 words/s, in_qsize 31, out_qsize 0\n",
      "2017-07-31 19:58:12,725 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2017-07-31 19:58:12,783 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2017-07-31 19:58:12,788 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2017-07-31 19:58:12,809 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2017-07-31 19:58:12,815 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2017-07-31 19:58:12,821 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2017-07-31 19:58:12,898 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2017-07-31 19:58:12,923 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2017-07-31 19:58:13,067 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2017-07-31 19:58:13,145 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2017-07-31 19:58:13,161 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2017-07-31 19:58:13,170 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2017-07-31 19:58:13,171 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-07-31 19:58:13,198 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-07-31 19:58:13,210 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-07-31 19:58:13,213 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-07-31 19:58:13,242 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-07-31 19:58:13,256 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-07-31 19:58:13,258 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-07-31 19:58:13,273 : INFO : PROGRESS: at 100.00% examples, 173218 words/s, in_qsize 0, out_qsize 1\n",
      "2017-07-31 19:58:13,274 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-07-31 19:58:13,275 : INFO : training on 7810040 raw words (6065643 effective words) took 35.0s, 173208 effective words/s\n",
      "2017-07-31 19:58:13,277 : INFO : saving Word2Vec object under word2vec.model, separately None\n",
      "2017-07-31 19:58:13,278 : INFO : not storing attribute syn0norm\n",
      "2017-07-31 19:58:13,279 : INFO : not storing attribute cum_table\n",
      "2017-07-31 19:58:14,059 : INFO : saved word2vec.model\n",
      "2017-07-31 19:58:14,061 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \n",
    "                    level=logging.INFO)\n",
    "word2vec = gensim.models.Word2Vec(np.array(words),\n",
    "                                  min_count=1,\n",
    "                                  size=EMBEDDING_SIZE,\n",
    "                                  workers=20,\n",
    "                                  iter=20,\n",
    "                                  window=8,\n",
    "                                  negative=8,\n",
    "                                  sg=1)\n",
    "word2vec.save('word2vec.model')\n",
    "#预先归一化\n",
    "word2vec.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-31 19:58:14,532 : INFO : loading Word2Vec object from word2vec.model\n",
      "2017-07-31 19:58:15,078 : INFO : loading wv recursively from word2vec.model.wv.* with mmap=None\n",
      "2017-07-31 19:58:15,079 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-07-31 19:58:15,080 : INFO : setting ignored attribute cum_table to None\n",
      "2017-07-31 19:58:15,082 : INFO : loaded word2vec.model\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-31 19:58:15,192 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('开通', 0.7537972927093506),\n",
       " ('第二条', 0.750749945640564),\n",
       " ('公交', 0.7351782321929932),\n",
       " ('打车', 0.7187823057174683),\n",
       " ('地下铁路', 0.7162506580352783),\n",
       " ('白楼', 0.7160401344299316),\n",
       " ('坐地铁', 0.7159051895141602),\n",
       " ('B C', 0.7105467319488525),\n",
       " ('线', 0.7093311548233032),\n",
       " ('直达', 0.7091985940933228)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('地铁')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 256\n",
    "BATCH_SIZE = 1024\n",
    "L1_FACTOR = 0.01 #通过L1正则项，使得输出更加稀疏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def micro_precision(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    计算 micro-averaged precision\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    micro_precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return micro_precision\n",
    "\n",
    "def macro_precision(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    计算 macro-averaged precision\n",
    "    \"\"\"\n",
    "    true_positives = tf.reduce_sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=[0, 1])\n",
    "    predicted_positives = tf.reduce_sum(K.round(K.clip(y_pred, 0, 1)), axis=[0, 1])\n",
    "    macro_precision = K.sum(true_positives / (predicted_positives + K.epsilon())) / 5\n",
    "    return macro_precision\n",
    "\n",
    "def micro_recall(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    计算 micro-averaged recall\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    micro_recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return micro_recall\n",
    "\n",
    "def macro_recall(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    计算 macro-averaged recall\n",
    "    \"\"\"\n",
    "    true_positives = tf.reduce_sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=[0, 1])\n",
    "    possible_positives = tf.reduce_sum(K.round(K.clip(y_true, 0, 1)), axis=[0, 1])\n",
    "    macro_recall = K.sum(true_positives / (possible_positives + K.epsilon())) / 5\n",
    "    return macro_recall\n",
    "\n",
    "def macro_f1_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    计算 macro-averaged f1-score\n",
    "    \"\"\"\n",
    "    \n",
    "    c1 = tf.reduce_sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=[0, 1])\n",
    "    c2 = tf.reduce_sum(K.round(K.clip(y_pred, 0, 1)), axis=[0, 1])\n",
    "    c3 = tf.reduce_sum(K.round(K.clip(y_true, 0, 1)), axis=[0, 1])\n",
    "    c = K.int_shape(c1)[-1]\n",
    "\n",
    "    if c3 == K.zeros((c,)):\n",
    "        return 0\n",
    "    precision = c1 / (c2 + K.epsilon())\n",
    "    recall = c1 / (c3 + K.epsilon())\n",
    "    macro_f1_score = 2 / c * K.sum((precision * recall) / (precision + recall + K.epsilon()))\n",
    "    return macro_f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Builde Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, LSTM, TimeDistributed, Input, Masking, Bidirectional \n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence = Input(shape=(MAXLEN, EMBEDDING_SIZE))\n",
    "mask = Masking(mask_value=0.)(sequence)\n",
    "blstm = Bidirectional(LSTM(64, return_sequences=True), merge_mode='sum')(mask)\n",
    "blstm = Bidirectional(LSTM(32, return_sequences=True), merge_mode='sum')(blstm)\n",
    "output = TimeDistributed(Dense(5, activation='softmax', activity_regularizer=l1(0.01)))(blstm)\n",
    "model = Model(inputs=sequence, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[macro_f1_score], sample_weight_mode='temporal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_x(z):\n",
    "    \"\"\"\n",
    "    从分词后的list中输出训练样本\n",
    "    超过MAXLEN则截断，不足补0\n",
    "    \"\"\"\n",
    "    gen = np.vstack((word2vec[z[:MAXLEN]], np.zeros((MAXLEN-len(z[:MAXLEN]), EMBEDDING_SIZE))))\n",
    "    return gen #80x128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_y(z):\n",
    "    \"\"\"\n",
    "    将输出序列转换为独热码\n",
    "    超过MAXLEN则截断，不足补0\n",
    "    \"\"\"\n",
    "    gen = np_utils.to_categorical(np.array(z[:MAXLEN] + [0]*(MAXLEN-len(z[:MAXLEN]))), 5)\n",
    "    return gen #80x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generator(data, targets, batch_size):\n",
    "    \"\"\"\n",
    "    为节省内存使用生成器\n",
    "    \"\"\"\n",
    "    t = 0\n",
    "    while True:\n",
    "        if t >= len(data): t = 0\n",
    "        x = np.zeros((batch_size, MAXLEN, EMBEDDING_SIZE))\n",
    "        y = np.zeros((batch_size, MAXLEN, 5))\n",
    "        for i in range(batch_size):\n",
    "            n = i + t\n",
    "            if n > len(data)-1:\n",
    "                break\n",
    "            x[i, :, :] = gen_x(data[n])\n",
    "            y[i, :, :] = gen_y(targets[n])\n",
    "        t += batch_size\n",
    "        yield (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8711, 1867, 1867)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain, Xval, Ytrain, Yval = train_test_split(words, labels, test_size=0.3, random_state=42)\n",
    "Xval, Xtest, Yval, Ytest = train_test_split(Xval, Yval, test_size=0.5, random_state=42)\n",
    "len(Xtrain), len(Xval), len(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xt = np.array([gen_x(i) for i in Xtrain])\n",
    "yt = np.array([gen_y(i) for i in Ytrain])\n",
    "xv = np.array([gen_x(i) for i in Xval])\n",
    "yv = np.array([gen_y(i) for i in Yval])\n",
    "xte = np.array([gen_x(i) for i in Xtest])\n",
    "yte = np.array([gen_y(i) for i in Ytest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8711, 80, 5)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain, Xval, Ytrain, Yval = train_test_split(words, labels, test_size=0.15, random_state=42)\n",
    "xt = np.array([gen_x(i) for i in Xtrain])\n",
    "yt = np.array([gen_y(i) for i in Ytrain])\n",
    "xv = np.array([gen_x(i) for i in Xval])\n",
    "yv = np.array([gen_y(i) for i in Yval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "fh = h5py.File('test_ner_data.h5', 'w')\n",
    "fh.create_dataset('train_ner_x', data=xt)\n",
    "fh.create_dataset('train_ner_y', data=yt)\n",
    "fh.create_dataset('val_ner_x', data=xv)\n",
    "fh.create_dataset('val_ner_y', data=yv)\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def macro_f1_score(x, y_true):\n",
    "    \"\"\"\n",
    "    计算 macro-averaged f1-score\n",
    "    \"\"\"\n",
    "    x = np.array([gen_x(x[i]) for i in range(len(x))])\n",
    "    y_true = np.array([gen_y(y_true[i]) for i in range(len(y_true))])\n",
    "    y_pred = model.predict(x)\n",
    "    y_true = np.reshape(np.array([t for t in np.argmax(y_true, -1)]), -1)\n",
    "    y_pred = np.reshape(np.array([t for t in np.argmax(y_pred, -1)]), -1)\n",
    "    return f1_score(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#gen_train = data_generator(Xtrain, Ytrain, BATCH_SIZE)\n",
    "#gen_val = data_generator(Xval, Yval, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tensorboard = TensorBoard(log_dir='tb_logs/1', histogram_freq=1, write_graph=True, write_images=True)\n",
    "#filepath = 'cp_logs/weights.{epoch:03d}-{val_macro_f1_score:.12f}.hdf5'\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='val_macro_f1_score', verbose=0, save_best_only=True)\n",
    "#earlystopping = EarlyStopping(monitor='val_loss', verbose=1, patience=2)\n",
    "#reducelr = ReduceLROnPlateau(monitor='val_loss', verbose=1, factor=0.8, patience=3, min_lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#h = model.fit_generator(gen_train, \n",
    "#                        steps_per_epoch=len(Xtrain)//BATCH_SIZE, \n",
    "#                        epochs=10, \n",
    "#                        validation_data=gen_val, \n",
    "#                        validation_steps=len(Xval)//BATCH_SIZE,\n",
    "#                        verbose=1,\n",
    "#                        callbacks=[tensorboard, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_weight = np.array([i for i in range(1, 81)][::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for a in range(10577):\n",
    "    sample_weight=np.vstack((np.array([i for i in range(1, 81)][::-1]), sample_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10578, 80)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS 1\n",
      "Train on 10578 samples, validate on 1867 samples\n",
      "Epoch 1/1\n",
      " 6144/10578 [================>.............] - ETA: 26s - loss: 893.1186 - macro_f1_score: 0.0051    "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-1c4c4bd4f4d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'EPOCHS'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mF1_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmacro_f1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'macro-averaged f1-score = %.6f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmacro_f1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "F1_score = []\n",
    "for e in range(NUM_EPOCHS):    \n",
    "    print('EPOCHS', e+1)\n",
    "    h = model.fit(x=xt, y=yt, batch_size=BATCH_SIZE, epochs=1, validation_data=(xv, yv), sample_weight=sample_weight)\n",
    "    F1_score.append(macro_f1_score(Xval, Yval))\n",
    "    print('macro-averaged f1-score = %.6f' % (macro_f1_score(Xval, Yval)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8711 samples, validate on 1867 samples\n",
      "Epoch 1/256\n",
      "8711/8711 [==============================] - 58s - loss: 796.2748 - macro_f1_score: 0.0227 - val_loss: 754.2688 - val_macro_f1_score: 0.1032\n",
      "Epoch 2/256\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-e82964878810>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1498\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2227\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2229\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x=xt, y=yt, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, validation_data=(xv, yv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#F1_score = []\n",
    "#for e in range(NUM_EPOCHS):    \n",
    "#    print('EPOCHS', e+1)\n",
    "#    h = model.fit_generator(gen_train, \n",
    "#                            steps_per_epoch=len(Xtrain)//BATCH_SIZE, \n",
    "#                            epochs=1, \n",
    "#                            validation_data=gen_val, \n",
    "#                            validation_steps=len(Xval)//BATCH_SIZE,\n",
    "#                            verbose=1)\n",
    "#    F1_score.append(macro_f1_score(Xval, Yval))\n",
    "#    print('macro-averaged f1-score = %.6f' % (macro_f1_score(Xval, Yval)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.save_weights('train_1.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def predict_data(data, batch_size):\n",
    "#    \"\"\"\n",
    "#    输出预测结果（原始数据，未整理）\n",
    "#    \"\"\"\n",
    "#    data = np.array(data)\n",
    "#    batches = [range(batch_size*i, min(len(data), batch_size*(i+1))) for i in range(len(data)//batch_size)]\n",
    "#    p = model.predict(np.array(list(map(gen_x, data[batches[0]]))), verbose=1)\n",
    "#    for i in batches[1:]:\n",
    "#        print(min(i), 'done.')\n",
    "#        p = np.vstack((p, model.predict(np.array(list(map(gen_x, data[i]))), verbose=1)))\n",
    "#    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict = predict_data(Xtest, BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "84px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
