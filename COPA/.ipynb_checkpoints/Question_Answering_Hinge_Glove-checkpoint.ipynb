{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Load-Data\" data-toc-modified-id=\"Load-Data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Load Data</a></div><div class=\"lev1 toc-item\"><a href=\"#Word-Segmentation\" data-toc-modified-id=\"Word-Segmentation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Word Segmentation</a></div><div class=\"lev1 toc-item\"><a href=\"#Tokenize-Text\" data-toc-modified-id=\"Tokenize-Text-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Tokenize Text</a></div><div class=\"lev1 toc-item\"><a href=\"#Create-Word-Embeddings-with-GloVe\" data-toc-modified-id=\"Create-Word-Embeddings-with-GloVe-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Create Word Embeddings with GloVe</a></div><div class=\"lev2 toc-item\"><a href=\"#Read-Glove\" data-toc-modified-id=\"Read-Glove-41\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Read Glove</a></div><div class=\"lev2 toc-item\"><a href=\"#Use-Glove-to-Initialize-Embedding-Matrix\" data-toc-modified-id=\"Use-Glove-to-Initialize-Embedding-Matrix-42\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Use Glove to Initialize Embedding Matrix</a></div><div class=\"lev1 toc-item\"><a href=\"#Build-Dataset\" data-toc-modified-id=\"Build-Dataset-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Build Dataset</a></div><div class=\"lev1 toc-item\"><a href=\"#Save-Data\" data-toc-modified-id=\"Save-Data-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Save Data</a></div><div class=\"lev1 toc-item\"><a href=\"#Checkpoint\" data-toc-modified-id=\"Checkpoint-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Checkpoint</a></div><div class=\"lev1 toc-item\"><a href=\"#Load-Model\" data-toc-modified-id=\"Load-Model-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Load Model</a></div><div class=\"lev2 toc-item\"><a href=\"#Set-Hyperparameters\" data-toc-modified-id=\"Set-Hyperparameters-81\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Set Hyperparameters</a></div><div class=\"lev2 toc-item\"><a href=\"#Import-Libraries\" data-toc-modified-id=\"Import-Libraries-82\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Import Libraries</a></div><div class=\"lev2 toc-item\"><a href=\"#Build-Graph\" data-toc-modified-id=\"Build-Graph-83\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Build Graph</a></div><div class=\"lev1 toc-item\"><a href=\"#Train\" data-toc-modified-id=\"Train-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Train</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(path, name):\n",
    "    \"\"\"\n",
    "    Load date from file\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file) as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        item = json.loads(line)\n",
    "        data.append(item[name])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "premise = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-all.json', 'premise')\n",
    "asks_for = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-all.json', 'asks-for')\n",
    "alternative1 = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-all.json', 'alternative1')\n",
    "alternative2 = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-all.json', 'alternative2')\n",
    "\n",
    "for i in range(len(premise)):\n",
    "    if asks_for[i] == 'cause':\n",
    "        premise[i] += ' What was the CAUSE of this?'\n",
    "    else:\n",
    "        premise[i] += ' What happened as a RESULT'\n",
    "\n",
    "premise.extend(premise)\n",
    "alternative = []\n",
    "alternative.extend(alternative1)\n",
    "alternative.extend(alternative2)\n",
    "\n",
    "rawLabel = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-all.json', 'most-plausible-alternative')\n",
    "rawLabel = [int(l) for l in rawLabel]\n",
    "\n",
    "l = [0] * len(premise)\n",
    "for i in range(len(rawLabel)):\n",
    "    if rawLabel[i] == 1:\n",
    "        l[i] = 1\n",
    "        l[i+len(rawLabel)] = 0\n",
    "    if rawLabel[i] == 2:\n",
    "        l[i] = 0\n",
    "        l[i+len(rawLabel)] = 1\n",
    "        \n",
    "label = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import regexp_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replDict = {\"woman's\": 'woman', \"man's\": 'man', \"patient's\": 'patient', \"student's\": 'student', \"boy's\": 'boy', \n",
    "            \"friend's\": 'friend', \"enemy's\": 'enemy', \"parent's\": 'parent', \"humanitarian's\": 'humanitarian', \n",
    "            \"child's\": 'child', \"professor's\": 'professor', \"daughter's\": 'daughter', \"mother's\": 'mother', \n",
    "            \"children's\": 'children', \"teller's\": 'teller', \"company's\": 'company', \"group's\": 'group', \n",
    "            \"laptop's\": 'laptop', \"girl's\": 'girl', \"salesman's\": 'salesman', \"cook's\": 'cook', \"car's\": 'car', \n",
    "            \"offender's\": 'offender', \"detective's\": 'detective', \"librarian's\": 'librarian', \"caller's\": 'caller', \n",
    "            \"victim's\": 'victim', \"interviewer's\": 'interviewer', \"ship's\": 'ship', \"site's\": 'site', \n",
    "            \"chandelier's\": 'chandelier', \"bully's\": 'bully', \"river's\": 'river', \"puppy's\": 'puppy', \n",
    "            \"pilot's\": 'pilot', \"girlfriend's\": 'girlfriend', \"politician's\": 'politician', \"couple's\": 'couple', \n",
    "            \"son's\": 'son', \"actor's\": 'actor', \"neighbor's\": 'neighbor', \"nation's\": 'nation', \n",
    "            \"classmate's\": 'classmate', \"businessman's\": 'businessman', \"architect's\": 'architect', \n",
    "            \"imposter's\": 'imposter', \"kidnapper's\": 'kidnapper', \"colleague's\": 'colleague', \"flower's\": 'flower',\n",
    "            \"bull's\": 'bull', \"employee's\": 'employee', \"wouldn't\": 'wouldn', \"team's\": 'team', \"other's\": 'other', \n",
    "            \"writer's\": 'writer', \"baby's\": 'baby', \"attacker's\": 'attacker', \"uncle's\": 'uncle', \"driver's\": 'driver'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cut(s):\n",
    "    \"\"\"\n",
    "    Word segmentation\n",
    "    \"\"\"\n",
    "    pattern = r'''\n",
    "              (?x)                   # set flag to allow verbose regexps \n",
    "              (?:[A-Z]\\.)+           # abbreviations, e.g. U.S.A. \n",
    "              |\\w+(?:[-&']\\w+)*      # words w/ optional internal hyphens/apostrophe  \n",
    "            '''  \n",
    "    return regexp_tokenize(s, pattern=pattern)\n",
    "\n",
    "def clean(s):\n",
    "    \"\"\"\n",
    "    Clean data \n",
    "    \"\"\"\n",
    "    for i in range(len(s)):\n",
    "        if s[i] == \"couldn't\":\n",
    "            s[i] = 'could'\n",
    "            s.insert(i+1, 'not')\n",
    "    s = [i for i in s if i != '']\n",
    "    return [replDict.get(i.lower(), i.lower()) for i in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pWords = [clean(cut(s)) for s in premise]\n",
    "aWords = [clean(cut(s)) for s in alternative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_P_LEN = 19\n",
    "MAX_A_LEN = 11\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3370 unique tokens.\n",
      "Distribution of premise lengths (number of words):\n",
      "Min: 8   Max: 19   Mean: 11.668   Med: 12.000\n",
      "Distribution of alternative lengths (number of words):\n",
      "Min: 2   Max: 11   Mean: 5.074   Med: 5.000\n"
     ]
    }
   ],
   "source": [
    "tokWords = []\n",
    "tokWords.extend(pWords)\n",
    "tokWords.extend(aWords)\n",
    "tokenizer = Tokenizer(filters='')\n",
    "tokTexts = [' '.join(i) for i in tokWords]\n",
    "tokenizer.fit_on_texts(tokTexts)\n",
    "word2index = tokenizer.word_index\n",
    "index2word = {i: w for w, i in word2index.items()}\n",
    "sentLens = np.array([len(i) for i in pWords])\n",
    "print('Found %s unique tokens.' % len(word2index))\n",
    "print('Distribution of premise lengths (number of words):')\n",
    "print('Min: {:d}   Max: {:d}   Mean: {:.3f}   Med: {:.3f}'.format(np.min(sentLens), np.max(sentLens), np.mean(sentLens), np.median(sentLens)))\n",
    "sentLens = np.array([len(i) for i in aWords])\n",
    "print('Distribution of alternative lengths (number of words):')\n",
    "print('Min: {:d}   Max: {:d}   Mean: {:.3f}   Med: {:.3f}'.format(np.min(sentLens), np.max(sentLens), np.mean(sentLens), np.median(sentLens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/Users/lizhn7/Downloads/EXPERIMENT/COPA/Question_Answering_Hinge_Glove/data/index.pkl', 'wb') as fp:\n",
    "    pickle.dump((word2index), fp, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Word Embeddings with GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "VOCAB_SIZE = 3371\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#glove_n_symbols = !wc -l /Users/lizhn7/Downloads/DATA/Glove/glove.42B.300d.txt\n",
    "#glove_n_symbols = int(glove_n_symbols[0].split()[0])\n",
    "glove_n_symbols = 1917495"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_index_dict = {}\n",
    "glove_embedding_weights = np.empty((glove_n_symbols, EMBEDDING_DIM))\n",
    "globale_scale = 0.1\n",
    "with open('/Users/lizhn7/Downloads/DATA/Glove/glove.42B.300d.txt', 'r') as fp:\n",
    "    index = 0\n",
    "    for l in fp:\n",
    "        l = l.strip().split()\n",
    "        word = l[0]\n",
    "        glove_index_dict[word] = index\n",
    "        glove_embedding_weights[index, :] = [float(n) for n in l[1:]]\n",
    "        index += 1\n",
    "glove_embedding_weights *= globale_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Glove to Initialize Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer, LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate random embedding with same scale as glove\n",
    "np.random.seed(SEED)\n",
    "shape = (VOCAB_SIZE, EMBEDDING_DIM)\n",
    "scale = glove_embedding_weights.std() * np.sqrt(12) / 2 \n",
    "embedding = np.random.uniform(low=-scale, high=scale, size=shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3370-99.97% tokens in vocab found in glove and copied to embedding.\n"
     ]
    }
   ],
   "source": [
    "# Copy from glove weights of words that appear in index2word\n",
    "count = 0 \n",
    "for i in range(1, VOCAB_SIZE):\n",
    "    w = index2word[i]\n",
    "    g = glove_index_dict.get(w)\n",
    "    if g is None:\n",
    "        ww = wnl.lemmatize(w)\n",
    "        g = glove_index_dict.get(ww)\n",
    "    if g is None:\n",
    "        ww = porter.stem(w)\n",
    "        g = glove_index_dict.get(ww)\n",
    "    if g is None:\n",
    "        ww = lancaster.stem(w)\n",
    "        g = glove_index_dict.get(ww)\n",
    "    if g is not None:\n",
    "        embedding[i, :] = glove_embedding_weights[g, :]\n",
    "        count += 1\n",
    "print('{num_tokens}-{per:.2f}% tokens in vocab found in glove and copied to embedding.'.format(num_tokens=count, per=count/float(VOCAB_SIZE)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'tattled' in glove_index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "356749"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_index_dict['unlaced']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"didn't\", \"didn't\", \"didn't\"]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in sum(tokWords, []) if i[-2:] == \"'t\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[i for i in replDict if i[-1] != 's']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/Users/lizhn7/Downloads/EXPERIMENT/COPA/Question_Answering_Hinge_Glove/data/index.pkl', 'rb') as fp:\n",
    "    word2index = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pSeq = [[word2index.get(w, w) for w in s] for s in pWords]\n",
    "aSeq = [[word2index.get(w, w) for w in s] for s in aWords]\n",
    "\n",
    "xp = pad_sequences(pSeq, maxlen=MAX_P_LEN, padding='post', truncating='post')\n",
    "xa = pad_sequences(aSeq, maxlen=MAX_A_LEN, padding='post', truncating='post')\n",
    "\n",
    "xpTest = xp[:500]\n",
    "xa1Test = xa[:500]\n",
    "xa2Test =  xa[500:]\n",
    "yTest = np.array(rawLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pSeq = [[word2index.get(w, w) for w in s] for s in pWords]\n",
    "aSeq = [[word2index.get(w, w) for w in s] for s in aWords]\n",
    "\n",
    "xp = pad_sequences(pSeq, maxlen=MAX_P_LEN, padding='post', truncating='post')\n",
    "xa = pad_sequences(aSeq, maxlen=MAX_A_LEN, padding='post', truncating='post')\n",
    "y = np.array(label)\n",
    "\n",
    "xpTrain, _, yTrain, _ = train_test_split(xp, y, test_size=0., random_state=SEED)\n",
    "xaTrain, _ = train_test_split(xa, test_size=0., random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pSeq = [[word2index.get(w, w) for w in s] for s in pWords]\n",
    "aSeq = [[word2index.get(w, w) for w in s] for s in aWords]\n",
    "\n",
    "xp = pad_sequences(pSeq, maxlen=MAX_P_LEN, padding='post', truncating='post')\n",
    "xa = pad_sequences(aSeq, maxlen=MAX_A_LEN, padding='post', truncating='post')\n",
    "y = np.array(label)\n",
    "\n",
    "\n",
    "xpTrain1, xpVal1, yTrain1, yVal1 = train_test_split(xp[:500], y[:500], test_size=0.2, random_state=SEED)\n",
    "xaTrain1, xaVal1 = train_test_split(xa[:500], test_size=0.2, random_state=SEED)\n",
    "xpTrain2, xpVal2, yTrain2, yVal2 = train_test_split(xp[500:], y[500:], test_size=0.2, random_state=SEED)\n",
    "xaTrain2, xaVal2 = train_test_split(xa[500:], test_size=0.2, random_state=SEED)\n",
    "\n",
    "xpTrain = np.vstack((xpTrain1, xpTrain2))\n",
    "xpVal = np.vstack((xpVal1, xpVal2))\n",
    "xaTrain = np.vstack((xaTrain1, xaTrain2))\n",
    "xaVal = np.vstack((xaVal1, xaVal2))\n",
    "yTrain = np.concatenate((yTrain1, yTrain2))\n",
    "yVal = np.concatenate((yVal1, yVal2))\n",
    "xpTest = xpVal1\n",
    "xa1Test = xaVal1\n",
    "xa2Test = xaVal2\n",
    "yTest = np.array([1 if yVal1[i] == 1 else 2 for i in range(len(yVal1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fh = h5py.File('/Users/lizhn7/Downloads/EXPERIMENT/COPA/Question_Answering_Hinge_Glove/data/embedding.h5', 'w')\n",
    "fh['embedding'] = embedding\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fh = h5py.File('/Users/lizhn7/Downloads/EXPERIMENT/COPA/Question_Answering_Hinge_Glove/data/train.h5', 'w')\n",
    "fh['xpTrain'] = xpTrain\n",
    "fh['xaTrain'] = xaTrain\n",
    "fh['yTrain'] = yTrain\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fh = h5py.File('/Users/lizhn7/Downloads/EXPERIMENT/COPA/Question_Answering_Hinge_Glove/data/test.h5', 'w')\n",
    "fh['xpTest'] = xpTest\n",
    "fh['xa1Test'] = xa1Test\n",
    "fh['xa2Test'] = xa2Test\n",
    "fh['yTest'] = yTest\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to create file (Unable to truncate a file which is already open)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-e84bad84ccf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/lizhn7/Downloads/EXPERIMENT/COPA/Question_Answering_Hinge_Glove/data/train.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'xpTrain'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxpTrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'xpVal'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxpVal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'xaTrain'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxaTrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'xaVal'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxaVal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to create file (Unable to truncate a file which is already open)"
     ]
    }
   ],
   "source": [
    "fh = h5py.File('/Users/lizhn7/Downloads/EXPERIMENT/COPA/Question_Answering_Hinge_Glove/data/train.h5', 'w')\n",
    "fh['xpTrain'] = xpTrain\n",
    "fh['xpVal'] = xpVal\n",
    "fh['xaTrain'] = xaTrain\n",
    "fh['xaVal'] = xaVal\n",
    "fh['yTrain'] = yTrain\n",
    "fh['yVal'] = yVal\n",
    "fh['xpTest'] = xpTest\n",
    "fh['xa1Test'] = xa1Test\n",
    "fh['xa2Test'] = xa2Test\n",
    "fh['yTest'] = yTest\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File('/Users/lizhn7/Downloads/EXPERIMENT/COPA/Question_Answering_Hinge_Glove/data/embedding.h5', 'r') as fh:\n",
    "    embedding = fh['embedding'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File('/Users/lizhn7/Downloads/EXPERIMENT/COPA/Question_Answering_Hinge_Glove/data/train.h5', 'r') as fh:\n",
    "    xpTrain = fh['xpTrain'][:]\n",
    "    xpVal = fh['xpVal'][:]\n",
    "    xaTrain = fh['xaTrain'][:]\n",
    "    xaVal = fh['xaVal'][:]\n",
    "    yTrain = fh['yTrain'][:]\n",
    "    yVal = fh['yVal'][:]\n",
    "    xpTest = fh['xpTest'][:]\n",
    "    xa1Test = fh['xa1Test'][:]\n",
    "    xa2Test = fh['xa2Test'][:]\n",
    "    yTest = fh['yTest'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_P_LEN = 19\n",
    "MAX_A_LEN = 11\n",
    "VOCAB_SIZE = 3371\n",
    "SEED = 42\n",
    "EMBEDDING_DIM = 300\n",
    "TUNE = False\n",
    "BATCH_SIZE = 1024\n",
    "NUM_EPOCHS = 256\n",
    "CNN_SIZE = 128\n",
    "WINDOW_SIZE = 3\n",
    "DROPOUT_RATE = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Activation, Lambda, dot\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.callbacks import*\n",
    "from keras import losses\n",
    "from keras import optimizers\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build():\n",
    "    \"\"\"\n",
    "    Build model\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=(None,))\n",
    "    emb_seq = Embedding(VOCAB_SIZE, \n",
    "                    EMBEDDING_DIM, \n",
    "                    weights=[embedding], \n",
    "                    mask_zero=False, \n",
    "                    trainable=TUNE)(inputs)\n",
    "    #dropout = Dropout(DROPOUT_RATE)(emb_seq)\n",
    "    conv = Conv1D(CNN_SIZE, \n",
    "                  WINDOW_SIZE, \n",
    "                  padding='same', \n",
    "                  activation='elu')(emb_seq)\n",
    "    pool = GlobalMaxPooling1D()(conv)\n",
    "    dropout = Dropout(DROPOUT_RATE)(pool)\n",
    "    outputs = Activation('tanh')(pool)\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "def hinge_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Define hinge loss function\n",
    "    \"\"\"\n",
    "    pos = K.sum(y_true * y_pred, axis=-1)\n",
    "    neg = K.max((1. - y_true) * y_pred, axis=-1)\n",
    "    return K.maximum(0., neg - pos + 0.009)\n",
    "\n",
    "def squared_hinge(y_true, y_pred):\n",
    "    return K.mean(K.square(K.maximum(1. - y_true * y_pred, 0.)), axis=-1)\n",
    "\n",
    "\n",
    "def hinge(y_true, y_pred):\n",
    "    return K.mean(K.maximum(1. - y_true * y_pred, 0.), axis=-1)\n",
    "\n",
    "\n",
    "def categorical_hinge(y_true, y_pred):\n",
    "    pos = K.sum(y_true * y_pred, axis=-1)\n",
    "    neg = K.max((1. - y_true) * y_pred, axis=-1)\n",
    "    return K.maximum(0., neg - pos + 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4, 10, 18])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1, 2, 3]) * np.array([4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, None, 300)         1011300   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 128)         115328    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "=================================================================\n",
      "Total params: 1,126,628\n",
      "Trainable params: 115,328\n",
      "Non-trainable params: 1,011,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_seq, a_seq = Input(shape=(MAX_P_LEN,)), Input(shape=(MAX_A_LEN,))\n",
    "p_out, a_out = model(p_seq), model(a_seq)\n",
    "similarity = dot([p_out, a_out], axes=-1, normalize=True)\n",
    "similarity = Lambda(lambda x: K.abs(x))(similarity)\n",
    "qa = Model(inputs=[p_seq, a_seq], outputs=[similarity])\n",
    "sgd = optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "qa.compile(loss=hinge_loss, optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_86 (InputLayer)            (None, 19)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_87 (InputLayer)            (None, 11)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "model_53 (Model)                 (None, 128)           775328      input_86[0][0]                   \n",
      "                                                                   input_87[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dot_25 (Dot)                     (None, 1)             0           model_53[1][0]                   \n",
      "                                                                   model_53[2][0]                   \n",
      "====================================================================================================\n",
      "Total params: 775,328\n",
      "Trainable params: 115,328\n",
      "Non-trainable params: 660,000\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "qa.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filepath = '/Users/lizhn7/Downloads/EXPERIMENT/COPA/Sentence_Classification_Glove/cp_logs/weights.{epoch:03d}-{loss:.6f}.hdf5'\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=True, mode='max')\n",
    "log_string = '/Users/lizhn7/Downloads/EXPERIMENT/COPA/Sentence_Classification_Glove/tb_logs/128_3_0.5'\n",
    "tensorboard = TensorBoard(log_dir=log_string)\n",
    "#callbacks_list = [checkpoint, tensorboard]\n",
    "callbacks_list = [tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Not a dataset (Not a dataset)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-265-aac73ec097d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                  \u001b[0;31m#callbacks=callbacks_list,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                  \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxpVal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxaVal\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myVal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                  shuffle=True)\n\u001b[0m",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1358\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1359\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1232\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1234\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1235\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1236\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/Users/ilan/minonda/conda-bld/h5py_1496887972496/work/h5py/_objects.c:2846)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/Users/ilan/minonda/conda-bld/h5py_1496887972496/work/h5py/_objects.c:2804)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;34m\"\"\"Numpy-style shape tuple giving dataset dimensions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwith_phil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/h5d.pyx\u001b[0m in \u001b[0;36mh5py.h5d.DatasetID.shape.__get__ (/Users/ilan/minonda/conda-bld/h5py_1496887972496/work/h5py/h5d.c:2793)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5d.pyx\u001b[0m in \u001b[0;36mh5py.h5d.DatasetID.shape.__get__ (/Users/ilan/minonda/conda-bld/h5py_1496887972496/work/h5py/h5d.c:2713)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/Users/ilan/minonda/conda-bld/h5py_1496887972496/work/h5py/_objects.c:2846)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/Users/ilan/minonda/conda-bld/h5py_1496887972496/work/h5py/_objects.c:2804)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5d.pyx\u001b[0m in \u001b[0;36mh5py.h5d.DatasetID.get_space (/Users/ilan/minonda/conda-bld/h5py_1496887972496/work/h5py/h5d.c:4443)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Not a dataset (Not a dataset)"
     ]
    }
   ],
   "source": [
    "history = qa.fit([xpTrain, xaTrain], \n",
    "                 yTrain,\n",
    "                 batch_size=BATCH_SIZE,\n",
    "                 epochs=NUM_EPOCHS,\n",
    "                 #callbacks=callbacks_list,\n",
    "                 validation_data=([xpVal, xaVal], yVal),\n",
    "                 shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
