{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Load-Data\" data-toc-modified-id=\"Load-Data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Load Data</a></div><div class=\"lev2 toc-item\"><a href=\"#Load-Train-Data\" data-toc-modified-id=\"Load-Train-Data-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Load Train Data</a></div><div class=\"lev2 toc-item\"><a href=\"#Load-Test-Data\" data-toc-modified-id=\"Load-Test-Data-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Load Test Data</a></div><div class=\"lev1 toc-item\"><a href=\"#Word-Segmentation\" data-toc-modified-id=\"Word-Segmentation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Word Segmentation</a></div><div class=\"lev1 toc-item\"><a href=\"#Build-Dataset\" data-toc-modified-id=\"Build-Dataset-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Build Dataset</a></div><div class=\"lev1 toc-item\"><a href=\"#Save-Data\" data-toc-modified-id=\"Save-Data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Save Data</a></div><div class=\"lev1 toc-item\"><a href=\"#Checkpoint\" data-toc-modified-id=\"Checkpoint-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Checkpoint</a></div><div class=\"lev1 toc-item\"><a href=\"#Load-Model\" data-toc-modified-id=\"Load-Model-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Load Model</a></div><div class=\"lev2 toc-item\"><a href=\"#Set-Hyperparameters\" data-toc-modified-id=\"Set-Hyperparameters-61\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Set Hyperparameters</a></div><div class=\"lev2 toc-item\"><a href=\"#Import-Libraries\" data-toc-modified-id=\"Import-Libraries-62\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Import Libraries</a></div><div class=\"lev2 toc-item\"><a href=\"#Build-Graph\" data-toc-modified-id=\"Build-Graph-63\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Build Graph</a></div><div class=\"lev2 toc-item\"><a href=\"#Model-Visualization\" data-toc-modified-id=\"Model-Visualization-64\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Model Visualization</a></div><div class=\"lev1 toc-item\"><a href=\"#Train\" data-toc-modified-id=\"Train-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Train</a></div><div class=\"lev2 toc-item\"><a href=\"#Load-Weights\" data-toc-modified-id=\"Load-Weights-71\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Load Weights</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(path, name):\n",
    "    \"\"\"\n",
    "    Load date from file\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file) as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        item = json.loads(line)\n",
    "        data.append(item[name])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "premise = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-dev.json', 'premise')\n",
    "asks_for = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-dev.json', 'asks-for')\n",
    "alternative1 = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-dev.json', 'alternative1')\n",
    "alternative2 = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-dev.json', 'alternative2')\n",
    "\n",
    "question = []\n",
    "for i in range(len(premise)):\n",
    "    if asks_for[i] == 'cause':\n",
    "        question.append('What was the CAUSE of this?')\n",
    "    else:\n",
    "        question.append('What happened as a RESULT?')\n",
    "        \n",
    "premise.extend(premise)\n",
    "alternative1.extend(alternative2)\n",
    "alternative = alternative1\n",
    "question.extend(question)\n",
    "\n",
    "premise.extend(alternative)\n",
    "alternative.extend(premise[:1000])\n",
    "\n",
    "q = []\n",
    "for i in range(len(question)):\n",
    "    if question[i] == 'What was the CAUSE of this?':\n",
    "        q.append('What happened as a RESULT?')\n",
    "    else:\n",
    "        q.append('What was the CAUSE of this?')\n",
    "question.extend(q)\n",
    "\n",
    "rawLabel = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-dev.json', 'most-plausible-alternative')\n",
    "rawLabel = [int(l) for l in rawLabel]\n",
    "\n",
    "l = [0] * 1000\n",
    "for i in range(len(rawLabel)):\n",
    "    if rawLabel[i] == 1:\n",
    "        l[i] = 1\n",
    "        l[i+len(rawLabel)] = 0\n",
    "    if rawLabel[i] == 2:\n",
    "        l[i] = 0\n",
    "        l[i+len(rawLabel)] = 1\n",
    "\n",
    "labelCe = l*2\n",
    "labelHi = [1 if i == 1 else -1 for i in l*2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "premiseTest = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-test.json', 'premise')\n",
    "asks_for = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-test.json', 'asks-for')\n",
    "alternative1 = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-test.json', 'alternative1')\n",
    "alternative2 = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-test.json', 'alternative2')\n",
    "\n",
    "questionTest = []\n",
    "for i in range(len(premiseTest)):\n",
    "    if asks_for[i] == 'cause':\n",
    "        questionTest.append('What was the CAUSE of this?')\n",
    "    else:\n",
    "        questionTest.append('What happened as a RESULT?')\n",
    "        \n",
    "rawLabel = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-test.json', 'most-plausible-alternative')\n",
    "labelTest = [int(l) for l in rawLabel]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replDict = {\"woman's\": 'woman', \"man's\": 'man', \"patient's\": 'patient', \"student's\": 'student', \"boy's\": 'boy', \n",
    "            \"friend's\": 'friend', \"enemy's\": 'enemy', \"parent's\": 'parent', \"humanitarian's\": 'humanitarian', \n",
    "            \"child's\": 'child', \"professor's\": 'professor', \"daughter's\": 'daughter', \"mother's\": 'mother', \n",
    "            \"children's\": 'children', \"teller's\": 'teller', \"company's\": 'company', \"group's\": 'group', \n",
    "            \"laptop's\": 'laptop', \"girl's\": 'girl', \"salesman's\": 'salesman', \"cook's\": 'cook', \"car's\": 'car', \n",
    "            \"offender's\": 'offender', \"detective's\": 'detective', \"librarian's\": 'librarian', \"caller's\": 'caller', \n",
    "            \"victim's\": 'victim', \"interviewer's\": 'interviewer', \"ship's\": 'ship', \"site's\": 'site', \n",
    "            \"chandelier's\": 'chandelier', \"bully's\": 'bully', \"river's\": 'river', \"puppy's\": 'puppy', \n",
    "            \"pilot's\": 'pilot', \"girlfriend's\": 'girlfriend', \"politician's\": 'politician', \"couple's\": 'couple', \n",
    "            \"son's\": 'son', \"actor's\": 'actor', \"neighbor's\": 'neighbor', \"nation's\": 'nation', \n",
    "            \"classmate's\": 'classmate', \"businessman's\": 'businessman', \"architect's\": 'architect', \n",
    "            \"imposter's\": 'imposter', \"kidnapper's\": 'kidnapper', \"colleague's\": 'colleague', \"flower's\": 'flower',\n",
    "            \"bull's\": 'bull', \"employee's\": 'employee', \"wouldn't\": 'wouldn', \"team's\": 'team', \"other's\": 'other', \n",
    "            \"writer's\": 'writer', \"baby's\": 'baby', \"attacker's\": 'attacker', \"uncle's\": 'uncle', \"driver's\": 'driver'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cut(s):\n",
    "    \"\"\"\n",
    "    Word segmentation\n",
    "    \"\"\"\n",
    "    pattern = r'''\n",
    "              (?x)                   # set flag to allow verbose regexps \n",
    "              (?:[A-Z]\\.)+           # abbreviations, e.g. U.S.A. \n",
    "              |\\w+(?:[-&']\\w+)*      # words w/ optional internal hyphens/apostrophe  \n",
    "            '''  \n",
    "    return regexp_tokenize(s, pattern=pattern)\n",
    "\n",
    "def clean(s):\n",
    "    \"\"\"\n",
    "    Clean data \n",
    "    \"\"\"\n",
    "    for i in range(len(s)):\n",
    "        if s[i] == \"couldn't\":\n",
    "            s[i] = 'could'\n",
    "            s.insert(i+1, 'not')\n",
    "    s = [i for i in s if i != '']\n",
    "    return [replDict.get(i.lower(), i.lower()) for i in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preWords = [clean(cut(s)) for s in premise]\n",
    "altWords = [clean(cut(s)) for s in alternative]\n",
    "queWords = [clean(cut(s)) for s in question]\n",
    "pretestWords = [clean(cut(s)) for s in premiseTest]\n",
    "alt1Words = [clean(cut(s)) for s in alternative1]\n",
    "alt2Words = [clean(cut(s)) for s in alternative2]\n",
    "quetestWords = [clean(cut(s)) for s in questionTest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 13\n",
    "MAX_Q_LEN = 6\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/Users/lizhn7/Downloads/EXPERIMENT/COPA/Sentence_Classification_Glove/data/index.pkl', 'rb') as fp:\n",
    "    word2index = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preSeq = [[word2index[w] for w in s] for s in preWords]\n",
    "altSeq = [[word2index[w] for w in s] for s in altWords]\n",
    "\n",
    "xpTrain = pad_sequences(preSeq, maxlen=MAX_LEN, padding='pre', truncating='pre')\n",
    "xaTrain = pad_sequences(altSeq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "yceTrain = np.array(labelCe)\n",
    "yhiTrain = np.array(labelHi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pretestSeq = [[word2index[w] for w in s] for s in pretestWords]\n",
    "alt1Seq = [[word2index[w] for w in s] for s in alt1Words]\n",
    "alt2Seq = [[word2index[w] for w in s] for s in alt2Words]\n",
    "\n",
    "xpTest = pad_sequences(pretestSeq, maxlen=MAX_LEN, padding='pre', truncating='pre')\n",
    "xa1Test = pad_sequences(alt1Seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "xa2Test = pad_sequences(alt2Seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "yTest = np.array(labelTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokWords = queWords.copy()\n",
    "tokWords.extend(quetestWords)\n",
    "tokTexts = [' '.join(i) for i in tokWords]\n",
    "tokenizer = Tokenizer(filters='')\n",
    "tokenizer.fit_on_texts(tokTexts)\n",
    "qword2index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(qword2index))\n",
    "\n",
    "queSeq = tokenizer.texts_to_sequences(question)\n",
    "xqTrain = pad_sequences(queSeq, maxlen=MAX_Q_LEN, padding='post', truncating='post')\n",
    "\n",
    "quetestSeq = tokenizer.texts_to_sequences(questionTest)\n",
    "xqTest = pad_sequences(quetestSeq, maxlen=MAX_Q_LEN, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xpTrain, _, xaTrain, _ = train_test_split(xpTrain, xaTrain, test_size=0., random_state=SEED)\n",
    "xqTrain, _ = train_test_split(xqTrain, test_size=0., random_state=SEED)\n",
    "yceTrain, _, yhiTrain, _ = train_test_split(yceTrain, yhiTrain, test_size=0., random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fh = h5py.File('/Users/lizhn7/Downloads/EXPERIMENT/COPA/RN/data/train.h5', 'w')\n",
    "fh['xpTrain'] = xpTrain\n",
    "fh['xaTrain'] = xaTrain\n",
    "fh['xqTrain'] = xqTrain\n",
    "fh['yceTrain'] = yceTrain\n",
    "fh['yhiTrain'] = yhiTrain\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fh = h5py.File('/Users/lizhn7/Downloads/EXPERIMENT/COPA/RN/data/test.h5', 'w')\n",
    "fh['xpTest'] = xpTest\n",
    "fh['xqTest'] = xqTest\n",
    "fh['xa1Test'] = xa1Test\n",
    "fh['xa2Test'] = xa2Test\n",
    "fh['yTest'] = yTest \n",
    "fh.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "883px",
    "left": "0px",
    "right": "777px",
    "top": "106px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
