{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Dataset-Preprocessing\" data-toc-modified-id=\"Dataset-Preprocessing-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Dataset Preprocessing</a></div><div class=\"lev1 toc-item\"><a href=\"#Word-Segmentation\" data-toc-modified-id=\"Word-Segmentation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Word Segmentation</a></div><div class=\"lev1 toc-item\"><a href=\"#Tokenize-Text\" data-toc-modified-id=\"Tokenize-Text-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Tokenize Text</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/lizhn7/Downloads/DATA/news/sample-1M.jsonl') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "content = []\n",
    "for line in lines:\n",
    "    item = json.loads(line)\n",
    "    if item['media-type'] == 'Blog':\n",
    "        content.append(item['content'])\n",
    "\n",
    "def write_to_file(content, name):\n",
    "    with open(name, 'a', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(content, ensure_ascii=False) + '\\n')\n",
    "        f.close()\n",
    "        \n",
    "def gen_data(data):\n",
    "    for i in range(len(data)):\n",
    "        yield {\n",
    "            'content': data[i]\n",
    "        }\n",
    "\n",
    "for i in gen_data(content):\n",
    "    write_to_file(i, '/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/data.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(path, name):\n",
    "    \"\"\"\n",
    "    Load date from file\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file) as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        item = json.loads(line)\n",
    "        data.append(item[name])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/data.json', 'content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import regexp_tokenize\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(s):\n",
    "    \"\"\"\n",
    "    Word segmentation\n",
    "    \"\"\"\n",
    "    pattern = r'''\n",
    "              (?x)                   # set flag to allow verbose regexps \n",
    "              (?:[A-Z]\\.)+           # abbreviations, e.g. U.S.A. \n",
    "              |\\w+(?:[-&']\\w+)*      # words w/ optional internal hyphens/apostrophe  \n",
    "           '''  \n",
    "    return regexp_tokenize(s, pattern=pattern)\n",
    "\n",
    "def clean(s):\n",
    "    \"\"\"\n",
    "    Clean data\n",
    "    \"\"\"\n",
    "    for i in range(len(s)):\n",
    "        if s[i].isdigit():\n",
    "            s[i] = '0'\n",
    "        if s[i] == 'p' and i < len(s)-1:\n",
    "            if s[i+1] == 'm':\n",
    "                s[i] = 'pm'\n",
    "                s[i+1] = ''\n",
    "        if s[i] == 'a' and i < len(s)-1:\n",
    "            if s[i+1] == 'm':\n",
    "                s[i] = 'am'\n",
    "                s[i+1] = ''\n",
    "        if s[i] == 's':\n",
    "            s[i] = ''\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 265512/265512 [01:43<00:00, 2574.16it/s]\n"
     ]
    }
   ],
   "source": [
    "contWords = [clean(cut(c)) for c in tqdm(content)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"readed's\"]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut(\"readed's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the return of the nike air max sensation has 80 s babies hyped home style the return of the nike air max sensation has 80 s babies hyped posted on sep 22 2015 if you were a basketball fan who was born in the 80 s you were lucky enough to witness the beauty that is 90 s basketball it was truly a great time to be basketball fan if you played close attention to what the players were wearing on their feet you would have also noticed the wide array of footwear these player used to rock one of those happens to the the nike air max sensation which is also set to receive the retro treatment this year originally released back in read more author kicksonfire share this post on googlefacebooktwitter'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(contWords[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk import WordNetLemmatizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 265512/265512 [10:03<00:00, 439.74it/s]          \n"
     ]
    }
   ],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "contWords = [[wnl.lemmatize(t.lower()) for t in toks] for toks in tqdm(contWords)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 265512/265512 [01:56<00:00, 2281.78it/s] \n"
     ]
    }
   ],
   "source": [
    "contWords = [[t.lower() for t in toks] for toks in tqdm(contWords)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok_sentWords = contWords.copy()\n",
    "tokTexts = [' '.join(i) for i in tok_sentWords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: \t265512\n",
      "Distribution of sentence lengths (number of words):\n",
      "Min: 7   Max: 17413   Mean: 396.790   Med: 233.000\n",
      "Found 1060614 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(filters='')\n",
    "tokenizer.fit_on_texts(tokTexts)\n",
    "word2index = tokenizer.word_index\n",
    "index2word = {i: w for w, i in word2index.items()}\n",
    "sentLens = np.array([len(i) for i in tok_sentWords])\n",
    "print('Number of sentences: \\t{:d}'.format(len(sentLens)))\n",
    "print('Distribution of sentence lengths (number of words):')\n",
    "print('Min: {:d}   Max: {:d}   Mean: {:.3f}   Med: {:.3f}'.format(np.min(sentLens), np.max(sentLens), np.mean(sentLens), np.median(sentLens)))\n",
    "print('Found %s unique tokens.' % len(word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: \t265512\n",
      "Distribution of sentence lengths (number of words):\n",
      "Min: 7   Max: 17413   Mean: 396.790   Med: 233.000\n",
      "Found 1039702 unique tokens.\n",
      "Found 1039702 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(filters='')\n",
    "tokenizer.fit_on_texts(tokTexts)\n",
    "word2index = tokenizer.word_index\n",
    "index2word = {i: w for w, i in word2index.items()}\n",
    "sentLens = np.array([len(i) for i in tok_sentWords])\n",
    "print('Number of sentences: \\t{:d}'.format(len(sentLens)))\n",
    "print('Distribution of sentence lengths (number of words):')\n",
    "print('Min: {:d}   Max: {:d}   Mean: {:.3f}   Med: {:.3f}'.format(np.min(sentLens), np.max(sentLens), np.mean(sentLens), np.median(sentLens)))\n",
    "print('Found %s unique tokens.' % len(word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"boy's \"\n",
    "s.endswith(\"'s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(w):\n",
    "    \"\"\"\n",
    "    Clean data\n",
    "    \"\"\"\n",
    "    if w.endswith(\"'s\")\n",
    "    \n",
    "    for i in range(len(s)):\n",
    "        if s[i] == ' ':\n",
    "            s[i] = None\n",
    "        if s[i] == '(':\n",
    "            for j in range(i+1, len(s)):\n",
    "                if s[j] == ')':\n",
    "                    for k in range(i, j+1):\n",
    "                        s[k] = None\n",
    "        if s[i] == '（':\n",
    "            for j in range(i+1, len(s)):\n",
    "                if s[j] == '）':\n",
    "                    for k in range(i, j+1):\n",
    "                        s[k] = None\n",
    "        if s[i] == '《':\n",
    "            for j in range(i+1, len(s)):\n",
    "                if s[j] == '》':\n",
    "                    for k in range(i, j+1):\n",
    "                        s[k] = None\n",
    "        if s[i] == '%':\n",
    "            if s[i-1] != None:\n",
    "                s[i-1] = s[i-1]+'%'\n",
    "            s[i] = None   \n",
    "        if s[i] == '“' or s[i] == '”' or s[i] == '\"':\n",
    "            s[i] = None\n",
    "        if s[i] == '於':\n",
    "            s[i] = '于'\n",
    "        if s[i] == '後':\n",
    "            s[i] == '后'\n",
    "    return [i for i in s if i != None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Save Dictionarie**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/index.pkl', 'wb') as fp:\n",
    "    pickle.dump((word2index, index2word), fp, -1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
