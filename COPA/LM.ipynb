{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Dataset-Preprocessing\" data-toc-modified-id=\"Dataset-Preprocessing-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Dataset Preprocessing</a></div><div class=\"lev2 toc-item\"><a href=\"#Save-Data\" data-toc-modified-id=\"Save-Data-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Save Data</a></div><div class=\"lev2 toc-item\"><a href=\"#Load-Data\" data-toc-modified-id=\"Load-Data-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Load Data</a></div><div class=\"lev1 toc-item\"><a href=\"#Word-Segmentation\" data-toc-modified-id=\"Word-Segmentation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Word Segmentation</a></div><div class=\"lev1 toc-item\"><a href=\"#Tokenize-Text\" data-toc-modified-id=\"Tokenize-Text-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Tokenize Text</a></div><div class=\"lev1 toc-item\"><a href=\"#Create-Word-Embeddings-with-GloVe\" data-toc-modified-id=\"Create-Word-Embeddings-with-GloVe-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Create Word Embeddings with GloVe</a></div><div class=\"lev2 toc-item\"><a href=\"#Read-GloVe\" data-toc-modified-id=\"Read-GloVe-41\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Read GloVe</a></div><div class=\"lev2 toc-item\"><a href=\"#Use-Glove-to-Initialize-Embedding-Matrix\" data-toc-modified-id=\"Use-Glove-to-Initialize-Embedding-Matrix-42\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Use Glove to Initialize Embedding Matrix</a></div><div class=\"lev2 toc-item\"><a href=\"#Map-Words\" data-toc-modified-id=\"Map-Words-43\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Map Words</a></div><div class=\"lev1 toc-item\"><a href=\"#Build-Dataset\" data-toc-modified-id=\"Build-Dataset-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Build Dataset</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('/Users/lizhn7/Downloads/DATA/news/sample-1M.jsonl') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "content = []\n",
    "for line in lines:\n",
    "    item = json.loads(line)\n",
    "    if item['media-type'] == 'Blog':\n",
    "        content.append(item['content'])\n",
    "\n",
    "def write_to_file(content, name):\n",
    "    with open(name, 'a', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(content, ensure_ascii=False) + '\\n')\n",
    "        f.close()\n",
    "        \n",
    "def gen_data(data):\n",
    "    for i in range(len(data)):\n",
    "        yield {\n",
    "            'content': data[i]\n",
    "        }\n",
    "\n",
    "for i in gen_data(content):\n",
    "    write_to_file(i, '/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/data.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(path, name):\n",
    "    \"\"\"\n",
    "    Load date from file\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file) as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        item = json.loads(line)\n",
    "        data.append(item[name])\n",
    "    return data\n",
    "\n",
    "def add_token(s):\n",
    "    \"\"\"\n",
    "    Add end token\n",
    "    \"\"\"\n",
    "    s = s.split()\n",
    "    n = []\n",
    "    for i in s:\n",
    "        if i.endswith('.') or i.endswith('!') or i.endswith('?'):\n",
    "            i += 'endtok'\n",
    "        n.append(i)\n",
    "    return ' '.join(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 265512/265512 [01:02<00:00, 4249.29it/s]\n"
     ]
    }
   ],
   "source": [
    "content = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/data.json', 'content')\n",
    "content = [add_token(c) for c in tqdm(content)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(s):\n",
    "    \"\"\"\n",
    "    Word segmentation\n",
    "    \"\"\"\n",
    "    pattern = r'''\n",
    "              (?x)                   # set flag to allow verbose regexps \n",
    "              (?:[A-Z]\\.)+           # abbreviations, e.g. U.S.A. \n",
    "              |\\w+(?:[-&']\\w+)*      # words w/ optional internal hyphens/apostrophe  \n",
    "            '''  \n",
    "    return regexp_tokenize(s, pattern=pattern)\n",
    "\n",
    "def clean(s):\n",
    "    \"\"\"\n",
    "    Clean data\n",
    "    \"\"\"\n",
    "    for i in range(len(s)):\n",
    "        for d in ['0', '1', '2', '3', '4', '5' ,'6', '7', '8', '9']:\n",
    "            if d in s[i]:\n",
    "                s[i] = '0'\n",
    "        if s[i] == 'p' and i < len(s)-1:\n",
    "            if s[i+1] == 'm':\n",
    "                s[i] = 'pm'\n",
    "                s[i+1] = ''\n",
    "        if s[i] == 'a' and i < len(s)-1:\n",
    "            if s[i+1] == 'm':\n",
    "                s[i] = 'am'\n",
    "                s[i+1] = ''\n",
    "        if s[i] == 's':\n",
    "            s[i] = ''\n",
    "        if s[i].endswith(\"'s\"):\n",
    "            s[i] = s[i][:-2]\n",
    "    return [i for i in s if i != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 265512/265512 [03:23<00:00, 1307.07it/s]\n"
     ]
    }
   ],
   "source": [
    "contWords = [clean(cut(c)) for c in tqdm(content)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: \t265512\n",
      "Distribution of sentence lengths (number of words):\n",
      "Min: 0   Max: 12998   Mean: 403.903   Med: 234.000\n",
      "Found 855058 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tok_sentWords = contWords.copy()\n",
    "tokTexts = [' '.join(i) for i in tok_sentWords]\n",
    "tokenizer = Tokenizer(num_words=None,\n",
    "                      filters='',\n",
    "                      lower=True)\n",
    "tokenizer.fit_on_texts(tokTexts)\n",
    "word2index = tokenizer.word_index\n",
    "index2word = {i: w for w, i in word2index.items()}\n",
    "sentLens = np.array([len(i) for i in tok_sentWords])\n",
    "print('Number of sentences: \\t{:d}'.format(len(sentLens)))\n",
    "print('Distribution of sentence lengths (number of words):')\n",
    "print('Min: {:d}   Max: {:d}   Mean: {:.3f}   Med: {:.3f}'.format(np.min(sentLens), np.max(sentLens), np.mean(sentLens), np.median(sentLens)))\n",
    "print('Found %s unique tokens.' % len(word2index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Word Embeddings with GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEED=42\n",
    "VOCAB_SIZE = 20000\n",
    "EMBEDDING_DIM = 200\n",
    "NUM_UNK_WORDS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1193513"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_n_symbols = !wc -l /Users/lizhn7/Downloads/DATA/glove/glove.twitter.27B.200d.txt\n",
    "glove_n_symbols = int(glove_n_symbols[0].split()[0])\n",
    "glove_n_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_index_dict = {}\n",
    "glove_embedding_weights = np.empty((glove_n_symbols, EMBEDDING_DIM))\n",
    "globale_scale = 0.1\n",
    "with open('/Users/lizhn7/Downloads/DATA/glove/glove.twitter.27B.200d.txt', 'r') as fp:\n",
    "    index = 0\n",
    "    for l in fp:\n",
    "        l = l.strip().split()\n",
    "        word = l[0]\n",
    "        glove_index_dict[word] = index\n",
    "        glove_embedding_weights[index, :] = [float(n) for n in l[1:]]\n",
    "        index += 1\n",
    "glove_embedding_weights *= globale_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Glove to Initialize Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer, LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate random embedding with same scale as glove\n",
    "np.random.seed(SEED)\n",
    "shape = (VOCAB_SIZE, EMBEDDING_DIM)\n",
    "scale = glove_embedding_weights.std() * np.sqrt(12) / 2 \n",
    "embedding = np.random.uniform(low=-scale, high=scale, size=shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19817-99.09% tokens in vocab found in glove and copied to embedding.\n"
     ]
    }
   ],
   "source": [
    "# Copy from glove weights of words that appear in index2word\n",
    "count = 0 \n",
    "for i in range(1, VOCAB_SIZE):\n",
    "    w = index2word[i]\n",
    "    g = glove_index_dict.get(w)\n",
    "    if g is None:\n",
    "        ww = wnl.lemmatize(w)\n",
    "        g = glove_index_dict.get(ww)\n",
    "    if g is None:\n",
    "        ww = porter.stem(w)\n",
    "        g = glove_index_dict.get(ww)\n",
    "    if g is None:\n",
    "        ww = lancaster.stem(w)\n",
    "        g = glove_index_dict.get(ww)\n",
    "    if g is not None:\n",
    "        embedding[i, :] = glove_embedding_weights[g, :]\n",
    "        count += 1\n",
    "print('{num_tokens}-{per:.2f}% tokens in vocab found in glove and copied to embedding.'.format(num_tokens=count, per=count/float(VOCAB_SIZE)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lots of word in the full vocabulary are outside `vocab_size`. Build an alterantive which will map them to their closest match in glove but only if the match is good enough (cos distance score above `glove_thr`)\n",
    "- For every word outside the embedding matrix find the closest word inside the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set cos distance threshold\n",
    "glove_thr = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 855058/855058 [00:36<00:00, 23253.63it/s]\n"
     ]
    }
   ],
   "source": [
    "word2glove = {}\n",
    "for w in tqdm(word2index):\n",
    "    if w in glove_index_dict:\n",
    "        g = w\n",
    "    elif wnl.lemmatize(w) in glove_index_dict:\n",
    "        g = wnl.lemmatize(w)\n",
    "    elif porter.stem(w) in glove_index_dict:\n",
    "        g = porter.stem(w)\n",
    "    elif lancaster.stem(w) in glove_index_dict:\n",
    "        g = lancaster.stem(w)\n",
    "    elif w.endswith('s') and w[:-1] in glove_index_dict:\n",
    "        g = w[:-1]\n",
    "    elif w.endswith('ed') and w[:-2] in glove_index_dict:\n",
    "        g = w[:-2]\n",
    "    else:\n",
    "        continue\n",
    "    word2glove[w] = g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 855058/855058 [06:10<00:00, 2305.82it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53551 of glove substitutes found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "normed_embedding = embedding/np.array([np.sqrt(np.dot(gweight, gweight)) \n",
    "                                       for gweight in embedding])[:, None] #1D -> 2D\n",
    "\n",
    "glove_match = []\n",
    "for w, i in tqdm(word2index.items()):\n",
    "    if i >= VOCAB_SIZE and w.isalpha() and w in word2glove:\n",
    "        gi = glove_index_dict[word2glove[w]]\n",
    "        gweight = glove_embedding_weights[gi, :].copy()\n",
    "        # find row in embedding that has the highest cos score with glove_weight\n",
    "        gweight /= np.sqrt(np.dot(gweight, gweight))\n",
    "        score = np.dot(normed_embedding, gweight) \n",
    "        while True:\n",
    "            embedding_index = score.argmax()\n",
    "            s = score[embedding_index]\n",
    "            if s < glove_thr:\n",
    "                break\n",
    "            if index2word[embedding_index] in word2glove:\n",
    "                glove_match.append((w, embedding_index, s))\n",
    "                break\n",
    "            score[embedding_index] = -1       \n",
    "glove_match.sort(key=lambda x: -x[2])\n",
    "print('{n} of glove substitutes found'.format(n=len(glove_match)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Manually check that the worst substitutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.500040976809 oks => ok\n",
      "0.500040976809 oksanen => ok\n",
      "0.500040976809 oksal => ok\n",
      "0.500040201514 lissette => abigail\n",
      "0.500038572367 enigmatic => endearing\n",
      "0.500026638723 duhon => randolph\n",
      "0.500017312467 aslong => regardless\n",
      "0.500010669787 nohhis => dang\n",
      "0.500009611564 drugging => assaulting\n",
      "0.500009611564 druggings => assaulting\n"
     ]
    }
   ],
   "source": [
    "for origin, sub, score in glove_match[-10:]:\n",
    "    print(score, origin, '=>', index2word[sub])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Build a lookup table of index of outside words to index of inside words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_index2index = {word2index[w]: embedding_index \n",
    "                     for w, embedding_index, _ in glove_match}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_GRAMS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(unknown_words):\n",
    "    idx2word[VOCAB_SIZE-1-i] = '<%d>'%i\n",
    "\n",
    "unk0 = VOCAB_SIZE - NUM_UNK_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vocab_fold(xs):\n",
    "    \"\"\"\n",
    "    Convert list of word indexes that may contain words outside vocab_size to words inside.\n",
    "    If a word is outside, try first to use glove_idx2idx to find a similar word inside.\n",
    "    If none exist then replace all accurancies of the same unknown word with <0>, <1>, ...\n",
    "    \"\"\"\n",
    "    xs = [x if x < unk0 else glove_index2index.get(x, x) for x in xs]\n",
    "    # the more popular word is <0> and so on\n",
    "    outside = sorted([x for x in xs if x >= unk0])\n",
    "    # if there are more than unknown_words unk words then put them all in unknown_words-1\n",
    "    outside = {x: VOCAB_SIZE-1-min(i, NUM_UNK_WORDS-1) for i, x in enumerate(outside)}\n",
    "    xs = [outside.get(x, x) for x in xs]\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 265512/265512 [00:39<00:00, 6729.47it/s]\n"
     ]
    }
   ],
   "source": [
    "contSeq = [vocab_fold(s) for s in tqdm(tokenizer.texts_to_sequences(tokTexts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 197852/265512 [28:34<01:05, 1026.22it/s]  "
     ]
    }
   ],
   "source": [
    "x = []\n",
    "y = []\n",
    "for c in tqdm(contSeq):\n",
    "        for i in range(len(c)):\n",
    "            x.append([0]*(NUM_GRAMS-i + max(0, i-NUM_GRAMS)) + c[max(0, i-NUM_GRAMS):i])\n",
    "            y.append([c[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array(x)\n",
    "y = np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "815780"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx2word)- VOCAB_SIZE -len(glove_index2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Save Dictionarie**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/index.pkl', 'wb') as fp:\n",
    "    pickle.dump((word2index, index2word), fp, -1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
