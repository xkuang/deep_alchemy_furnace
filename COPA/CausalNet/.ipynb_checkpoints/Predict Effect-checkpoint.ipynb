{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Checkpoint\" data-toc-modified-id=\"Checkpoint-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Checkpoint</a></div><div class=\"lev1 toc-item\"><a href=\"#Import-Libraries\" data-toc-modified-id=\"Import-Libraries-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Import Libraries</a></div><div class=\"lev1 toc-item\"><a href=\"#Set-Hyperparameters\" data-toc-modified-id=\"Set-Hyperparameters-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Set Hyperparameters</a></div><div class=\"lev1 toc-item\"><a href=\"#Build-Graph\" data-toc-modified-id=\"Build-Graph-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Build Graph</a></div><div class=\"lev1 toc-item\"><a href=\"#Model-Visualization\" data-toc-modified-id=\"Model-Visualization-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Model Visualization</a></div><div class=\"lev1 toc-item\"><a href=\"#Train\" data-toc-modified-id=\"Train-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Train</a></div><div class=\"lev1 toc-item\"><a href=\"#Predict\" data-toc-modified-id=\"Predict-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Predict</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/Users/lizhn7/Downloads/EXPERIMENT/COPA/CausalNet/index.pkl', 'rb') as fp:\n",
    "    word2index, index2word = pickle.load(fp)\n",
    "    \n",
    "with h5py.File('/Users/lizhn7/Downloads/EXPERIMENT/COPA/CausalNet/embedding.h5', 'r') as fh:\n",
    "    embedding = fh['embedding'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File('/Users/lizhn7/Downloads/EXPERIMENT/COPA/CausalNet/Predict Effect/train.h5', 'r') as fh:\n",
    "    xTrain = fh['xTrain'][:]\n",
    "    xVal = fh['xVal'][:]\n",
    "    yTrain = fh['yTrain'][:]\n",
    "    yVal = fh['yVal'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((102875905, 1), (18154572, 1), (102875905, 1), (18154572, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTrain.shape, xVal.shape, yTrain.shape, yVal.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Embedding, Dense, Dropout, Flatten, BatchNormalization, Activation\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.callbacks import*\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1845\n",
    "EMBEDDING_DIM = 300\n",
    "BATCH_SIZE = 1024\n",
    "STEPS_PER_EPOCH = 1024\n",
    "VALIDATION_STEPS = 160\n",
    "NUM_EPOCHS = 256\n",
    "HIDDEN_SIZE = 600\n",
    "DROPOUT_RATE = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build():\n",
    "    \"\"\"\n",
    "    Build model\n",
    "    \"\"\"\n",
    "    K.clear_session()\n",
    "    seq = Input(shape=(1,), name='INPUT')\n",
    "    emb = Embedding(VOCAB_SIZE,\n",
    "                    EMBEDDING_DIM,\n",
    "                    weights=[embedding],\n",
    "                    mask_zero=False,\n",
    "                    input_length=1,\n",
    "                    trainable=True,\n",
    "                    name='EMBEDDING')(seq)\n",
    "    emb = Dropout(DROPOUT_RATE, name='DROPOUT_1')(emb)\n",
    "    emb = Flatten(name='FLATTEN')(emb)\n",
    "    dense = Dense(HIDDEN_SIZE, activation=None, name='DENSE')(emb)\n",
    "    bn = BatchNormalization(name='BN')(dense)\n",
    "    act = Activation('relu', name='RELU')(bn)\n",
    "    dp = Dropout(DROPOUT_RATE, name='DROPOUT_2')(act)\n",
    "    out = Dense(VOCAB_SIZE, activation='softmax', name='OUTPUT')(dp)\n",
    "    model = Model(inputs=seq, outputs=out)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "INPUT (InputLayer)           (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "EMBEDDING (Embedding)        (None, 1, 300)            553500    \n",
      "_________________________________________________________________\n",
      "DROPOUT_1 (Dropout)          (None, 1, 300)            0         \n",
      "_________________________________________________________________\n",
      "FLATTEN (Flatten)            (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "DENSE (Dense)                (None, 600)               180600    \n",
      "_________________________________________________________________\n",
      "BN (BatchNormalization)      (None, 600)               2400      \n",
      "_________________________________________________________________\n",
      "RELU (Activation)            (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "DROPOUT_2 (Dropout)          (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "OUTPUT (Dense)               (None, 1845)              1108845   \n",
      "=================================================================\n",
      "Total params: 1,845,345\n",
      "Trainable params: 1,844,145\n",
      "Non-trainable params: 1,200\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"629pt\" viewBox=\"0.00 0.00 179.93 629.00\" width=\"180pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 625)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-625 175.9277,-625 175.9277,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 113968531720 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>113968531720</title>\n",
       "<polygon fill=\"none\" points=\"22.9106,-584.5 22.9106,-620.5 149.0171,-620.5 149.0171,-584.5 22.9106,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.9639\" y=\"-598.3\">INPUT: InputLayer</text>\n",
       "</g>\n",
       "<!-- 113969185904 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>113969185904</title>\n",
       "<polygon fill=\"none\" points=\"0,-511.5 0,-547.5 171.9277,-547.5 171.9277,-511.5 0,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.9639\" y=\"-525.3\">EMBEDDING: Embedding</text>\n",
       "</g>\n",
       "<!-- 113968531720&#45;&gt;113969185904 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>113968531720-&gt;113969185904</title>\n",
       "<path d=\"M85.9639,-584.4551C85.9639,-576.3828 85.9639,-566.6764 85.9639,-557.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"89.464,-557.5903 85.9639,-547.5904 82.464,-557.5904 89.464,-557.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 113969186240 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>113969186240</title>\n",
       "<polygon fill=\"none\" points=\"10.8794,-438.5 10.8794,-474.5 161.0483,-474.5 161.0483,-438.5 10.8794,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.9639\" y=\"-452.3\">DROPOUT_1: Dropout</text>\n",
       "</g>\n",
       "<!-- 113969185904&#45;&gt;113969186240 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>113969185904-&gt;113969186240</title>\n",
       "<path d=\"M85.9639,-511.4551C85.9639,-503.3828 85.9639,-493.6764 85.9639,-484.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"89.464,-484.5903 85.9639,-474.5904 82.464,-484.5904 89.464,-484.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 113969186408 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>113969186408</title>\n",
       "<polygon fill=\"none\" points=\"24.4966,-365.5 24.4966,-401.5 147.4312,-401.5 147.4312,-365.5 24.4966,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.9639\" y=\"-379.3\">FLATTEN: Flatten</text>\n",
       "</g>\n",
       "<!-- 113969186240&#45;&gt;113969186408 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>113969186240-&gt;113969186408</title>\n",
       "<path d=\"M85.9639,-438.4551C85.9639,-430.3828 85.9639,-420.6764 85.9639,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"89.464,-411.5903 85.9639,-401.5904 82.464,-411.5904 89.464,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 113969186576 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>113969186576</title>\n",
       "<polygon fill=\"none\" points=\"34.2207,-292.5 34.2207,-328.5 137.707,-328.5 137.707,-292.5 34.2207,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.9639\" y=\"-306.3\">DENSE: Dense</text>\n",
       "</g>\n",
       "<!-- 113969186408&#45;&gt;113969186576 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>113969186408-&gt;113969186576</title>\n",
       "<path d=\"M85.9639,-365.4551C85.9639,-357.3828 85.9639,-347.6764 85.9639,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"89.464,-338.5903 85.9639,-328.5904 82.464,-338.5904 89.464,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 113974130896 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>113974130896</title>\n",
       "<polygon fill=\"none\" points=\"7.7861,-219.5 7.7861,-255.5 164.1416,-255.5 164.1416,-219.5 7.7861,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.9639\" y=\"-233.3\">BN: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 113969186576&#45;&gt;113974130896 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>113969186576-&gt;113974130896</title>\n",
       "<path d=\"M85.9639,-292.4551C85.9639,-284.3828 85.9639,-274.6764 85.9639,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"89.464,-265.5903 85.9639,-255.5904 82.464,-265.5904 89.464,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 113974195760 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>113974195760</title>\n",
       "<polygon fill=\"none\" points=\"26.8276,-146.5 26.8276,-182.5 145.1001,-182.5 145.1001,-146.5 26.8276,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.9639\" y=\"-160.3\">RELU: Activation</text>\n",
       "</g>\n",
       "<!-- 113974130896&#45;&gt;113974195760 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>113974130896-&gt;113974195760</title>\n",
       "<path d=\"M85.9639,-219.4551C85.9639,-211.3828 85.9639,-201.6764 85.9639,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"89.464,-192.5903 85.9639,-182.5904 82.464,-192.5904 89.464,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 113974281832 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>113974281832</title>\n",
       "<polygon fill=\"none\" points=\"10.8794,-73.5 10.8794,-109.5 161.0483,-109.5 161.0483,-73.5 10.8794,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.9639\" y=\"-87.3\">DROPOUT_2: Dropout</text>\n",
       "</g>\n",
       "<!-- 113974195760&#45;&gt;113974281832 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>113974195760-&gt;113974281832</title>\n",
       "<path d=\"M85.9639,-146.4551C85.9639,-138.3828 85.9639,-128.6764 85.9639,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"89.464,-119.5903 85.9639,-109.5904 82.464,-119.5904 89.464,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 113974280544 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>113974280544</title>\n",
       "<polygon fill=\"none\" points=\"29.5142,-.5 29.5142,-36.5 142.4136,-36.5 142.4136,-.5 29.5142,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.9639\" y=\"-14.3\">OUTPUT: Dense</text>\n",
       "</g>\n",
       "<!-- 113974281832&#45;&gt;113974280544 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>113974281832-&gt;113974280544</title>\n",
       "<path d=\"M85.9639,-73.4551C85.9639,-65.3828 85.9639,-55.6764 85.9639,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"89.464,-46.5903 85.9639,-36.5904 82.464,-46.5904 89.464,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build()\n",
    "model.summary()\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = '/Users/lizhn7/Downloads/EXPERIMENT/COPA/CausalNet/Predict Effect/cp_logs/weights.{epoch:03d}-{loss:.6f}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "log_string = '/Users/lizhn7/Downloads/EXPERIMENT/COPA/CausalNet/Predict Effect/tb_logs/1'\n",
    "tensorboard = TensorBoard(log_dir=log_string)\n",
    "callbacks_list = [checkpoint, tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generator(data, label, batch_size): \n",
    "    \"\"\"\n",
    "    Yield batches \n",
    "    \"\"\"\n",
    "    index = np.arange(len(data))\n",
    "    np.random.shuffle(index)    \n",
    "    batches = [index[range(batch_size*i, min(len(data), batch_size*(i+1)))] for i in range(len(data)//batch_size)]\n",
    "    while True:\n",
    "        for i in batches:\n",
    "            x = data[i]\n",
    "            y = label[i]\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_train = data_generator(xTrain, yTrain, BATCH_SIZE)\n",
    "gen_val = data_generator(xVal, yVal, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = build()\n",
    "history = model.fit_generator(gen_train,\n",
    "                              steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                              epochs=NUM_EPOCHS,\n",
    "                              validation_data=gen_val,\n",
    "                              callbacks=callbacks_list,\n",
    "                              validation_steps=VALIDATION_STEPS,\n",
    "                              workers=4,\n",
    "                              use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from nltk import regexp_tokenize\n",
    "from nltk import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopWord = '/Users/lizhn7/Documents/Github/深度炼丹炉/COPA/CausalNet/stopwords.txt'\n",
    "stopWord = open(stopWord).read().split()\n",
    "lemmWord = '/Users/lizhn7/Documents/Github/深度炼丹炉/COPA/CausalNet/WordNetLemmatizer.txt'\n",
    "lemmWord = open(lemmWord).read().split()\n",
    "stemWord = '/Users/lizhn7/Documents/Github/深度炼丹炉/COPA/CausalNet/PorterStemmer.txt'\n",
    "stemWord = open(stemWord).read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replDict = {\"woman's\": 'woman', \"man's\": 'man', \"patient's\": 'patient', \"student's\": 'student', \"boy's\": 'boy', \n",
    "            \"friend's\": 'friend', \"enemy's\": 'enemy', \"parent's\": 'parent', \"humanitarian's\": 'humanitarian', \n",
    "            \"child's\": 'child', \"professor's\": 'professor', \"daughter's\": 'daughter', \"mother's\": 'mother', \n",
    "            \"children's\": 'children', \"teller's\": 'teller', \"company's\": 'company', \"group's\": 'group', \n",
    "            \"laptop's\": 'laptop', \"girl's\": 'girl', \"salesman's\": 'salesman', \"cook's\": 'cook', \"car's\": 'car', \n",
    "            \"offender's\": 'offender', \"detective's\": 'detective', \"librarian's\": 'librarian', \"caller's\": 'caller', \n",
    "            \"victim's\": 'victim', \"interviewer's\": 'interviewer', \"ship's\": 'ship', \"site's\": 'site', \n",
    "            \"chandelier's\": 'chandelier', \"bully's\": 'bully', \"river's\": 'river', \"puppy's\": 'puppy', \n",
    "            \"pilot's\": 'pilot', \"girlfriend's\": 'girlfriend', \"politician's\": 'politician', \"couple's\": 'couple', \n",
    "            \"son's\": 'son', \"actor's\": 'actor', \"neighbor's\": 'neighbor', \"nation's\": 'nation', \n",
    "            \"classmate's\": 'classmate', \"businessman's\": 'businessman', \"architect's\": 'architect', \n",
    "            \"imposter's\": 'imposter', \"kidnapper's\": 'kidnapper', \"colleague's\": 'colleague', \"flower's\": 'flower',\n",
    "            \"bull's\": 'bull', \"employee's\": 'employee', \"team's\": 'team', \"other's\": 'other', \n",
    "            \"writer's\": 'writer', \"baby's\": 'baby', \"attacker's\": 'attacker', \"uncle's\": 'uncle', \"driver's\": 'driver',\n",
    "            \"chuckling\": 'chuckle', \"drank\": 'drink', 'relied': 'rely', 'wore': 'wear', \"grew\": 'grow', \"slid\": 'slide',\n",
    "            \"worried\": 'worry', \"clumsily\": 'clumsy', \"heavily\": 'heavy', \"applied\": 'apply', \"rang\": 'ring', \"forgot\": 'forget',\n",
    "            \"shook\": 'shake', 'cried': 'cry', \"defied\": 'defy', \"incriminating\": 'incriminate', \"bitten\": 'bite', \n",
    "            \"blew\": 'blow', \"carried\": 'carry', \"told\": 'tell', \"sweaty\": 'sweat', \"buried\": 'bury', \"threw\": 'throw',\n",
    "            \"bought\": 'buy', \"woke\": 'wake', \"testified\": 'testify', \"froze\": 'freeze', \"outgrew\": 'outgrow', \"caught\": 'catch',\n",
    "            \"stood\": 'stand', \"preparing\": 'prepare', \"met\": 'meet', \"fought\": 'fight', \"faux\": 'fake',\n",
    "            \"spun\": 'spin', \"wrote\": 'write', \"easily\": 'easy', \"sped\": 'speed', \"leapt\": 'leap', \"taller\": 'tall',\n",
    "            \"underwent\": 'undergo', \"bled\": 'bleed', \"taught\": 'teach', \"spoke\": 'speak', \"stronger\": 'strong',\n",
    "            \"hung\": 'hang', \"brought\": 'bring', \"shrunk\": 'shrink', \"withheld\": 'withhold', \"re-elected\": 'reelect',\n",
    "            \"mimicked\": 'mimic', \"flew\": 'fly', \"interminably\": 'interminable', \"stolen\": 'steal', \"flung\": 'fling',\n",
    "            \"swung\": 'swing', \"awoke\": 'awake', \"aloud\": 'loud', \"receiving\": 'receive', \"withdrew\": 'withdraw', \n",
    "            \"forbade\": 'forbid', \"lagging\": 'lag', \"shaking\" : 'shake', \"lying\": 'lie', \"making\": 'make', 'diving': 'dive',\n",
    "            \"travelling\": 'travel', \"coming\": 'come', \"giving\": 'give', \"moving\": 'move', \"sobbing\": 'sob',\n",
    "            \"saving\": 'save', \"sitting\": 'sit', \"hiking\": 'hike', \"running\": 'run', \"convincing\": 'convince', \"getting\": 'get',\n",
    "            \"rising\": 'rise', 'pockets': 'pocket'}\n",
    "            \n",
    "\n",
    "ING = ['recovering', 'carrying', 'littering', 'yawning', 'restraining', 'returning', 'ringing', \n",
    "       'whispering', 'ticking', 'attending', 'laughing', 'reading', 'overflowing', 'helping',\n",
    "       'playing', 'bleeding', 'talking', 'telling', 'acting', 'collecting', 'greeting', 'going', 'entering',\n",
    "       'belonging', 'washing', 'looking', 'wailing', 'steering', 'screaming', 'trying', 'watching', 'sleeping']\n",
    "\n",
    "D = ['confused', 'believed', 'recognized', 'inflated', 'undercharged', 'wrinkled', 'prepared', 'smudged', 'desired', \n",
    "     'promised', 'wrestled', 'trampled', 'tickled', 'disorganized', 'anticipated', 'rescued', 'intimidated', 'served',\n",
    "     'repulsed', 'raised', 'subsided', 'disliked', 'violated', 'inhaled', 'vaccinated', 'intoxicated', 'abused', \n",
    "     'misplaced', 'contemplated', 'urged', 'exhaled', 'escaped', 'received', 'dehydrated', 'persuaded', 'irritated', \n",
    "     'emerged', 'provoked', 'whistled', 'disabled', 'dangled', 'collided', 'sabotaged', 'animated', 'lounged', \n",
    "     'announced', 'upgraded', 'sensed', 'reminisced', 'endured', 'mishandled', 'retired', 'graduated', 'scheduled', \n",
    "     'noticed', 'buckled', 'bruised', 'reclined', 'manufactured', 'advertised', 'removed', 'migrated', 'aspired', \n",
    "     'sneezed', 'fumbled', 'intrigued', 'eradicated', 'disagreed', 'diagnosed', 'rotated', 'swerved', 'engaged', \n",
    "     'increased', 'injured', 'expired', 'refused', 'overpriced', 'separated', 'interrogated', 'produced', 'fined', \n",
    "     'browsed', 'retraced', 'fatigued', 'eliminated', 'sued', 'joked', 'required', 'involved', 'released', 'ached', \n",
    "     'closed', 'paddled', 'renovated', 'confiscated', 'relieved', 'grieved', 'manipulated', 'giggled', 'stared', \n",
    "     'grimaced', 'commenced', 'moved', 'deleted', 'invited', 'relocated', 'evaporated', 'calculated', 'replaced', \n",
    "     'enraged', 'perceived', 'decorated', 'tackled', 'obligated', 'postponed', 'faded', 'congratulated', 'amputated', \n",
    "     'locked', 'blended', 'craved', 'ceased', 'glanced', 'tattled', 'excused', 'crumbled', 'crinkled', 'fabricated', \n",
    "     'crossed', 'jingled', 'declined', 'exploded', 'retrieved', 'tolerated', 'glided', 'owed', 'choked', 'rinsed', \n",
    "     'validated', 'pierced', 'excavated', 'skated', 'snored', 'finalized', 'tuned', 'tensed', 'deactivated', \n",
    "     'evacuated', 'accelerated', 'excommunicated', 'squeezed', 'casted', 'filed', 'donated', 'chimed', 'shined', \n",
    "     'organized', 'promoted', 'capsized', 'chuckled', 'browned', 'exaggerated', 'collapsed', 'glued', 'struggled', \n",
    "     'encouraged', 'smiled', 'located', 'baked', 'waited', 'disclosed', 'sliced', 'apologized', 'packaged']\n",
    "\n",
    "ED = ['recovered', 'discovered', 'misspelled', 'outnumbered', 'blocked', 'hailed', 'leaped', 'vetoed', 'ironed', \n",
    "      'delayed', 'leaked', 'fined', 'booed', 'constructed', 'hardened', 'altered', 'voided', 'smeared', 'worsened', \n",
    "      'rested', 'delivered', 'blackmailed', 'handcuffed', 'splashed', 'flowed', 'triggered', 'growled', 'evicted', \n",
    "      'discarded', 'attended', 'stared', 'cooled', 'drifted', 'fainted', 'collected', 'confronted', 'reminded', \n",
    "      'subtracted', 'welled', 'watered', 'banished', 'encountered', 'cheered', 'dressed', 'unlocked', 'sprained', \n",
    "      'rejected', 'fixed', 'washed', 'returned', 'recorded', 'reloaded', 'arrested', 'lathered', 'concealed', \n",
    "      'faded', 'dismissed', 'contacted', 'locked', 'blended', 'groaned', 'towered', 'purred', 'polished', \n",
    "      'flaunted', 'knocked', 'barked', 'quieted', 'towed', 'corrected', 'bargained', 'packed', 'searched', \n",
    "      'resigned', 'clamped', 'crossed', 'obeyed', 'repaired', 'covered', 'squeaked', 'scolded', 'diminished', \n",
    "      'splattered', 'stenciled', 'halted', 'shattered', 'foamed', 'stretched', 'reappeared', 'regained', \n",
    "      'scattered', 'endangered', 'surrendered', 'gasped', 'protested', 'awakened', 'coasted', 'powered', \n",
    "      'lingered', 'remembered', 'disappeared', 'handed', 'blushed', 'tested', 'tuned', 'honked', 'calmed', \n",
    "      'vanished', 'replayed', 'sprouted', 'punished', 'casted', 'hosted', 'melted', 'shined', 'erupted', \n",
    "      'suspended', 'formed', 'roamed', 'crawled', 'browned', 'slowed', 'recoiled', 'adapted', 'healed', \n",
    "      'recalled', 'wavered', 'kneeled', 'waited', 'expanded', 'fizzed', 'distracted', 'offered', 'botched', \n",
    "      'lavished', 'blistered', 'chucked', 'campaigned', 'flattened']\n",
    "\n",
    "LY = ['illegally', 'unethically', 'badly', 'permanently', 'blankly', 'instinctively', 'loosely', 'slowly', \n",
    "      'urgently', 'wrongly', 'safely', 'politely']\n",
    "\n",
    "Y = ['tidied', 'envied', 'denied', 'dried', 'emptied']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(path, name):\n",
    "    \"\"\"\n",
    "    Load date from file\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file) as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        item = json.loads(line)\n",
    "        data.append(item[name])\n",
    "    return data\n",
    "\n",
    "def cut(s):\n",
    "    \"\"\"\n",
    "    Word segmentation\n",
    "    \"\"\"\n",
    "    pattern = r'''\n",
    "              (?x)                   # set flag to allow verbose regexps \n",
    "              (?:[A-Z]\\.)+           # abbreviations, e.g. U.S.A. \n",
    "              |\\w+(?:[-&']\\w+)*      # words w/ optional internal hyphens/apostrophe  \n",
    "            '''  \n",
    "    return regexp_tokenize(s, pattern=pattern)\n",
    "\n",
    "def clean(s):\n",
    "    \"\"\"\n",
    "    Clean words\n",
    "    \"\"\"\n",
    "    s = [replDict.get(i.lower(), i.lower()) for i in s]\n",
    "    s = [wnl.lemmatize(i) if i in lemmWord else i for i in s]\n",
    "    s = [porter.stem(i) if i in stemWord else i for i in s]\n",
    "    s = [i[:-3] if i in ING else i for i in s]\n",
    "    s = [i[:-1] if i in D else i for i in s]\n",
    "    s = [i[:-2] if i in ED else i for i in s]\n",
    "    s = [i[:-2] if i in LY else i for i in s]\n",
    "    s = [i[:-3]+'y' if i in Y else i for i in s]\n",
    "    return s\n",
    "\n",
    "def del_stop(s):\n",
    "    \"\"\"\n",
    "    Delete stop words\n",
    "    \"\"\"\n",
    "    return [i for i in s if i not in stopWord]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-all.json', 'premise')\n",
    "asks_for = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-all.json', 'asks-for')\n",
    "alternative1 = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-all.json', 'alternative1')\n",
    "alternative2 = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-all.json', 'alternative2')\n",
    "\n",
    "rawLabel = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-dev.json', 'most-plausible-alternative')\n",
    "rawLabel = [int(l) for l in rawLabel]\n",
    "\n",
    "cause = []\n",
    "effect = []\n",
    "for i in range(500):\n",
    "    c = []\n",
    "    e = []\n",
    "    if asks_for[i] == 'cause':\n",
    "        c.append(alternative1[i])\n",
    "        c.append(alternative2[i])\n",
    "        e.append(premise[i])\n",
    "    else:\n",
    "        c.append(premise[i])\n",
    "        e.append(alternative1[i])\n",
    "        e.append(alternative2[i])\n",
    "    cause.append(c)\n",
    "    effect.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preWord = [del_stop(clean(cut(s))) for s in premise]\n",
    "a1Word = [del_stop(clean(cut(s))) for s in alternative1]\n",
    "a2Word = [del_stop(clean(cut(s))) for s in alternative2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2906"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(sum(preWord+a1Word+a2Word, [])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'termites' in sum(preWord, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'termites'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-93903f41187c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreWord\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-101-93903f41187c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreWord\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-101-93903f41187c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreWord\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'termites'"
     ]
    }
   ],
   "source": [
    "p = [[word2index[i] for i in s] for s in preWord]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
