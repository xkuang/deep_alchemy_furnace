{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Checkpoint\" data-toc-modified-id=\"Checkpoint-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Checkpoint</a></div><div class=\"lev1 toc-item\"><a href=\"#Import-Libraries\" data-toc-modified-id=\"Import-Libraries-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Import Libraries</a></div><div class=\"lev1 toc-item\"><a href=\"#Set-Hyperparameters\" data-toc-modified-id=\"Set-Hyperparameters-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Set Hyperparameters</a></div><div class=\"lev1 toc-item\"><a href=\"#Build-Graph\" data-toc-modified-id=\"Build-Graph-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Build Graph</a></div><div class=\"lev1 toc-item\"><a href=\"#Train\" data-toc-modified-id=\"Train-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Train</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('index.pkl', 'rb') as fp:\n",
    "    word2index, index2word = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File('train.h5', 'r') as fh:\n",
    "    xTrain = fh['xTrain'][:]\n",
    "    yTrain = fh['yTrain'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Embedding, Dense, Flatten, BatchNormalization, Activation\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.callbacks import*\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word2index)+1\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_SIZE = 512\n",
    "BATCH_SIZE = 1024\n",
    "NUM_EPOCHS = 256\n",
    "STEPS_PER_EPOCH = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build():\n",
    "    \"\"\"\n",
    "    Build embedding matrix\n",
    "    \"\"\"\n",
    "    K.clear_session()\n",
    "    seq = Input(shape=(None,), dtype='int64')\n",
    "    emb = Embedding(VOCAB_SIZE,\n",
    "                    EMBEDDING_DIM,\n",
    "                    mask_zero=False,\n",
    "                    trainable=True)(seq)\n",
    "    model = Model(inputs=seq, outputs=emb)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding = build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(1,), name='INPUT', dtype='int64')\n",
    "embSeq = embedding(inputs)\n",
    "embSeq = Flatten(name='FLATTEN')(embSeq)\n",
    "dense1 = Dense(HIDDEN_SIZE, activation=None, name='DENSE_1')(embSeq)\n",
    "bn1 = BatchNormalization(name='BN_1')(dense1)\n",
    "act1 = Activation('relu', name='RELU_1')(bn1)\n",
    "dense2 = Dense(HIDDEN_SIZE, activation=None, name='DENSE_2')(act1)\n",
    "bn2 = BatchNormalization(name='BN_2')(dense2)\n",
    "act2 = Activation('relu', name='RELU_2')(bn2)\n",
    "dense3 = Dense(VOCAB_SIZE, activation=None, name='DENSE_3')(act2)\n",
    "bn3 = BatchNormalization(name='BN_3')(dense3)\n",
    "out = Activation('softmax', name='OUTPUT')(bn3)\n",
    "model = Model(inputs=inputs, outputs=out)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "INPUT (InputLayer)           (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              multiple                  715520    \n",
      "_________________________________________________________________\n",
      "FLATTEN (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "DENSE_1 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "BN_1 (BatchNormalization)    (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "RELU_1 (Activation)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "DENSE_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "BN_2 (BatchNormalization)    (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "RELU_2 (Activation)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "DENSE_3 (Dense)              (None, 2795)              1433835   \n",
      "_________________________________________________________________\n",
      "BN_3 (BatchNormalization)    (None, 2795)              11180     \n",
      "_________________________________________________________________\n",
      "OUTPUT (Activation)          (None, 2795)              0         \n",
      "=================================================================\n",
      "Total params: 2,558,871\n",
      "Trainable params: 2,551,233\n",
      "Non-trainable params: 7,638\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = 'cp_logs/weights.{epoch:03d}-{loss:.6f}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "log_string = 'tb_logs/2'\n",
    "tensorboard = TensorBoard(log_dir=log_string)\n",
    "callbacks_list = [checkpoint, tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generator(data, label, batch_size):\n",
    "    \"\"\"\n",
    "    Yield batches of all data\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    while True:\n",
    "        if count >= len(data): \n",
    "            count = 0\n",
    "        x = np.zeros((batch_size, 1))\n",
    "        y = np.zeros((batch_size, 1))\n",
    "        for i in range(batch_size):\n",
    "            n = i + count\n",
    "            if n > len(data)-1:\n",
    "                break\n",
    "            x[i, :] = data[n]\n",
    "            y[i, :] = label[n]\n",
    "        count += batch_size\n",
    "        yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_train = data_generator(xTrain, yTrain, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "2045/2048 [============================>.] - ETA: 0s - loss: 7.1196- ETA: 0s - loss: Epoch 00000: loss improved from inf to 7.11914, saving model to cp_logs/weights.000-7.119139.hdf5\n",
      "2048/2048 [==============================] - 43s - loss: 7.1191    \n",
      "Epoch 2/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.6667Epoch 00001: loss improved from 7.11914 to 6.66671, saving model to cp_logs/weights.001-6.666711.hdf5\n",
      "2048/2048 [==============================] - 46s - loss: 6.6667    \n",
      "Epoch 3/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.5900Epoch 00002: loss improved from 6.66671 to 6.58995, saving model to cp_logs/weights.002-6.589954.hdf5\n",
      "2048/2048 [==============================] - 64s - loss: 6.5900    \n",
      "Epoch 4/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.5657Epoch 00003: loss improved from 6.58995 to 6.56569, saving model to cp_logs/weights.003-6.565694.hdf5\n",
      "2048/2048 [==============================] - 70s - loss: 6.5657    \n",
      "Epoch 5/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.5474Epoch 00004: loss improved from 6.56569 to 6.54743, saving model to cp_logs/weights.004-6.547425.hdf5\n",
      "2048/2048 [==============================] - 86s - loss: 6.5474    \n",
      "Epoch 6/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.5328Epoch 00005: loss improved from 6.54743 to 6.53276, saving model to cp_logs/weights.005-6.532761.hdf5\n",
      "2048/2048 [==============================] - 93s - loss: 6.5328    \n",
      "Epoch 7/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.5212- ETA: 0s - loEpoch 00006: loss improved from 6.53276 to 6.52117, saving model to cp_logs/weights.006-6.521167.hdf5\n",
      "2048/2048 [==============================] - 92s - loss: 6.5212    \n",
      "Epoch 8/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.5120Epoch 00007: loss improved from 6.52117 to 6.51200, saving model to cp_logs/weights.007-6.511998.hdf5\n",
      "2048/2048 [==============================] - 100s - loss: 6.5120   \n",
      "Epoch 9/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.5096Epoch 00008: loss improved from 6.51200 to 6.50959, saving model to cp_logs/weights.008-6.509589.hdf5\n",
      "2048/2048 [==============================] - 101s - loss: 6.5096   \n",
      "Epoch 10/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.5069Epoch 00009: loss improved from 6.50959 to 6.50689, saving model to cp_logs/weights.009-6.506889.hdf5\n",
      "2048/2048 [==============================] - 101s - loss: 6.5069   \n",
      "Epoch 11/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.5052Epoch 00010: loss improved from 6.50689 to 6.50524, saving model to cp_logs/weights.010-6.505239.hdf5\n",
      "2048/2048 [==============================] - 102s - loss: 6.5052   \n",
      "Epoch 12/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.5021Epoch 00011: loss improved from 6.50524 to 6.50207, saving model to cp_logs/weights.011-6.502071.hdf5\n",
      "2048/2048 [==============================] - 102s - loss: 6.5021   \n",
      "Epoch 13/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.5014Epoch 00012: loss improved from 6.50207 to 6.50137, saving model to cp_logs/weights.012-6.501367.hdf5\n",
      "2048/2048 [==============================] - 102s - loss: 6.5014   \n",
      "Epoch 14/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4982Epoch 00013: loss improved from 6.50137 to 6.49817, saving model to cp_logs/weights.013-6.498174.hdf5\n",
      "2048/2048 [==============================] - 101s - loss: 6.4982   \n",
      "Epoch 15/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4981Epoch 00014: loss improved from 6.49817 to 6.49810, saving model to cp_logs/weights.014-6.498101.hdf5\n",
      "2048/2048 [==============================] - 102s - loss: 6.4981   \n",
      "Epoch 16/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4958Epoch 00015: loss improved from 6.49810 to 6.49574, saving model to cp_logs/weights.015-6.495737.hdf5\n",
      "2048/2048 [==============================] - 103s - loss: 6.4957   \n",
      "Epoch 17/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4975Epoch 00016: loss did not improve\n",
      "2048/2048 [==============================] - 101s - loss: 6.4975   \n",
      "Epoch 18/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4969Epoch 00017: loss did not improve\n",
      "2048/2048 [==============================] - 102s - loss: 6.4969   \n",
      "Epoch 19/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4961Epoch 00018: loss did not improve\n",
      "2048/2048 [==============================] - 102s - loss: 6.4961   \n",
      "Epoch 20/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4927Epoch 00019: loss improved from 6.49574 to 6.49274, saving model to cp_logs/weights.019-6.492745.hdf5\n",
      "2048/2048 [==============================] - 102s - loss: 6.4927   \n",
      "Epoch 21/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4921Epoch 00020: loss improved from 6.49274 to 6.49206, saving model to cp_logs/weights.020-6.492056.hdf5\n",
      "2048/2048 [==============================] - 103s - loss: 6.4921   \n",
      "Epoch 22/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4902Epoch 00021: loss improved from 6.49206 to 6.49026, saving model to cp_logs/weights.021-6.490260.hdf5\n",
      "2048/2048 [==============================] - 103s - loss: 6.4903   \n",
      "Epoch 23/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4920Epoch 00022: loss did not improve\n",
      "2048/2048 [==============================] - 103s - loss: 6.4920   \n",
      "Epoch 24/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4907Epoch 00023: loss did not improve\n",
      "2048/2048 [==============================] - 103s - loss: 6.4907   \n",
      "Epoch 25/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4910Epoch 00024: loss did not improve\n",
      "2048/2048 [==============================] - 103s - loss: 6.4910   \n",
      "Epoch 26/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4910Epoch 00025: loss did not improve\n",
      "2048/2048 [==============================] - 104s - loss: 6.4910   \n",
      "Epoch 27/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4884Epoch 00026: loss improved from 6.49026 to 6.48841, saving model to cp_logs/weights.026-6.488409.hdf5\n",
      "2048/2048 [==============================] - 103s - loss: 6.4884   \n",
      "Epoch 28/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4895Epoch 00027: loss did not improve\n",
      "2048/2048 [==============================] - 103s - loss: 6.4895   \n",
      "Epoch 29/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4883Epoch 00028: loss improved from 6.48841 to 6.48835, saving model to cp_logs/weights.028-6.488345.hdf5\n",
      "2048/2048 [==============================] - 103s - loss: 6.4883   \n",
      "Epoch 30/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4886Epoch 00029: loss did not improve\n",
      "2048/2048 [==============================] - 104s - loss: 6.4886   \n",
      "Epoch 31/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4870Epoch 00030: loss improved from 6.48835 to 6.48702, saving model to cp_logs/weights.030-6.487016.hdf5\n",
      "2048/2048 [==============================] - 104s - loss: 6.4870   \n",
      "Epoch 32/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4854Epoch 00031: loss improved from 6.48702 to 6.48536, saving model to cp_logs/weights.031-6.485358.hdf5\n",
      "2048/2048 [==============================] - 105s - loss: 6.4854   \n",
      "Epoch 33/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4862Epoch 00032: loss did not improve\n",
      "2048/2048 [==============================] - 105s - loss: 6.4862   \n",
      "Epoch 34/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4882- ETA: 0s - loss: 6.48Epoch 00033: loss did not improve\n",
      "2048/2048 [==============================] - 105s - loss: 6.4883   \n",
      "Epoch 35/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4860Epoch 00034: loss did not improve\n",
      "2048/2048 [==============================] - 105s - loss: 6.4860   \n",
      "Epoch 36/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4843Epoch 00035: loss improved from 6.48536 to 6.48430, saving model to cp_logs/weights.035-6.484299.hdf5\n",
      "2048/2048 [==============================] - 105s - loss: 6.4843   \n",
      "Epoch 37/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4853Epoch 00036: loss did not improve\n",
      "2048/2048 [==============================] - 96s - loss: 6.4853    \n",
      "Epoch 38/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4863Epoch 00037: loss did not improve\n",
      "2048/2048 [==============================] - 96s - loss: 6.4863    \n",
      "Epoch 39/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4833Epoch 00038: loss improved from 6.48430 to 6.48329, saving model to cp_logs/weights.038-6.483287.hdf5\n",
      "2048/2048 [==============================] - 96s - loss: 6.4833    \n",
      "Epoch 40/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4824Epoch 00039: loss improved from 6.48329 to 6.48246, saving model to cp_logs/weights.039-6.482457.hdf5\n",
      "2048/2048 [==============================] - 87s - loss: 6.4825    \n",
      "Epoch 41/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4844Epoch 00040: loss did not improve\n",
      "2048/2048 [==============================] - 90s - loss: 6.4843    \n",
      "Epoch 42/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4840Epoch 00041: loss did not improve\n",
      "2048/2048 [==============================] - 96s - loss: 6.4841    \n",
      "Epoch 43/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4837Epoch 00042: loss did not improve\n",
      "2048/2048 [==============================] - 92s - loss: 6.4837    \n",
      "Epoch 44/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4843Epoch 00043: loss did not improve\n",
      "2048/2048 [==============================] - 91s - loss: 6.4843    \n",
      "Epoch 45/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4850Epoch 00044: loss did not improve\n",
      "2048/2048 [==============================] - 92s - loss: 6.4850    \n",
      "Epoch 46/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4824- ETA: 0s - losEpoch 00045: loss did not improve\n",
      "2048/2048 [==============================] - 94s - loss: 6.4825    \n",
      "Epoch 47/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4822Epoch 00046: loss improved from 6.48246 to 6.48227, saving model to cp_logs/weights.046-6.482266.hdf5\n",
      "2048/2048 [==============================] - 93s - loss: 6.4823    \n",
      "Epoch 48/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4811Epoch 00047: loss improved from 6.48227 to 6.48109, saving model to cp_logs/weights.047-6.481087.hdf5\n",
      "2048/2048 [==============================] - 97s - loss: 6.4811    \n",
      "Epoch 49/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4816Epoch 00048: loss did not improve\n",
      "2048/2048 [==============================] - 88s - loss: 6.4816    \n",
      "Epoch 50/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4821Epoch 00049: loss did not improve\n",
      "2048/2048 [==============================] - 93s - loss: 6.4821    \n",
      "Epoch 51/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4823Epoch 00050: loss did not improve\n",
      "2048/2048 [==============================] - 94s - loss: 6.4823    \n",
      "Epoch 52/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4826Epoch 00051: loss did not improve\n",
      "2048/2048 [==============================] - 93s - loss: 6.4826    \n",
      "Epoch 53/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4813Epoch 00052: loss did not improve\n",
      "2048/2048 [==============================] - 85s - loss: 6.4813    \n",
      "Epoch 54/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4807- ETA: 1Epoch 00053: loss improved from 6.48109 to 6.48072, saving model to cp_logs/weights.053-6.480721.hdf5\n",
      "2048/2048 [==============================] - 96s - loss: 6.4807    \n",
      "Epoch 55/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4830Epoch 00054: loss did not improve\n",
      "2048/2048 [==============================] - 91s - loss: 6.4830    \n",
      "Epoch 56/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4805Epoch 00055: loss improved from 6.48072 to 6.48046, saving model to cp_logs/weights.055-6.480458.hdf5\n",
      "2048/2048 [==============================] - 97s - loss: 6.4805    \n",
      "Epoch 57/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4810Epoch 00056: loss did not improve\n",
      "2048/2048 [==============================] - 91s - loss: 6.4810    \n",
      "Epoch 58/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4780Epoch 00057: loss improved from 6.48046 to 6.47793, saving model to cp_logs/weights.057-6.477931.hdf5\n",
      "2048/2048 [==============================] - 101s - loss: 6.4779   \n",
      "Epoch 59/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4795Epoch 00058: loss did not improve\n",
      "2048/2048 [==============================] - 101s - loss: 6.4794   \n",
      "Epoch 60/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4777Epoch 00059: loss improved from 6.47793 to 6.47766, saving model to cp_logs/weights.059-6.477663.hdf5\n",
      "2048/2048 [==============================] - 101s - loss: 6.4777   \n",
      "Epoch 61/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4812- ETA: 1s - Epoch 00060: loss did not improve\n",
      "2048/2048 [==============================] - 102s - loss: 6.4812   \n",
      "Epoch 62/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4792Epoch 00061: loss did not improve\n",
      "2048/2048 [==============================] - 102s - loss: 6.4791   \n",
      "Epoch 63/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4777Epoch 00062: loss did not improve\n",
      "2048/2048 [==============================] - 101s - loss: 6.4778   \n",
      "Epoch 64/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4791Epoch 00063: loss did not improve\n",
      "2048/2048 [==============================] - 102s - loss: 6.4791   \n",
      "Epoch 65/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4788Epoch 00064: loss did not improve\n",
      "2048/2048 [==============================] - 102s - loss: 6.4788   \n",
      "Epoch 66/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4790Epoch 00065: loss did not improve\n",
      "2048/2048 [==============================] - 102s - loss: 6.4790   \n",
      "Epoch 67/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4772Epoch 00066: loss improved from 6.47766 to 6.47720, saving model to cp_logs/weights.066-6.477202.hdf5\n",
      "2048/2048 [==============================] - 102s - loss: 6.4772   \n",
      "Epoch 68/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4776Epoch 00067: loss did not improve\n",
      "2048/2048 [==============================] - 102s - loss: 6.4776   \n",
      "Epoch 69/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4789Epoch 00068: loss did not improve\n",
      "2048/2048 [==============================] - 103s - loss: 6.4789   \n",
      "Epoch 70/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4784Epoch 00069: loss did not improve\n",
      "2048/2048 [==============================] - 103s - loss: 6.4783   \n",
      "Epoch 71/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4780Epoch 00070: loss did not improve\n",
      "2048/2048 [==============================] - 102s - loss: 6.4780   \n",
      "Epoch 72/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4790Epoch 00071: loss did not improve\n",
      "2048/2048 [==============================] - 103s - loss: 6.4790   \n",
      "Epoch 73/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4780Epoch 00072: loss did not improve\n",
      "2048/2048 [==============================] - 103s - loss: 6.4779   \n",
      "Epoch 74/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4785Epoch 00073: loss did not improve\n",
      "2048/2048 [==============================] - 103s - loss: 6.4785   \n",
      "Epoch 75/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4771Epoch 00074: loss improved from 6.47720 to 6.47713, saving model to cp_logs/weights.074-6.477126.hdf5\n",
      "2048/2048 [==============================] - 104s - loss: 6.4771   \n",
      "Epoch 76/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4783Epoch 00075: loss did not improve\n",
      "2048/2048 [==============================] - 104s - loss: 6.4783   \n",
      "Epoch 77/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4773Epoch 00076: loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048/2048 [==============================] - 105s - loss: 6.4773   \n",
      "Epoch 78/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4780Epoch 00077: loss did not improve\n",
      "2048/2048 [==============================] - 96s - loss: 6.4780    \n",
      "Epoch 79/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4766Epoch 00078: loss improved from 6.47713 to 6.47668, saving model to cp_logs/weights.078-6.476682.hdf5\n",
      "2048/2048 [==============================] - 96s - loss: 6.4767    \n",
      "Epoch 80/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4775Epoch 00079: loss did not improve\n",
      "2048/2048 [==============================] - 93s - loss: 6.4775    \n",
      "Epoch 81/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4765Epoch 00080: loss improved from 6.47668 to 6.47655, saving model to cp_logs/weights.080-6.476553.hdf5\n",
      "2048/2048 [==============================] - 93s - loss: 6.4766    \n",
      "Epoch 82/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4759Epoch 00081: loss improved from 6.47655 to 6.47597, saving model to cp_logs/weights.081-6.475973.hdf5\n",
      "2048/2048 [==============================] - 89s - loss: 6.4760    \n",
      "Epoch 83/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4757Epoch 00082: loss improved from 6.47597 to 6.47564, saving model to cp_logs/weights.082-6.475640.hdf5\n",
      "2048/2048 [==============================] - 94s - loss: 6.4756    \n",
      "Epoch 84/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4770Epoch 00083: loss did not improve\n",
      "2048/2048 [==============================] - 96s - loss: 6.4770    \n",
      "Epoch 85/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4788Epoch 00084: loss did not improve\n",
      "2048/2048 [==============================] - 96s - loss: 6.4788    \n",
      "Epoch 86/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4749Epoch 00085: loss improved from 6.47564 to 6.47488, saving model to cp_logs/weights.085-6.474885.hdf5\n",
      "2048/2048 [==============================] - 95s - loss: 6.4749    \n",
      "Epoch 87/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4777Epoch 00086: loss did not improve\n",
      "2048/2048 [==============================] - 93s - loss: 6.4777    \n",
      "Epoch 88/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4767Epoch 00087: loss did not improve\n",
      "2048/2048 [==============================] - 96s - loss: 6.4767    \n",
      "Epoch 89/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4763Epoch 00088: loss did not improve\n",
      "2048/2048 [==============================] - 97s - loss: 6.4763    \n",
      "Epoch 90/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4789Epoch 00089: loss did not improve\n",
      "2048/2048 [==============================] - 97s - loss: 6.4789    \n",
      "Epoch 91/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4745Epoch 00090: loss improved from 6.47488 to 6.47449, saving model to cp_logs/weights.090-6.474491.hdf5\n",
      "2048/2048 [==============================] - 97s - loss: 6.4745    \n",
      "Epoch 92/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4743- ETA: 0s - loss: Epoch 00091: loss improved from 6.47449 to 6.47428, saving model to cp_logs/weights.091-6.474278.hdf5\n",
      "2048/2048 [==============================] - 97s - loss: 6.4743    \n",
      "Epoch 93/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4741Epoch 00092: loss improved from 6.47428 to 6.47405, saving model to cp_logs/weights.092-6.474054.hdf5\n",
      "2048/2048 [==============================] - 98s - loss: 6.4741    \n",
      "Epoch 94/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4731Epoch 00093: loss improved from 6.47405 to 6.47304, saving model to cp_logs/weights.093-6.473038.hdf5\n",
      "2048/2048 [==============================] - 96s - loss: 6.4730    \n",
      "Epoch 95/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4737Epoch 00094: loss did not improve\n",
      "2048/2048 [==============================] - 97s - loss: 6.4737    \n",
      "Epoch 96/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4759Epoch 00095: loss did not improve\n",
      "2048/2048 [==============================] - 97s - loss: 6.4759    \n",
      "Epoch 97/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4738- ETA: 0s - loss: 6.Epoch 00096: loss did not improve\n",
      "2048/2048 [==============================] - 97s - loss: 6.4739    \n",
      "Epoch 98/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4749Epoch 00097: loss did not improve\n",
      "2048/2048 [==============================] - 98s - loss: 6.4749    \n",
      "Epoch 99/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4735Epoch 00098: loss did not improve\n",
      "2048/2048 [==============================] - 97s - loss: 6.4735    \n",
      "Epoch 100/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4733Epoch 00099: loss did not improve\n",
      "2048/2048 [==============================] - 98s - loss: 6.4733    \n",
      "Epoch 101/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4740Epoch 00100: loss did not improve\n",
      "2048/2048 [==============================] - 98s - loss: 6.4740    \n",
      "Epoch 102/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4737Epoch 00101: loss did not improve\n",
      "2048/2048 [==============================] - 97s - loss: 6.4737    \n",
      "Epoch 103/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4727Epoch 00102: loss improved from 6.47304 to 6.47267, saving model to cp_logs/weights.102-6.472666.hdf5\n",
      "2048/2048 [==============================] - 97s - loss: 6.4727    \n",
      "Epoch 104/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4723- ETA: 0s - loss:Epoch 00103: loss improved from 6.47267 to 6.47229, saving model to cp_logs/weights.103-6.472290.hdf5\n",
      "2048/2048 [==============================] - 98s - loss: 6.4723    \n",
      "Epoch 105/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4725Epoch 00104: loss did not improve\n",
      "2048/2048 [==============================] - 97s - loss: 6.4725    \n",
      "Epoch 106/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4734Epoch 00105: loss did not improve\n",
      "2048/2048 [==============================] - 98s - loss: 6.4733    \n",
      "Epoch 107/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4746Epoch 00106: loss did not improve\n",
      "2048/2048 [==============================] - 97s - loss: 6.4745    \n",
      "Epoch 108/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4744Epoch 00107: loss did not improve\n",
      "2048/2048 [==============================] - 98s - loss: 6.4744    \n",
      "Epoch 109/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4747Epoch 00108: loss did not improve\n",
      "2048/2048 [==============================] - 98s - loss: 6.4747    \n",
      "Epoch 110/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4720Epoch 00109: loss improved from 6.47229 to 6.47200, saving model to cp_logs/weights.109-6.471999.hdf5\n",
      "2048/2048 [==============================] - 98s - loss: 6.4720    \n",
      "Epoch 111/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4714Epoch 00110: loss improved from 6.47200 to 6.47139, saving model to cp_logs/weights.110-6.471388.hdf5\n",
      "2048/2048 [==============================] - 98s - loss: 6.4714    \n",
      "Epoch 112/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4717Epoch 00111: loss did not improve\n",
      "2048/2048 [==============================] - 97s - loss: 6.4717    \n",
      "Epoch 113/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4726Epoch 00112: loss did not improve\n",
      "2048/2048 [==============================] - 97s - loss: 6.4726    \n",
      "Epoch 114/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4741Epoch 00113: loss did not improve\n",
      "2048/2048 [==============================] - 98s - loss: 6.4741    \n",
      "Epoch 115/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4728Epoch 00114: loss did not improve\n",
      "2048/2048 [==============================] - 98s - loss: 6.4728    \n",
      "Epoch 116/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4739Epoch 00115: loss did not improve\n",
      "2048/2048 [==============================] - 97s - loss: 6.4739    \n",
      "Epoch 117/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4726Epoch 00116: loss did not improve\n",
      "2048/2048 [==============================] - 84s - loss: 6.4726    \n",
      "Epoch 118/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4728Epoch 00117: loss did not improve\n",
      "2048/2048 [==============================] - 95s - loss: 6.4728    \n",
      "Epoch 119/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4732Epoch 00118: loss did not improve\n",
      "2048/2048 [==============================] - 94s - loss: 6.4732    \n",
      "Epoch 120/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4719Epoch 00119: loss did not improve\n",
      "2048/2048 [==============================] - 86s - loss: 6.4720    \n",
      "Epoch 121/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4722Epoch 00120: loss did not improve\n",
      "2048/2048 [==============================] - 93s - loss: 6.4722    \n",
      "Epoch 122/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4719Epoch 00121: loss did not improve\n",
      "2048/2048 [==============================] - 91s - loss: 6.4719    \n",
      "Epoch 123/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4722Epoch 00122: loss did not improve\n",
      "2048/2048 [==============================] - 95s - loss: 6.4722    \n",
      "Epoch 124/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4732Epoch 00123: loss did not improve\n",
      "2048/2048 [==============================] - 88s - loss: 6.4731    \n",
      "Epoch 125/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4720Epoch 00124: loss did not improve\n",
      "2048/2048 [==============================] - 88s - loss: 6.4721    \n",
      "Epoch 126/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4718Epoch 00125: loss did not improve\n",
      "2048/2048 [==============================] - 92s - loss: 6.4718    \n",
      "Epoch 127/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4722- ETA: 0s - loss:Epoch 00126: loss did not improve\n",
      "2048/2048 [==============================] - 91s - loss: 6.4722    \n",
      "Epoch 128/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4725Epoch 00127: loss did not improve\n",
      "2048/2048 [==============================] - 95s - loss: 6.4725    \n",
      "Epoch 129/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4706Epoch 00128: loss improved from 6.47139 to 6.47062, saving model to cp_logs/weights.128-6.470622.hdf5\n",
      "2048/2048 [==============================] - 87s - loss: 6.4706    \n",
      "Epoch 130/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4720Epoch 00129: loss did not improve\n",
      "2048/2048 [==============================] - 91s - loss: 6.4720    \n",
      "Epoch 131/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4722Epoch 00130: loss did not improve\n",
      "2048/2048 [==============================] - 101s - loss: 6.4722   \n",
      "Epoch 132/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4720Epoch 00131: loss did not improve\n",
      "2048/2048 [==============================] - 101s - loss: 6.4721   \n",
      "Epoch 133/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4721Epoch 00132: loss did not improve\n",
      "2048/2048 [==============================] - 101s - loss: 6.4721   \n",
      "Epoch 134/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4726Epoch 00133: loss did not improve\n",
      "2048/2048 [==============================] - 101s - loss: 6.4726   \n",
      "Epoch 135/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4736- ETEpoch 00134: loss did not improve\n",
      "2048/2048 [==============================] - 102s - loss: 6.4736   \n",
      "Epoch 136/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4712Epoch 00135: loss did not improve\n",
      "2048/2048 [==============================] - 101s - loss: 6.4713   \n",
      "Epoch 137/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4719Epoch 00136: loss did not improve\n",
      "2048/2048 [==============================] - 101s - loss: 6.4719   \n",
      "Epoch 138/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4702Epoch 00137: loss improved from 6.47062 to 6.47016, saving model to cp_logs/weights.137-6.470159.hdf5\n",
      "2048/2048 [==============================] - 102s - loss: 6.4702   \n",
      "Epoch 139/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4718Epoch 00138: loss did not improve\n",
      "2048/2048 [==============================] - 101s - loss: 6.4718   \n",
      "Epoch 140/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4726Epoch 00139: loss did not improve\n",
      "2048/2048 [==============================] - 102s - loss: 6.4725   \n",
      "Epoch 141/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4728Epoch 00140: loss did not improve\n",
      "2048/2048 [==============================] - 102s - loss: 6.4728   \n",
      "Epoch 142/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4709Epoch 00141: loss did not improve\n",
      "2048/2048 [==============================] - 102s - loss: 6.4709   \n",
      "Epoch 143/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4730Epoch 00142: loss did not improve\n",
      "2048/2048 [==============================] - 102s - loss: 6.4729   \n",
      "Epoch 144/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4719Epoch 00143: loss did not improve\n",
      "2048/2048 [==============================] - 102s - loss: 6.4719   \n",
      "Epoch 145/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4724Epoch 00144: loss did not improve\n",
      "2048/2048 [==============================] - 102s - loss: 6.4724   \n",
      "Epoch 146/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4694Epoch 00145: loss improved from 6.47016 to 6.46940, saving model to cp_logs/weights.145-6.469399.hdf5\n",
      "2048/2048 [==============================] - 103s - loss: 6.4694   \n",
      "Epoch 147/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4717-Epoch 00146: loss did not improve\n",
      "2048/2048 [==============================] - 102s - loss: 6.4717   \n",
      "Epoch 148/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4694Epoch 00147: loss did not improve\n",
      "2048/2048 [==============================] - 102s - loss: 6.4695   \n",
      "Epoch 149/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4700Epoch 00148: loss did not improve\n",
      "2048/2048 [==============================] - 102s - loss: 6.4701   \n",
      "Epoch 150/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4712Epoch 00149: loss did not improve\n",
      "2048/2048 [==============================] - 102s - loss: 6.4712   \n",
      "Epoch 151/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4698Epoch 00150: loss did not improve\n",
      "2048/2048 [==============================] - 102s - loss: 6.4699   \n",
      "Epoch 152/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4710Epoch 00151: loss did not improve\n",
      "2048/2048 [==============================] - 102s - loss: 6.4711   \n",
      "Epoch 153/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4689Epoch 00152: loss improved from 6.46940 to 6.46889, saving model to cp_logs/weights.152-6.468889.hdf5\n",
      "2048/2048 [==============================] - 103s - loss: 6.4689   \n",
      "Epoch 154/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4720Epoch 00153: loss did not improve\n",
      "2048/2048 [==============================] - 102s - loss: 6.4720   \n",
      "Epoch 155/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4702Epoch 00154: loss did not improve\n",
      "2048/2048 [==============================] - 103s - loss: 6.4702   \n",
      "Epoch 156/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4707Epoch 00155: loss did not improve\n",
      "2048/2048 [==============================] - 102s - loss: 6.4707   \n",
      "Epoch 157/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4689Epoch 00156: loss improved from 6.46889 to 6.46886, saving model to cp_logs/weights.156-6.468860.hdf5\n",
      "2048/2048 [==============================] - 102s - loss: 6.4689   \n",
      "Epoch 158/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4707Epoch 00157: loss did not improve\n",
      "2048/2048 [==============================] - 103s - loss: 6.4706   \n",
      "Epoch 159/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4701Epoch 00158: loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048/2048 [==============================] - 102s - loss: 6.4701   \n",
      "Epoch 160/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4707Epoch 00159: loss did not improve\n",
      "2048/2048 [==============================] - 96s - loss: 6.4707    \n",
      "Epoch 161/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4713Epoch 00160: loss did not improve\n",
      "2048/2048 [==============================] - 94s - loss: 6.4712    \n",
      "Epoch 162/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4716Epoch 00161: loss did not improve\n",
      "2048/2048 [==============================] - 92s - loss: 6.4716    \n",
      "Epoch 163/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4700Epoch 00162: loss did not improve\n",
      "2048/2048 [==============================] - 81s - loss: 6.4700    \n",
      "Epoch 164/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4710Epoch 00163: loss did not improve\n",
      "2048/2048 [==============================] - 95s - loss: 6.4710    \n",
      "Epoch 165/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4710Epoch 00164: loss did not improve\n",
      "2048/2048 [==============================] - 86s - loss: 6.4710    \n",
      "Epoch 166/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4697Epoch 00165: loss did not improve\n",
      "2048/2048 [==============================] - 88s - loss: 6.4697    \n",
      "Epoch 167/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4704Epoch 00166: loss did not improve\n",
      "2048/2048 [==============================] - 88s - loss: 6.4704    \n",
      "Epoch 168/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4703Epoch 00167: loss did not improve\n",
      "2048/2048 [==============================] - 90s - loss: 6.4703    \n",
      "Epoch 169/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4713Epoch 00168: loss did not improve\n",
      "2048/2048 [==============================] - 85s - loss: 6.4713    \n",
      "Epoch 170/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4702Epoch 00169: loss did not improve\n",
      "2048/2048 [==============================] - 91s - loss: 6.4702    \n",
      "Epoch 171/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4688Epoch 00170: loss improved from 6.46886 to 6.46879, saving model to cp_logs/weights.170-6.468791.hdf5\n",
      "2048/2048 [==============================] - 89s - loss: 6.4688    \n",
      "Epoch 172/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4692Epoch 00171: loss did not improve\n",
      "2048/2048 [==============================] - 89s - loss: 6.4692    \n",
      "Epoch 173/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4708Epoch 00172: loss did not improve\n",
      "2048/2048 [==============================] - 82s - loss: 6.4708    \n",
      "Epoch 174/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4700Epoch 00173: loss did not improve\n",
      "2048/2048 [==============================] - 88s - loss: 6.4699    \n",
      "Epoch 175/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4705Epoch 00174: loss did not improve\n",
      "2048/2048 [==============================] - 91s - loss: 6.4705    \n",
      "Epoch 176/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4701Epoch 00175: loss did not improve\n",
      "2048/2048 [==============================] - 85s - loss: 6.4701    \n",
      "Epoch 177/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4714Epoch 00176: loss did not improve\n",
      "2048/2048 [==============================] - 89s - loss: 6.4714    \n",
      "Epoch 178/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4698Epoch 00177: loss did not improve\n",
      "2048/2048 [==============================] - 88s - loss: 6.4698    \n",
      "Epoch 179/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4733Epoch 00178: loss did not improve\n",
      "2048/2048 [==============================] - 78s - loss: 6.4733    \n",
      "Epoch 180/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4694Epoch 00179: loss did not improve\n",
      "2048/2048 [==============================] - 95s - loss: 6.4694    \n",
      "Epoch 181/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4688Epoch 00180: loss did not improve\n",
      "2048/2048 [==============================] - 80s - loss: 6.4689    \n",
      "Epoch 182/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4672- ETEpoch 00181: loss improved from 6.46879 to 6.46722, saving model to cp_logs/weights.181-6.467215.hdf5\n",
      "2048/2048 [==============================] - 91s - loss: 6.4672    \n",
      "Epoch 183/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4691Epoch 00182: loss did not improve\n",
      "2048/2048 [==============================] - 87s - loss: 6.4691    \n",
      "Epoch 184/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4682Epoch 00183: loss did not improve\n",
      "2048/2048 [==============================] - 90s - loss: 6.4682    \n",
      "Epoch 185/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4685Epoch 00184: loss did not improve\n",
      "2048/2048 [==============================] - 85s - loss: 6.4685    \n",
      "Epoch 186/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4701Epoch 00185: loss did not improve\n",
      "2048/2048 [==============================] - 84s - loss: 6.4701    \n",
      "Epoch 187/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4676Epoch 00186: loss did not improve\n",
      "2048/2048 [==============================] - 89s - loss: 6.4676    \n",
      "Epoch 188/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4695Epoch 00187: loss did not improve\n",
      "2048/2048 [==============================] - 91s - loss: 6.4694    \n",
      "Epoch 189/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4691Epoch 00188: loss did not improve\n",
      "2048/2048 [==============================] - 84s - loss: 6.4691    \n",
      "Epoch 190/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4691Epoch 00189: loss did not improve\n",
      "2048/2048 [==============================] - 86s - loss: 6.4691    \n",
      "Epoch 191/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4683Epoch 00190: loss did not improve\n",
      "2048/2048 [==============================] - 92s - loss: 6.4683    \n",
      "Epoch 192/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4682Epoch 00191: loss did not improve\n",
      "2048/2048 [==============================] - 84s - loss: 6.4682    \n",
      "Epoch 193/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4671Epoch 00192: loss improved from 6.46722 to 6.46709, saving model to cp_logs/weights.192-6.467090.hdf5\n",
      "2048/2048 [==============================] - 87s - loss: 6.4671    \n",
      "Epoch 194/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4684Epoch 00193: loss did not improve\n",
      "2048/2048 [==============================] - 89s - loss: 6.4684    \n",
      "Epoch 195/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4668Epoch 00194: loss improved from 6.46709 to 6.46672, saving model to cp_logs/weights.194-6.466716.hdf5\n",
      "2048/2048 [==============================] - 94s - loss: 6.4667    \n",
      "Epoch 196/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4699Epoch 00195: loss did not improve\n",
      "2048/2048 [==============================] - 91s - loss: 6.4699    \n",
      "Epoch 197/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4699Epoch 00196: loss did not improve\n",
      "2048/2048 [==============================] - 89s - loss: 6.4700    \n",
      "Epoch 198/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4702Epoch 00197: loss did not improve\n",
      "2048/2048 [==============================] - 84s - loss: 6.4702    \n",
      "Epoch 199/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4675Epoch 00198: loss did not improve\n",
      "2048/2048 [==============================] - 89s - loss: 6.4675    \n",
      "Epoch 200/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4674Epoch 00199: loss did not improve\n",
      "2048/2048 [==============================] - 83s - loss: 6.4673    \n",
      "Epoch 201/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4667Epoch 00200: loss improved from 6.46672 to 6.46668, saving model to cp_logs/weights.200-6.466680.hdf5\n",
      "2048/2048 [==============================] - 88s - loss: 6.4667    \n",
      "Epoch 202/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4690Epoch 00201: loss did not improve\n",
      "2048/2048 [==============================] - 85s - loss: 6.4690    \n",
      "Epoch 203/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4684Epoch 00202: loss did not improve\n",
      "2048/2048 [==============================] - 83s - loss: 6.4684    \n",
      "Epoch 204/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4686Epoch 00203: loss did not improve\n",
      "2048/2048 [==============================] - 80s - loss: 6.4686    \n",
      "Epoch 205/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4700Epoch 00204: loss did not improve\n",
      "2048/2048 [==============================] - 88s - loss: 6.4700    \n",
      "Epoch 206/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4678Epoch 00205: loss did not improve\n",
      "2048/2048 [==============================] - 85s - loss: 6.4678    \n",
      "Epoch 207/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4690Epoch 00206: loss did not improve\n",
      "2048/2048 [==============================] - 82s - loss: 6.4690    \n",
      "Epoch 208/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4678Epoch 00207: loss did not improve\n",
      "2048/2048 [==============================] - 90s - loss: 6.4678    \n",
      "Epoch 209/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4691Epoch 00208: loss did not improve\n",
      "2048/2048 [==============================] - 83s - loss: 6.4691    \n",
      "Epoch 210/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4678Epoch 00209: loss did not improve\n",
      "2048/2048 [==============================] - 82s - loss: 6.4678    \n",
      "Epoch 211/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4669Epoch 00210: loss did not improve\n",
      "2048/2048 [==============================] - 85s - loss: 6.4669    \n",
      "Epoch 212/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4680Epoch 00211: loss did not improve\n",
      "2048/2048 [==============================] - 83s - loss: 6.4680    \n",
      "Epoch 213/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4699Epoch 00212: loss did not improve\n",
      "2048/2048 [==============================] - 78s - loss: 6.4699    \n",
      "Epoch 214/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4688Epoch 00213: loss did not improve\n",
      "2048/2048 [==============================] - 82s - loss: 6.4688    \n",
      "Epoch 215/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4667Epoch 00214: loss did not improve\n",
      "2048/2048 [==============================] - 96s - loss: 6.4668    \n",
      "Epoch 216/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4680Epoch 00215: loss did not improve\n",
      "2048/2048 [==============================] - 88s - loss: 6.4680    \n",
      "Epoch 217/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4695- ETA: 0s - loss: 6.Epoch 00216: loss did not improve\n",
      "2048/2048 [==============================] - 86s - loss: 6.4695    \n",
      "Epoch 218/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4670Epoch 00217: loss did not improve\n",
      "2048/2048 [==============================] - 88s - loss: 6.4670    \n",
      "Epoch 219/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4661Epoch 00218: loss improved from 6.46668 to 6.46605, saving model to cp_logs/weights.218-6.466049.hdf5\n",
      "2048/2048 [==============================] - 85s - loss: 6.4660    \n",
      "Epoch 220/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4687Epoch 00219: loss did not improve\n",
      "2048/2048 [==============================] - 91s - loss: 6.4687    \n",
      "Epoch 221/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4681Epoch 00220: loss did not improve\n",
      "2048/2048 [==============================] - 85s - loss: 6.4681    \n",
      "Epoch 222/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4685Epoch 00221: loss did not improve\n",
      "2048/2048 [==============================] - 84s - loss: 6.4685    \n",
      "Epoch 223/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4692Epoch 00222: loss did not improve\n",
      "2048/2048 [==============================] - 90s - loss: 6.4692    \n",
      "Epoch 224/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4703-Epoch 00223: loss did not improve\n",
      "2048/2048 [==============================] - 86s - loss: 6.4702    \n",
      "Epoch 225/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4677Epoch 00224: loss did not improve\n",
      "2048/2048 [==============================] - 90s - loss: 6.4677    \n",
      "Epoch 226/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4680Epoch 00225: loss did not improve\n",
      "2048/2048 [==============================] - 76s - loss: 6.4680    \n",
      "Epoch 227/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4664Epoch 00226: loss did not improve\n",
      "2048/2048 [==============================] - 96s - loss: 6.4664    \n",
      "Epoch 228/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4677Epoch 00227: loss did not improve\n",
      "2048/2048 [==============================] - 88s - loss: 6.4677    \n",
      "Epoch 229/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4685Epoch 00228: loss did not improve\n",
      "2048/2048 [==============================] - 88s - loss: 6.4685    \n",
      "Epoch 230/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4684Epoch 00229: loss did not improve\n",
      "2048/2048 [==============================] - 87s - loss: 6.4685    \n",
      "Epoch 231/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4693Epoch 00230: loss did not improve\n",
      "2048/2048 [==============================] - 86s - loss: 6.4693    \n",
      "Epoch 232/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4682Epoch 00231: loss did not improve\n",
      "2048/2048 [==============================] - 91s - loss: 6.4682    \n",
      "Epoch 233/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4673Epoch 00232: loss did not improve\n",
      "2048/2048 [==============================] - 90s - loss: 6.4673    \n",
      "Epoch 234/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4701Epoch 00233: loss did not improve\n",
      "2048/2048 [==============================] - 80s - loss: 6.4700    \n",
      "Epoch 235/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4672Epoch 00234: loss did not improve\n",
      "2048/2048 [==============================] - 91s - loss: 6.4672    \n",
      "Epoch 236/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4685Epoch 00235: loss did not improve\n",
      "2048/2048 [==============================] - 92s - loss: 6.4685    \n",
      "Epoch 237/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4655Epoch 00236: loss improved from 6.46605 to 6.46552, saving model to cp_logs/weights.236-6.465517.hdf5\n",
      "2048/2048 [==============================] - 85s - loss: 6.4655    \n",
      "Epoch 238/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4672Epoch 00237: loss did not improve\n",
      "2048/2048 [==============================] - 90s - loss: 6.4672    \n",
      "Epoch 239/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4657Epoch 00238: loss did not improve\n",
      "2048/2048 [==============================] - 93s - loss: 6.4657    \n",
      "Epoch 240/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4688Epoch 00239: loss did not improve\n",
      "2048/2048 [==============================] - 94s - loss: 6.4688    \n",
      "Epoch 241/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4674Epoch 00240: loss did not improve\n",
      "2048/2048 [==============================] - 95s - loss: 6.4674    \n",
      "Epoch 242/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4654Epoch 00241: loss improved from 6.46552 to 6.46542, saving model to cp_logs/weights.241-6.465423.hdf5\n",
      "2048/2048 [==============================] - 87s - loss: 6.4654    \n",
      "Epoch 243/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4679Epoch 00242: loss did not improve\n",
      "2048/2048 [==============================] - 88s - loss: 6.4679    \n",
      "Epoch 244/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4673Epoch 00243: loss did not improve\n",
      "2048/2048 [==============================] - 96s - loss: 6.4673    \n",
      "Epoch 245/256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4673- ETA:Epoch 00244: loss did not improve\n",
      "2048/2048 [==============================] - 90s - loss: 6.4673    \n",
      "Epoch 246/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4661Epoch 00245: loss did not improve\n",
      "2048/2048 [==============================] - 85s - loss: 6.4661    \n",
      "Epoch 247/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4661Epoch 00246: loss did not improve\n",
      "2048/2048 [==============================] - 88s - loss: 6.4660    \n",
      "Epoch 248/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4680Epoch 00247: loss did not improve\n",
      "2048/2048 [==============================] - 89s - loss: 6.4680    \n",
      "Epoch 249/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4672Epoch 00248: loss did not improve\n",
      "2048/2048 [==============================] - 82s - loss: 6.4673    \n",
      "Epoch 250/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4672Epoch 00249: loss did not improve\n",
      "2048/2048 [==============================] - 90s - loss: 6.4672    \n",
      "Epoch 251/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4684Epoch 00250: loss did not improve\n",
      "2048/2048 [==============================] - 79s - loss: 6.4684    \n",
      "Epoch 252/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4673Epoch 00251: loss did not improve\n",
      "2048/2048 [==============================] - 86s - loss: 6.4673    \n",
      "Epoch 253/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4678Epoch 00252: loss did not improve\n",
      "2048/2048 [==============================] - 80s - loss: 6.4678    \n",
      "Epoch 254/256\n",
      "2046/2048 [============================>.] - ETA: 0s - loss: 6.4671Epoch 00253: loss did not improve\n",
      "2048/2048 [==============================] - 82s - loss: 6.4671    \n",
      "Epoch 255/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4680- EEpoch 00254: loss did not improve\n",
      "2048/2048 [==============================] - 89s - loss: 6.4680    \n",
      "Epoch 256/256\n",
      "2047/2048 [============================>.] - ETA: 0s - loss: 6.4674Epoch 00255: loss did not improve\n",
      "2048/2048 [==============================] - 79s - loss: 6.4674    \n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(gen_train,\n",
    "                              steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                              epochs=NUM_EPOCHS,\n",
    "                              callbacks=callbacks_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
