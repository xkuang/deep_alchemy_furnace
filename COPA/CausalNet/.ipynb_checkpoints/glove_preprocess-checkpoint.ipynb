{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#CausalNet-Preprocess\" data-toc-modified-id=\"CausalNet-Preprocess-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>CausalNet Preprocess</a></div><div class=\"lev1 toc-item\"><a href=\"#COPA-Preprocess-Pro\" data-toc-modified-id=\"COPA-Preprocess-Pro-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>COPA Preprocess Pro</a></div><div class=\"lev1 toc-item\"><a href=\"#Build-Co-occurence-Matrix\" data-toc-modified-id=\"Build-Co-occurence-Matrix-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Build Co-occurence Matrix</a></div><div class=\"lev1 toc-item\"><a href=\"#Trian-Glvoe\" data-toc-modified-id=\"Trian-Glvoe-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Trian Glvoe</a></div><div class=\"lev1 toc-item\"><a href=\"#COPA-Preprocess\" data-toc-modified-id=\"COPA-Preprocess-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>COPA Preprocess</a></div><div class=\"lev1 toc-item\"><a href=\"#Build-Dataset\" data-toc-modified-id=\"Build-Dataset-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Build Dataset</a></div><div class=\"lev2 toc-item\"><a href=\"#Tokenize-Text\" data-toc-modified-id=\"Tokenize-Text-61\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Tokenize Text</a></div><div class=\"lev2 toc-item\"><a href=\"#Create-Word-Embeddings-with-GloVe\" data-toc-modified-id=\"Create-Word-Embeddings-with-GloVe-62\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Create Word Embeddings with GloVe</a></div><div class=\"lev2 toc-item\"><a href=\"#Predict-Effect\" data-toc-modified-id=\"Predict-Effect-63\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Predict Effect</a></div><div class=\"lev2 toc-item\"><a href=\"#Causal-w2v\" data-toc-modified-id=\"Causal-w2v-64\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Causal w2v</a></div><div class=\"lev1 toc-item\"><a href=\"#Data-Preprocessing-for-Downstream-Tasks\" data-toc-modified-id=\"Data-Preprocessing-for-Downstream-Tasks-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Data Preprocessing for Downstream Tasks</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CausalNet Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = '/Users/lizhn7/Downloads/EXPERIMENT/COPA/CausalNet/CausalNet.txt'\n",
    "raw_text = [i.split() for i in open(filename).read().split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62675003"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62675002/62675002 [01:01<00:00, 1020168.89it/s]\n"
     ]
    }
   ],
   "source": [
    "causeWord = []\n",
    "for i in tqdm(range(len(raw_text[:-1]))):\n",
    "    causeWord.append(raw_text[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "causeWord = set(causeWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59411"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(causeWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62675002/62675002 [00:59<00:00, 1046915.86it/s]\n"
     ]
    }
   ],
   "source": [
    "effectWord = []\n",
    "for i in tqdm(range(len(raw_text[:-1]))):\n",
    "    effectWord.append(raw_text[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "effectWord = set(effectWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59710"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(effectWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COPA Preprocess Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File('/Users/lizhn7/Documents/Github/深度炼丹炉/COPA/CausalNet/val_s.h5', 'r') as fh:\n",
    "    dev1a = fh['dev1a'][:]\n",
    "    dev1b = fh['dev1b'][:]  \n",
    "    dev2a = fh['dev2a'][:]\n",
    "    dev2b = fh['dev2b'][:] \n",
    "    yVal = fh['yVal'][:]\n",
    "\n",
    "with h5py.File('/Users/lizhn7/Documents/Github/深度炼丹炉/COPA/CausalNet/test_s.h5', 'r') as fh:\n",
    "    test1a = fh['test1a'][:]\n",
    "    test1b = fh['test1b'][:]  \n",
    "    test2a = fh['test2a'][:]\n",
    "    test2b = fh['test2b'][:] \n",
    "    yTest = fh['yTest'][:]\n",
    "    \n",
    "with open('/Users/lizhn7/Downloads/EXPERIMENT/COPA/CausalNet/index.pkl', 'rb') as fp:\n",
    "    word2index, index2word = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data_1(x, y):\n",
    "    \"\"\"\n",
    "    Delete redundant data\n",
    "    \"\"\"\n",
    "    A = x.copy()\n",
    "    B = y.copy()\n",
    "    for i in range(500):\n",
    "        if any(A[i] != B[i]):\n",
    "            for a in range(MAX_LEN):\n",
    "                for b in range(MAX_LEN):\n",
    "                    if A[i][a] == B[i][b]:\n",
    "                        A[i][a] = 0\n",
    "                        B[i][b] = 0\n",
    "    return (A, B)\n",
    "\n",
    "def clean_data_2(d):\n",
    "    \"\"\"\n",
    "    Delete redundant 0\n",
    "    \"\"\"\n",
    "    d = [[i for i in s if i != 0] for s in d]\n",
    "    d = [[0] if s == [] else s for s in d] \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcd1a, fcd2a = clean_data_1(dev1a, dev2a)\n",
    "fcd1b, fcd2b = clean_data_1(dev1b, dev2b)\n",
    "\n",
    "scd1a, scd1b = clean_data_1(fcd1a, fcd1b)\n",
    "scd2a, scd2b = clean_data_1(fcd2a, fcd2b)\n",
    "\n",
    "tcd1a, _ = clean_data_1(scd1a, dev2b)\n",
    "tcd1b, _ = clean_data_1(scd1b, dev2a)\n",
    "tcd2a, _ = clean_data_1(scd2a, dev1b)\n",
    "tcd2b, _ = clean_data_1(scd2b, dev1a)\n",
    "\n",
    "dev1a = clean_data_2(tcd1a)\n",
    "dev1b = clean_data_2(tcd1b)\n",
    "dev2a = clean_data_2(tcd2a)\n",
    "dev2b = clean_data_2(tcd2b)\n",
    "\n",
    "fcd1a, fcd2a = clean_data_1(test1a, test2a)\n",
    "fcd1b, fcd2b = clean_data_1(test1b, test2b)\n",
    "\n",
    "scd1a, scd1b = clean_data_1(fcd1a, fcd1b)\n",
    "scd2a, scd2b = clean_data_1(fcd2a, fcd2b)\n",
    "\n",
    "tcd1a, _ = clean_data_1(scd1a, test2b)\n",
    "tcd1b, _ = clean_data_1(scd1b, test2a)\n",
    "tcd2a, _ = clean_data_1(scd2a, test1b)\n",
    "tcd2b, _ = clean_data_1(scd2b, test1a)\n",
    "\n",
    "test1a = clean_data_2(tcd1a)\n",
    "test1b = clean_data_2(tcd1b)\n",
    "test2a = clean_data_2(tcd2a)\n",
    "test2b = clean_data_2(tcd2b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = set(sum(dev1a+dev1b+dev2a+dev2b+test1a+test1b+test2a+test2b, []))\n",
    "word = [index2word[i] if i != 0 else 0 for i in index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Co-occurence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62675002/62675002 [37:53<00:00, 27567.77it/s]  \n"
     ]
    }
   ],
   "source": [
    "causalPair = []\n",
    "for i in tqdm(range(len(raw_text[:-1]))):\n",
    "    if raw_text[i][0] in word and raw_text[i][1] in word:\n",
    "        causalPair.append(raw_text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3759318"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(causalPair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "causal_words = [i[0] for i in causalPair] + [i[1] for i in causalPair]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3759318/3759318 [02:32<00:00, 24590.29it/s] \n"
     ]
    }
   ],
   "source": [
    "counts = Counter(causal_words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "# Create dictionary that maps vocab words to integers\n",
    "vocab2int = {word: ii for ii, word in enumerate(vocab, 0)}\n",
    "# Convert the causal words to integers\n",
    "causal_int = [[vocab2int[s[i]] if i < 2 else s[i] for i in range(len(s))] for s in tqdm(causalPair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int2vocab = {i: w for w, i in vocab2int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2618/2618 [08:42<00:00,  5.26it/s]\n"
     ]
    }
   ],
   "source": [
    "cooccur = {i: {c[1]: float(c[-1]) for c in causal_int if c[0] == i} for i in tqdm(range(len(vocab)))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trian Glvoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \t Error 0.075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 1/100 [00:12<20:58, 12.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 \t Error 0.053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 2/100 [00:26<21:16, 13.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 \t Error 0.042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 3/100 [00:42<22:25, 13.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 \t Error 0.036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 4/100 [00:57<22:40, 14.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 \t Error 0.031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 5/100 [01:10<21:59, 13.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 \t Error 0.028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 6/100 [01:27<23:13, 14.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 \t Error 0.025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 7/100 [01:43<23:29, 15.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 \t Error 0.022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 8/100 [01:57<22:38, 14.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 \t Error 0.020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|▉         | 9/100 [02:10<21:40, 14.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 \t Error 0.018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 10/100 [02:23<20:51, 13.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 \t Error 0.017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█         | 11/100 [02:38<21:03, 14.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 \t Error 0.016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 12/100 [02:53<21:23, 14.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 \t Error 0.015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 13/100 [03:09<21:37, 14.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 \t Error 0.014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 14/100 [03:26<22:07, 15.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 \t Error 0.014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 15/100 [03:40<21:27, 15.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 \t Error 0.014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 16/100 [03:52<20:03, 14.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 \t Error 0.013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 17/100 [04:05<18:52, 13.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 \t Error 0.013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 18/100 [04:17<18:02, 13.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 \t Error 0.013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|█▉        | 19/100 [04:29<17:34, 13.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 \t Error 0.013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 20/100 [04:41<17:00, 12.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 \t Error 0.013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██        | 21/100 [04:54<16:32, 12.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 \t Error 0.013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 22/100 [05:06<16:06, 12.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 \t Error 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 23/100 [05:18<15:48, 12.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 \t Error 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▍       | 24/100 [05:30<15:30, 12.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 \t Error 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 25/100 [05:42<15:16, 12.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 \t Error 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██▌       | 26/100 [05:54<15:03, 12.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 \t Error 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 27/100 [06:06<14:47, 12.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 \t Error 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 28/100 [06:18<14:33, 12.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 \t Error 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▉       | 29/100 [06:31<14:27, 12.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 \t Error 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 30/100 [06:43<14:11, 12.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 \t Error 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███       | 31/100 [06:55<13:55, 12.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 \t Error 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 32/100 [07:07<13:40, 12.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 \t Error 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 33/100 [07:19<13:26, 12.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 \t Error 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███▍      | 34/100 [07:31<13:15, 12.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 \t Error 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 35/100 [07:43<13:02, 12.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 \t Error 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▌      | 36/100 [07:55<12:50, 12.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 \t Error 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|███▋      | 37/100 [08:07<12:36, 12.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 \t Error 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 38/100 [08:19<12:24, 12.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 \t Error 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|███▉      | 39/100 [08:31<12:18, 12.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 \t Error 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 40/100 [08:43<12:02, 12.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 \t Error 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|████      | 41/100 [08:55<11:49, 12.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 \t Error 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████▏     | 42/100 [09:07<11:34, 11.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 \t Error 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████▎     | 43/100 [09:19<11:21, 11.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 \t Error 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████▍     | 44/100 [09:30<11:07, 11.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 \t Error 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 45/100 [09:42<10:54, 11.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 \t Error 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████▌     | 46/100 [09:54<10:43, 11.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 \t Error 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|████▋     | 47/100 [10:06<10:30, 11.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 \t Error 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████▊     | 48/100 [10:18<10:19, 11.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 \t Error 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|████▉     | 49/100 [10:31<10:16, 12.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 \t Error 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 50/100 [10:42<10:01, 12.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51 \t Error 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|█████     | 51/100 [10:54<09:48, 12.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52 \t Error 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|█████▏    | 52/100 [11:06<09:34, 11.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 \t Error 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|█████▎    | 53/100 [11:18<09:21, 11.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████▍    | 54/100 [11:30<09:08, 11.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 55/100 [11:42<08:56, 11.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████▌    | 56/100 [11:54<08:43, 11.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████▋    | 57/100 [12:06<08:31, 11.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|█████▊    | 58/100 [12:18<08:23, 11.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|█████▉    | 59/100 [12:30<08:14, 12.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 60/100 [12:42<08:00, 12.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|██████    | 61/100 [12:55<07:56, 12.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▏   | 62/100 [13:08<07:55, 12.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|██████▎   | 63/100 [13:22<08:02, 13.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████▍   | 64/100 [13:37<08:05, 13.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 65/100 [13:51<07:55, 13.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|██████▌   | 66/100 [14:05<07:52, 13.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 67/100 [14:20<07:50, 14.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████▊   | 68/100 [14:36<07:49, 14.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|██████▉   | 69/100 [14:51<07:42, 14.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 70/100 [15:10<07:57, 15.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|███████   | 71/100 [15:25<07:34, 15.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████▏  | 72/100 [15:40<07:11, 15.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████▎  | 73/100 [15:54<06:51, 15.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|███████▍  | 74/100 [16:09<06:31, 15.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 75/100 [16:23<06:08, 14.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████▌  | 76/100 [16:38<05:52, 14.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████▋  | 77/100 [16:51<05:32, 14.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|███████▊  | 78/100 [17:05<05:13, 14.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████▉  | 79/100 [17:19<04:55, 14.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 80/100 [17:33<04:38, 13.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|████████  | 81/100 [17:46<04:24, 13.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|████████▏ | 82/100 [18:01<04:12, 14.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 83/100 [18:14<03:57, 13.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|████████▍ | 84/100 [18:28<03:42, 13.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 85/100 [18:44<03:37, 14.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████▌ | 86/100 [18:57<03:17, 14.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████▋ | 87/100 [19:12<03:05, 14.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 88/100 [19:27<02:53, 14.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|████████▉ | 89/100 [19:42<02:40, 14.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 90/100 [19:58<02:30, 15.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|█████████ | 91/100 [20:13<02:14, 14.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 92/100 [20:27<01:58, 14.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|█████████▎| 93/100 [20:42<01:43, 14.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████▍| 94/100 [20:56<01:27, 14.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 95/100 [21:10<01:11, 14.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▌| 96/100 [21:23<00:56, 14.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|█████████▋| 97/100 [21:37<00:42, 14.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|█████████▊| 98/100 [21:51<00:28, 14.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|█████████▉| 99/100 [22:07<00:14, 14.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 \t Error 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [22:21<00:00, 14.31s/it]\n"
     ]
    }
   ],
   "source": [
    "model = glove.Glove(cooccur, d=300, alpha=0.75, x_max=100.0)\n",
    "\n",
    "for epoch in tqdm(range(100)):\n",
    "    err = model.train(step_size=0.05, workers=9, batch_size=50)\n",
    "    print(\"Epoch %d \\t Error %.3f\" % (epoch+1, err), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_embedding = model.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/Users/lizhn7/Downloads/EXPERIMENT/COPA/CausalNet/causal_index.pkl', 'wb') as fp:\n",
    "    pickle.dump((vocab2int, int2vocab), fp, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fh = h5py.File('/Users/lizhn7/Downloads/EXPERIMENT/COPA/CausalNet/causal_embedding.h5', 'w')\n",
    "fh['causal_embedding'] = causal_embedding\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# COPA Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from nltk import regexp_tokenize\n",
    "from nltk import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#oovWord = '/Users/lizhn7/Documents/Github/深度炼丹炉/COPA/CausalNet/OOV.txt'\n",
    "#oovWord = open(oovWord).read().split()\n",
    "stopWord = '/Users/lizhn7/Documents/Github/深度炼丹炉/COPA/CausalNet/stopwords.txt'\n",
    "stopWord = open(stopWord).read().split()\n",
    "lemmWord = '/Users/lizhn7/Documents/Github/深度炼丹炉/COPA/CausalNet/WordNetLemmatizer.txt'\n",
    "lemmWord = open(lemmWord).read().split()\n",
    "stemWord = '/Users/lizhn7/Documents/Github/深度炼丹炉/COPA/CausalNet/PorterStemmer.txt'\n",
    "stemWord = open(stemWord).read().split()\n",
    "ING = '/Users/lizhn7/Documents/Github/深度炼丹炉/COPA/CausalNet/ING.txt'\n",
    "ING = open(ING).read().split()\n",
    "D = '/Users/lizhn7/Documents/Github/深度炼丹炉/COPA/CausalNet/D.txt'\n",
    "D = open(D).read().split()\n",
    "ED = '/Users/lizhn7/Documents/Github/深度炼丹炉/COPA/CausalNet/ED.txt'\n",
    "ED = open(ED).read().split()\n",
    "LY = '/Users/lizhn7/Documents/Github/深度炼丹炉/COPA/CausalNet/LY.txt'\n",
    "LY = open(LY).read().split()\n",
    "Y = '/Users/lizhn7/Documents/Github/深度炼丹炉/COPA/CausalNet/Y.txt'\n",
    "Y = open(Y).read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "replDict = {\"woman's\": 'woman', \"man's\": 'man', \"patient's\": 'patient', \"student's\": 'student', \"boy's\": 'boy', \n",
    "            \"friend's\": 'friend', \"enemy's\": 'enemy', \"parent's\": 'parent', \"humanitarian's\": 'humanitarian', \n",
    "            \"child's\": 'child', \"professor's\": 'professor', \"daughter's\": 'daughter', \"mother's\": 'mother', \n",
    "            \"children's\": 'children', \"teller's\": 'teller', \"company's\": 'company', \"group's\": 'group', \n",
    "            \"laptop's\": 'laptop', \"girl's\": 'girl', \"salesman's\": 'salesman', \"cook's\": 'cook', \"car's\": 'car', \n",
    "            \"offender's\": 'offender', \"detective's\": 'detective', \"librarian's\": 'librarian', \"caller's\": 'caller', \n",
    "            \"victim's\": 'victim', \"interviewer's\": 'interviewer', \"ship's\": 'ship', \"site's\": 'site', \n",
    "            \"chandelier's\": 'chandelier', \"bully's\": 'bully', \"river's\": 'river', \"puppy's\": 'puppy', \n",
    "            \"pilot's\": 'pilot', \"girlfriend's\": 'girlfriend', \"politician's\": 'politician', \"couple's\": 'couple', \n",
    "            \"son's\": 'son', \"actor's\": 'actor', \"neighbor's\": 'neighbor', \"nation's\": 'nation', \n",
    "            \"classmate's\": 'classmate', \"businessman's\": 'businessman', \"architect's\": 'architect', \n",
    "            \"imposter's\": 'imposter', \"kidnapper's\": 'kidnapper', \"colleague's\": 'colleague', \"flower's\": 'flower',\n",
    "            \"bull's\": 'bull', \"employee's\": 'employee', \"team's\": 'team', \"other's\": 'other', \n",
    "            \"writer's\": 'writer', \"baby's\": 'baby', \"attacker's\": 'attacker', \"uncle's\": 'uncle', \"driver's\": 'driver',\n",
    "            \"chuckling\": 'chuckle', \"drank\": 'drink', 'relied': 'rely', 'wore': 'wear', \"grew\": 'grow', \"slid\": 'slide',\n",
    "            \"worried\": 'worry', \"clumsily\": 'clumsy', \"heavily\": 'heavy', \"applied\": 'apply', \"rang\": 'ring', \"forgot\": 'forget',\n",
    "            \"shook\": 'shake', 'cried': 'cry', \"defied\": 'defy', \"incriminating\": 'incriminate', \"bitten\": 'bite', \n",
    "            \"blew\": 'blow', \"carried\": 'carry', \"told\": 'tell', \"sweaty\": 'sweat', \"buried\": 'bury', \"threw\": 'throw',\n",
    "            \"bought\": 'buy', \"woke\": 'wake', \"testified\": 'testify', \"froze\": 'freeze', \"outgrew\": 'outgrow', \"caught\": 'catch',\n",
    "            \"stood\": 'stand', \"preparing\": 'prepare', \"met\": 'meet', \"fought\": 'fight', \"faux\": 'fake',\n",
    "            \"spun\": 'spin', \"wrote\": 'write', \"easily\": 'easy', \"sped\": 'speed', \"leapt\": 'leap', \"taller\": 'tall',\n",
    "            \"underwent\": 'undergo', \"bled\": 'bleed', \"taught\": 'teach', \"spoke\": 'speak', \"stronger\": 'strong',\n",
    "            \"hung\": 'hang', \"brought\": 'bring', \"shrunk\": 'shrink', \"withheld\": 'withhold', \"re-elected\": 'reelect',\n",
    "            \"mimicked\": 'mimic', \"flew\": 'fly', \"interminably\": 'interminable', \"stolen\": 'steal', \"flung\": 'fling',\n",
    "            \"swung\": 'swing', \"awoke\": 'awake', \"aloud\": 'loud', \"receiving\": 'receive', \"withdrew\": 'withdraw', \n",
    "            \"forbade\": 'forbid', \"lagging\": 'lag', \"shaking\" : 'shake', \"lying\": 'lie', \"making\": 'make', 'diving': 'dive',\n",
    "            \"travelling\": 'travel', \"coming\": 'come', \"giving\": 'give', \"moving\": 'move', \"sobbing\": 'sob',\n",
    "            \"saving\": 'save', \"sitting\": 'sit', \"hiking\": 'hike', \"running\": 'run', \"convincing\": 'convince', \"getting\": 'get',\n",
    "            \"rising\": 'rise', \"sprung\": 'spring', \"slept\": 'sleep', \"fled\": 'flee', \"swam\": 'swim', \"commemorating\": 'commemorate',\n",
    "            \"separating\": 'separate', \"snuck\": 'suck', \"exercising\": 'exercise', \"clung\": 'cling', \"overslept\": 'oversleep',\n",
    "            \"googles\": 'google', \"rearranging\": 'rearrange', \"illegibly\": 'illegible', \"arose\": 'arise', \"meditating\": 'meditate',\n",
    "            \"re-election\": 'reelection', \"upheld\": 'uphold', \"eaten\": 'eat', \"rode\": 'ride', \"disputing\": 'dispute',\n",
    "            \"hallucinating\": 'hallucinate', \"forgave\": 'forgive', \"goggles\": 'goggle', \"uncontrollably\": 'uncontrollable',\n",
    "            \"arguing\": 'argue', \"smuggling\": 'smuggle', \"ran\": 'run', \"took\": 'take', \"sent\": 'send', \"things\": 'thing', \n",
    "            \"began\": 'begin', \"hid\": 'hide', \"gave\": 'give', \"said\": 'say', \"came\": 'come', \"uncovere\": 'uncover', \n",
    "            \"went\": 'go', \"saw\": 'see', \"seeing\": 'see', \"became\": 'become', \"knew\": 'know', \"towards\": 'toward',\n",
    "            \"coworker\": 'co-worker', \"youngest\": 'young', \"misunderstood\": 'misunderstand', \"bigger\": 'big'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_data(path, name):\n",
    "    \"\"\"\n",
    "    Load date from file\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file) as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        item = json.loads(line)\n",
    "        data.append(item[name])\n",
    "    return data\n",
    "\n",
    "def cut(s):\n",
    "    \"\"\"\n",
    "    Word segmentation\n",
    "    \"\"\"\n",
    "    pattern = r'''\n",
    "              (?x)                   # set flag to allow verbose regexps \n",
    "              (?:[A-Z]\\.)+           # abbreviations, e.g. U.S.A. \n",
    "              |\\w+(?:[-&']\\w+)*      # words w/ optional internal hyphens/apostrophe  \n",
    "            '''  \n",
    "    return regexp_tokenize(s, pattern=pattern)\n",
    "\n",
    "def clean(s):\n",
    "    \"\"\"\n",
    "    Clean words\n",
    "    \"\"\"\n",
    "    s = [replDict.get(i.lower(), i.lower()) for i in s]\n",
    "    s = [wnl.lemmatize(i) if i in lemmWord else i for i in s]\n",
    "    s = [porter.stem(i) if i in stemWord else i for i in s]\n",
    "    s = [i[:-3] if i in ING else i for i in s]\n",
    "    s = [i[:-1] if i in D else i for i in s]\n",
    "    s = [i[:-2] if i in ED else i for i in s]\n",
    "    s = [i[:-2] if i in LY else i for i in s]\n",
    "    s = [i[:-3]+'y' if i in Y else i for i in s]\n",
    "    for i in range(len(s)):\n",
    "        if s[i] == \"better-paying\":\n",
    "            s[i] = 'better'\n",
    "            s.insert(i+1, 'pay')\n",
    "        if s[i] == \"dry-cleaned\":\n",
    "            s[i] = 'dry'\n",
    "            s.insert(i+1, 'clean')\n",
    "        if s[i] == \"ex-girlfriend\":\n",
    "            s[i] = 'ex'\n",
    "            s.insert(i+1, 'girlfriend')\n",
    "        if s[i] == \"life-threatening\":\n",
    "            s[i] = 'life'\n",
    "            s.insert(i+1, 'threatening')\n",
    "        if s[i] == \"thank-you\":\n",
    "            s[i] = 'thank'\n",
    "            s.insert(i+1, 'you')\n",
    "        if s[i] == \"midlife\":\n",
    "            s[i] = 'mid'\n",
    "            s.insert(i+1, 'life')\n",
    "        if s[i] == \"handprint\":\n",
    "            s[i] = 'hand'\n",
    "            s.insert(i+1, 'print')\n",
    "        if s[i] == \"lifejacket\":\n",
    "            s[i] = 'life'\n",
    "            s.insert(i+1, 'jacket')\n",
    "        if s[i] == \"bathwater\":\n",
    "            s[i] = 'bath'\n",
    "            s.insert(i+1, 'water')\n",
    "        if s[i] == \"sweatpants\":\n",
    "            s[i] = 'sport'\n",
    "            s.insert(i+1, 'wear')\n",
    "    return s\n",
    "\n",
    "def del_stop(s):\n",
    "    \"\"\"\n",
    "    Delete stop words\n",
    "    \"\"\"\n",
    "    return [i for i in s if i not in stopWord]\n",
    "\n",
    "def del_stop_oov(s):\n",
    "    \"\"\"\n",
    "    Delete stop words and oov words\n",
    "    \"\"\"\n",
    "    s = [i for i in s if i not in oovWord]\n",
    "    return [i for i in s if i not in stopWord]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "premise = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-all.json', 'premise')\n",
    "asks_for = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-all.json', 'asks-for')\n",
    "alternative1 = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-all.json', 'alternative1')\n",
    "alternative2 = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-all.json', 'alternative2')\n",
    "\n",
    "rawLabel = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-all.json', 'most-plausible-alternative')\n",
    "rawLabel = [int(l) for l in rawLabel]\n",
    "\n",
    "cause = []\n",
    "effect = []\n",
    "for i in range(1000):\n",
    "    c = []\n",
    "    e = []\n",
    "    if asks_for[i] == 'cause':\n",
    "        c.append(alternative1[i])\n",
    "        c.append(alternative2[i])\n",
    "        e.append(premise[i])\n",
    "    else:\n",
    "        c.append(premise[i])\n",
    "        e.append(alternative1[i])\n",
    "        e.append(alternative2[i])\n",
    "    cause.append(c)\n",
    "    effect.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preWord = [del_stop(clean(cut(s))) for s in premise]\n",
    "a1Word = [del_stop(clean(cut(s))) for s in alternative1]\n",
    "a2Word = [del_stop(clean(cut(s))) for s in alternative2]\n",
    "cWord = [del_stop(clean(cut(s))) for s in sum(cause, [])]\n",
    "eWord = [del_stop(clean(cut(s))) for s in sum(effect, [])]\n",
    "allWord = cWord.copy()\n",
    "allWord.extend(eWord)\n",
    "allWords = set(sum(allWord, []))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import pickle\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62675002/62675002 [01:07<00:00, 935302.85it/s] \n"
     ]
    }
   ],
   "source": [
    "causalPair = []\n",
    "for i in tqdm(range(len(raw_text[:-1]))):\n",
    "    if raw_text[i][0] in allWords and raw_text[i][1] in allWords:\n",
    "        causalPair.append(raw_text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4046755"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(causalPair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: \t3000\n",
      "Distribution of sentence lengths (number of words):\n",
      "Min: 1   Max: 7   Mean: 2.897   Med: 3.000\n",
      "Found 2794 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tok_sentWords = allWord.copy()\n",
    "tokTexts = [' '.join(i) for i in tok_sentWords]\n",
    "tokenizer = Tokenizer(num_words=None,\n",
    "                      filters='',\n",
    "                      lower=True)\n",
    "tokenizer.fit_on_texts(tokTexts)\n",
    "word2index = tokenizer.word_index\n",
    "index2word = {i: w for w, i in word2index.items()}\n",
    "sentLens = np.array([len(i) for i in tok_sentWords])\n",
    "print('Number of sentences: \\t{:d}'.format(len(sentLens)))\n",
    "print('Distribution of sentence lengths (number of words):')\n",
    "print('Min: {:d}   Max: {:d}   Mean: {:.3f}   Med: {:.3f}'.format(np.min(sentLens), np.max(sentLens), np.mean(sentLens), np.median(sentLens)))\n",
    "print('Found %s unique tokens.' % len(word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4046755/4046755 [00:09<00:00, 426629.20it/s]\n"
     ]
    }
   ],
   "source": [
    "causalPair = [[word2index[s[i]] if i < 2 else s[i] for i in range(len(s))] for s in tqdm(causalPair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4046755/4046755 [00:06<00:00, 587787.71it/s]\n"
     ]
    }
   ],
   "source": [
    "fre = [int(i[-1]) for i in causalPair]\n",
    "allPair = []\n",
    "for i in tqdm(range(len(causalPair))):\n",
    "    allPair.extend([[causalPair[i][0], causalPair[i][1]]]*fre[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187662738"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allPair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('/Users/lizhn7/Downloads/EXPERIMENT/COPA/CausalNet/index.pkl', 'wb') as fp:\n",
    "    pickle.dump((word2index, index2word), fp, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Create Word Embeddings with GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "VOCAB_SIZE =2795\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2793-99.93% tokens in vocab found in glove and copied to embedding.\n"
     ]
    }
   ],
   "source": [
    "glove_n_symbols = 1917495\n",
    "glove_index_dict = {}\n",
    "glove_embedding_weights = np.empty((glove_n_symbols, EMBEDDING_DIM))\n",
    "globale_scale = 0.1\n",
    "with open('/Users/lizhn7/Downloads/DATA/glove/glove.42B.300d.txt', 'r') as fp:\n",
    "    index = 0\n",
    "    for l in fp:\n",
    "        l = l.strip().split()\n",
    "        word = l[0]\n",
    "        glove_index_dict[word] = index\n",
    "        glove_embedding_weights[index, :] = [float(n) for n in l[1:]]\n",
    "        index += 1\n",
    "glove_embedding_weights *= globale_scale\n",
    "\n",
    "# Generate random embedding with same scale as glove\n",
    "np.random.seed(SEED)\n",
    "shape = (VOCAB_SIZE, EMBEDDING_DIM)\n",
    "scale = glove_embedding_weights.std() * np.sqrt(12) / 2 \n",
    "glove_embedding = np.random.uniform(low=-scale, high=scale, size=shape)\n",
    "\n",
    "# Copy from glove weights of words that appear in index2word\n",
    "count = 0 \n",
    "for i in range(1, VOCAB_SIZE):\n",
    "    w = index2word[i]\n",
    "    g = glove_index_dict.get(w)\n",
    "    if g is None:\n",
    "        ww = wnl.lemmatize(w)\n",
    "        g = glove_index_dict.get(ww)\n",
    "    if g is None:\n",
    "        ww = porter.stem(w)\n",
    "        g = glove_index_dict.get(ww)\n",
    "    if g is not None:\n",
    "        glove_embedding[i, :] = glove_embedding_weights[g, :]\n",
    "        count += 1\n",
    "print('{num_tokens}-{per:.2f}% tokens in vocab found in glove and copied to embedding.'.format(num_tokens=count, per=count/float(VOCAB_SIZE)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "glove_embedding[word2index['unsupervise'], :] = glove_embedding_weights[glove_index_dict.get('unsupervised'), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fh = h5py.File('/Users/lizhn7/Downloads/EXPERIMENT/COPA/CausalNet/glove_embedding.h5', 'w')\n",
    "fh['glove_embedding'] = glove_embedding\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Predict Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('/Users/lizhn7/Downloads/EXPERIMENT/COPA/CausalNet/index.pkl', 'rb') as fp:\n",
    "    word2index, index2word = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = np.array([[s[0]] for s in allPair])\n",
    "y = np.array([[s[-1]] for s in allPair])\n",
    "\n",
    "xTrain, xVal, yTrain, yVal = train_test_split(x, y, test_size=0.15, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fh = h5py.File('/Users/lizhn7/Downloads/EXPERIMENT/COPA/CausalNet/Predict Effect/train.h5', 'w')\n",
    "fh['xTrain'] = xTrain\n",
    "fh['xVal'] = xVal\n",
    "fh['yTrain'] = yTrain\n",
    "fh['yVal'] = yVal\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Causal w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = [[s[0]] for s in allPair]\n",
    "y = [[s[-1]] for s in allPair]\n",
    "\n",
    "xTrain, _, yTrain, _ = train_test_split(np.array(x), np.array(y), test_size=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fh = h5py.File('/Users/lizhn7/Downloads/EXPERIMENT/COPA/CausalNet/word2vec/train.h5', 'w')\n",
    "fh['xTrain'] = xTrain\n",
    "fh['yTrain'] = yTrain\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data Preprocessing for Downstream Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import pickle\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 12\n",
    "SEED = 666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('/Users/lizhn7/Downloads/EXPERIMENT/COPA/CausalNet/index.pkl', 'rb') as fp:\n",
    "    word2index, index2word = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "devSent1 = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-dev-pro.json', 'whole-sentence1')\n",
    "devSent2 = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-dev-pro.json', 'whole-sentence2')\n",
    "\n",
    "rawLabel = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-dev-pro.json', 'most-plausible-alternative')\n",
    "rawLabel = [int(l) for l in rawLabel]\n",
    "\n",
    "devSent = devSent1.copy()\n",
    "devSent.extend(devSent2)\n",
    "\n",
    "devWord = [del_stop(clean(cut(s))) for s in devSent]\n",
    "devWord = [[word2index[i] for i in s] for s in devWord]\n",
    "\n",
    "l = [0] * 1000\n",
    "for i in range(len(rawLabel)):\n",
    "    if rawLabel[i] == 1:\n",
    "        l[i] = 1\n",
    "        l[i+len(rawLabel)] = -1\n",
    "    if rawLabel[i] == 2:\n",
    "        l[i] = -1\n",
    "        l[i+len(rawLabel)] = 1\n",
    "\n",
    "label = l\n",
    "\n",
    "x = pad_sequences(devWord, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "y = np.array(label)\n",
    "\n",
    "xTrain, _, yTrain, _ = train_test_split(x, y, test_size=0., random_state=SEED)\n",
    "\n",
    "fh = h5py.File('/Users/lizhn7/Documents/Github/深度炼丹炉/COPA/CausalNet/train.h5', 'w')\n",
    "fh['xTrain'] = xTrain\n",
    "fh['yTrain'] = yTrain\n",
    "fh.close()\n",
    "\n",
    "dev1Word = [del_stop(clean(cut(s))) for s in devSent1]\n",
    "dev2Word = [del_stop(clean(cut(s))) for s in devSent2]\n",
    "\n",
    "dev1Word = [[word2index[i] for i in s] for s in dev1Word]\n",
    "dev2Word = [[word2index[i] for i in s] for s in dev2Word]\n",
    "\n",
    "val1 = pad_sequences(dev1Word, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "val2 = pad_sequences(dev2Word, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "yVal = np.array(rawLabel)\n",
    "\n",
    "fh = h5py.File('/Users/lizhn7/Documents/Github/深度炼丹炉/COPA/CausalNet/val.h5', 'w')\n",
    "fh['val1'] = val1\n",
    "fh['val2'] = val2\n",
    "fh['yVal'] = yVal\n",
    "fh.close()\n",
    "\n",
    "testSent1 = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-test-pro.json', 'whole-sentence1')\n",
    "testSent2 = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-test-pro.json', 'whole-sentence2')\n",
    "\n",
    "testLabel = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-test-pro.json', 'most-plausible-alternative')\n",
    "testLabel = [int(l) for l in testLabel]\n",
    "\n",
    "test1Word = [del_stop(clean(cut(s))) for s in testSent1]\n",
    "test2Word = [del_stop(clean(cut(s))) for s in testSent2]\n",
    "\n",
    "test1Word = [[word2index[i] for i in s] for s in test1Word]\n",
    "test2Word = [[word2index[i] for i in s] for s in test2Word]\n",
    "\n",
    "test1 = pad_sequences(test1Word, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "test2 = pad_sequences(test2Word, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "yTest = np.array(testLabel)\n",
    "\n",
    "fh = h5py.File('/Users/lizhn7/Documents/Github/深度炼丹炉/COPA/CausalNet/test.h5', 'w')\n",
    "fh['test1'] = test1\n",
    "fh['test2'] = test2\n",
    "fh['yTest'] = yTest\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "premise = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-dev.json', 'premise')\n",
    "asks_for = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-dev.json', 'asks-for')\n",
    "alternative1 = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-dev.json', 'alternative1')\n",
    "alternative2 = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-dev.json', 'alternative2')\n",
    "\n",
    "rawLabel = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-dev.json', 'most-plausible-alternative')\n",
    "rawLabel = [int(l) for l in rawLabel]\n",
    "\n",
    "sent1a = []\n",
    "sent1b = []\n",
    "sent2a = []\n",
    "sent2b = []\n",
    "for i in range(500):\n",
    "    if asks_for[i] == 'cause':\n",
    "        sent1a.append(alternative1[i])\n",
    "        sent1b.append(premise[i])\n",
    "        sent2a.append(alternative2[i])\n",
    "        sent2b.append(premise[i])\n",
    "    else:\n",
    "        sent1a.append(premise[i])\n",
    "        sent1b.append(alternative1[i])\n",
    "        sent2a.append(premise[i])\n",
    "        sent2b.append(alternative2[i])\n",
    "\n",
    "s1aWord = [del_stop(clean(cut(s))) for s in sent1a]\n",
    "s1bWord = [del_stop(clean(cut(s))) for s in sent1b]\n",
    "s2aWord = [del_stop(clean(cut(s))) for s in sent2a]\n",
    "s2bWord = [del_stop(clean(cut(s))) for s in sent2b]\n",
    "\n",
    "s1aWord = [[word2index[i] for i in s] for s in s1aWord]\n",
    "s1bWord = [[word2index[i] for i in s] for s in s1bWord]\n",
    "s2aWord = [[word2index[i] for i in s] for s in s2aWord]\n",
    "s2bWord = [[word2index[i] for i in s] for s in s2bWord]\n",
    "\n",
    "dev1a = pad_sequences(s1aWord, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "dev1b = pad_sequences(s1bWord, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "dev2a = pad_sequences(s2aWord, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "dev2b = pad_sequences(s2bWord, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "yVal = np.array(rawLabel)\n",
    "\n",
    "fh = h5py.File('/Users/lizhn7/Documents/Github/深度炼丹炉/COPA/CausalNet/val_s.h5', 'w')\n",
    "fh['dev1a'] = dev1a\n",
    "fh['dev1b'] = dev1b\n",
    "fh['dev2a'] = dev2a\n",
    "fh['dev2b'] = dev2b\n",
    "fh['yVal'] = yVal\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "premise = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-test.json', 'premise')\n",
    "asks_for = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-test.json', 'asks-for')\n",
    "alternative1 = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-test.json', 'alternative1')\n",
    "alternative2 = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-test.json', 'alternative2')\n",
    "\n",
    "rawLabel = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-test.json', 'most-plausible-alternative')\n",
    "rawLabel = [int(l) for l in rawLabel]\n",
    "\n",
    "sent1a = []\n",
    "sent1b = []\n",
    "sent2a = []\n",
    "sent2b = []\n",
    "for i in range(500):\n",
    "    if asks_for[i] == 'cause':\n",
    "        sent1a.append(alternative1[i])\n",
    "        sent1b.append(premise[i])\n",
    "        sent2a.append(alternative2[i])\n",
    "        sent2b.append(premise[i])\n",
    "    else:\n",
    "        sent1a.append(premise[i])\n",
    "        sent1b.append(alternative1[i])\n",
    "        sent2a.append(premise[i])\n",
    "        sent2b.append(alternative2[i])\n",
    "\n",
    "s1aWord = [del_stop(clean(cut(s))) for s in sent1a]\n",
    "s1bWord = [del_stop(clean(cut(s))) for s in sent1b]\n",
    "s2aWord = [del_stop(clean(cut(s))) for s in sent2a]\n",
    "s2bWord = [del_stop(clean(cut(s))) for s in sent2b]\n",
    "\n",
    "s1aWord = [[word2index[i] for i in s] for s in s1aWord]\n",
    "s1bWord = [[word2index[i] for i in s] for s in s1bWord]\n",
    "s2aWord = [[word2index[i] for i in s] for s in s2aWord]\n",
    "s2bWord = [[word2index[i] for i in s] for s in s2bWord]\n",
    "\n",
    "dev1a = pad_sequences(s1aWord, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "dev1b = pad_sequences(s1bWord, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "dev2a = pad_sequences(s2aWord, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "dev2b = pad_sequences(s2bWord, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "yVal = np.array(rawLabel)\n",
    "\n",
    "fh = h5py.File('/Users/lizhn7/Documents/Github/深度炼丹炉/COPA/CausalNet/test_s.h5', 'w')\n",
    "fh['test1a'] = dev1a\n",
    "fh['test1b'] = dev1b\n",
    "fh['test2a'] = dev2a\n",
    "fh['test2b'] = dev2b\n",
    "fh['yTest'] = yVal\n",
    "fh.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
