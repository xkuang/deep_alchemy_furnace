{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#CausalNet-Preprocess\" data-toc-modified-id=\"CausalNet-Preprocess-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>CausalNet Preprocess</a></div><div class=\"lev1 toc-item\"><a href=\"#COPA-Preprocess\" data-toc-modified-id=\"COPA-Preprocess-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>COPA Preprocess</a></div><div class=\"lev1 toc-item\"><a href=\"#Build-Dataset\" data-toc-modified-id=\"Build-Dataset-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Build Dataset</a></div><div class=\"lev2 toc-item\"><a href=\"#Tokenize-Text\" data-toc-modified-id=\"Tokenize-Text-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Tokenize Text</a></div><div class=\"lev2 toc-item\"><a href=\"#Create-Word-Embeddings-with-GloVe\" data-toc-modified-id=\"Create-Word-Embeddings-with-GloVe-32\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Create Word Embeddings with GloVe</a></div><div class=\"lev2 toc-item\"><a href=\"#Predict-Effect\" data-toc-modified-id=\"Predict-Effect-33\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Predict Effect</a></div><div class=\"lev2 toc-item\"><a href=\"#Causal-w2v\" data-toc-modified-id=\"Causal-w2v-34\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Causal w2v</a></div><div class=\"lev1 toc-item\"><a href=\"#Data-Preprocessing-for-Downstream-Tasks\" data-toc-modified-id=\"Data-Preprocessing-for-Downstream-Tasks-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Data Preprocessing for Downstream Tasks</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# CausalNet Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "filename = '/Users/lizhn7/Downloads/EXPERIMENT/COPA/CausalNet/CausalNet.txt'\n",
    "raw_text = [i.split() for i in open(filename).read().split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62675003"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62675002/62675002 [00:54<00:00, 1140731.06it/s]\n"
     ]
    }
   ],
   "source": [
    "causeWord = []\n",
    "for i in tqdm(range(len(raw_text[:-1]))):\n",
    "    causeWord.append(raw_text[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "causeWord = set(causeWordWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59411"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(causeWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62675002/62675002 [00:53<00:00, 1165438.31it/s]\n"
     ]
    }
   ],
   "source": [
    "effectWord = []\n",
    "for i in tqdm(range(len(raw_text[:-1]))):\n",
    "    effectWord.append(raw_text[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "effectWord = set(effectWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59710"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(effectWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COPA Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from nltk import regexp_tokenize\n",
    "from nltk import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopWord = '/Users/lizhn7/Documents/Github/深度炼丹炉/COPA/CausalNet/stopwords.txt'\n",
    "stopWord = open(stopWord).read().split()\n",
    "lemmWord = '/Users/lizhn7/Documents/Github/深度炼丹炉/COPA/CausalNet/WordNetLemmatizer.txt'\n",
    "lemmWord = open(lemmWord).read().split()\n",
    "stemWord = '/Users/lizhn7/Documents/Github/深度炼丹炉/COPA/CausalNet/PorterStemmer.txt'\n",
    "stemWord = open(stemWord).read().split()\n",
    "ING = '/Users/lizhn7/Documents/Github/深度炼丹炉/COPA/CausalNet/ING.txt'\n",
    "ING = open(ING).read().split()\n",
    "D = '/Users/lizhn7/Documents/Github/深度炼丹炉/COPA/CausalNet/D.txt'\n",
    "D = open(D).read().split()\n",
    "ED = '/Users/lizhn7/Documents/Github/深度炼丹炉/COPA/CausalNet/ED.txt'\n",
    "ED = open(ED).read().split()\n",
    "LY = '/Users/lizhn7/Documents/Github/深度炼丹炉/COPA/CausalNet/LY.txt'\n",
    "LY = open(LY).read().split()\n",
    "Y = '/Users/lizhn7/Documents/Github/深度炼丹炉/COPA/CausalNet/Y.txt'\n",
    "Y = open(Y).read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replDict = {\"woman's\": 'woman', \"man's\": 'man', \"patient's\": 'patient', \"student's\": 'student', \"boy's\": 'boy', \n",
    "            \"friend's\": 'friend', \"enemy's\": 'enemy', \"parent's\": 'parent', \"humanitarian's\": 'humanitarian', \n",
    "            \"child's\": 'child', \"professor's\": 'professor', \"daughter's\": 'daughter', \"mother's\": 'mother', \n",
    "            \"children's\": 'children', \"teller's\": 'teller', \"company's\": 'company', \"group's\": 'group', \n",
    "            \"laptop's\": 'laptop', \"girl's\": 'girl', \"salesman's\": 'salesman', \"cook's\": 'cook', \"car's\": 'car', \n",
    "            \"offender's\": 'offender', \"detective's\": 'detective', \"librarian's\": 'librarian', \"caller's\": 'caller', \n",
    "            \"victim's\": 'victim', \"interviewer's\": 'interviewer', \"ship's\": 'ship', \"site's\": 'site', \n",
    "            \"chandelier's\": 'chandelier', \"bully's\": 'bully', \"river's\": 'river', \"puppy's\": 'puppy', \n",
    "            \"pilot's\": 'pilot', \"girlfriend's\": 'girlfriend', \"politician's\": 'politician', \"couple's\": 'couple', \n",
    "            \"son's\": 'son', \"actor's\": 'actor', \"neighbor's\": 'neighbor', \"nation's\": 'nation', \n",
    "            \"classmate's\": 'classmate', \"businessman's\": 'businessman', \"architect's\": 'architect', \n",
    "            \"imposter's\": 'imposter', \"kidnapper's\": 'kidnapper', \"colleague's\": 'colleague', \"flower's\": 'flower',\n",
    "            \"bull's\": 'bull', \"employee's\": 'employee', \"team's\": 'team', \"other's\": 'other', \n",
    "            \"writer's\": 'writer', \"baby's\": 'baby', \"attacker's\": 'attacker', \"uncle's\": 'uncle', \"driver's\": 'driver',\n",
    "            \"chuckling\": 'chuckle', \"drank\": 'drink', 'relied': 'rely', 'wore': 'wear', \"grew\": 'grow', \"slid\": 'slide',\n",
    "            \"worried\": 'worry', \"clumsily\": 'clumsy', \"heavily\": 'heavy', \"applied\": 'apply', \"rang\": 'ring', \"forgot\": 'forget',\n",
    "            \"shook\": 'shake', 'cried': 'cry', \"defied\": 'defy', \"incriminating\": 'incriminate', \"bitten\": 'bite', \n",
    "            \"blew\": 'blow', \"carried\": 'carry', \"told\": 'tell', \"sweaty\": 'sweat', \"buried\": 'bury', \"threw\": 'throw',\n",
    "            \"bought\": 'buy', \"woke\": 'wake', \"testified\": 'testify', \"froze\": 'freeze', \"outgrew\": 'outgrow', \"caught\": 'catch',\n",
    "            \"stood\": 'stand', \"preparing\": 'prepare', \"met\": 'meet', \"fought\": 'fight', \"faux\": 'fake',\n",
    "            \"spun\": 'spin', \"wrote\": 'write', \"easily\": 'easy', \"sped\": 'speed', \"leapt\": 'leap', \"taller\": 'tall',\n",
    "            \"underwent\": 'undergo', \"bled\": 'bleed', \"taught\": 'teach', \"spoke\": 'speak', \"stronger\": 'strong',\n",
    "            \"hung\": 'hang', \"brought\": 'bring', \"shrunk\": 'shrink', \"withheld\": 'withhold', \"re-elected\": 'reelect',\n",
    "            \"mimicked\": 'mimic', \"flew\": 'fly', \"interminably\": 'interminable', \"stolen\": 'steal', \"flung\": 'fling',\n",
    "            \"swung\": 'swing', \"awoke\": 'awake', \"aloud\": 'loud', \"receiving\": 'receive', \"withdrew\": 'withdraw', \n",
    "            \"forbade\": 'forbid', \"lagging\": 'lag', \"shaking\" : 'shake', \"lying\": 'lie', \"making\": 'make', 'diving': 'dive',\n",
    "            \"travelling\": 'travel', \"coming\": 'come', \"giving\": 'give', \"moving\": 'move', \"sobbing\": 'sob',\n",
    "            \"saving\": 'save', \"sitting\": 'sit', \"hiking\": 'hike', \"running\": 'run', \"convincing\": 'convince', \"getting\": 'get',\n",
    "            \"rising\": 'rise', \"sprung\": 'spring', \"slept\": 'sleep', \"fled\": 'flee', \"swam\": 'swim', \"commemorating\": 'commemorate',\n",
    "            \"separating\": 'separate', \"snuck\": 'suck', \"exercising\": 'exercise', \"clung\": 'cling', \"overslept\": 'oversleep',\n",
    "            \"googles\": 'google', \"rearranging\": 'rearrange', \"illegibly\": 'illegible', \"arose\": 'arise', \"meditating\": 'meditate',\n",
    "            \"re-election\": 'reelection', \"upheld\": 'uphold', \"eaten\": 'eat', \"rode\": 'ride', \"disputing\": 'dispute',\n",
    "            \"hallucinating\": 'hallucinate', \"forgave\": 'forgive', \"goggles\": 'goggle', \"uncontrollably\": 'uncontrollable',\n",
    "            \"arguing\": 'argue', \"smuggling\": 'smuggle', \"ran\": 'run', \"took\": 'take', \"sent\": 'send', \"things\": 'thing', \n",
    "            \"began\": 'begin', \"hid\": 'hide', \"gave\": 'give', \"said\": 'say', \"came\": 'come', \"uncovere\": 'uncover', \n",
    "            \"went\": 'go', \"saw\": 'see', \"seeing\": 'see', \"became\": 'become', \"knew\": 'know', \"towards\": 'toward',\n",
    "            \"coworker\": 'co-worker', \"youngest\": 'young', \"misunderstood\": 'misunderstand', \"bigger\": 'big'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(path, name):\n",
    "    \"\"\"\n",
    "    Load date from file\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file) as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        item = json.loads(line)\n",
    "        data.append(item[name])\n",
    "    return data\n",
    "\n",
    "def cut(s):\n",
    "    \"\"\"\n",
    "    Word segmentation\n",
    "    \"\"\"\n",
    "    pattern = r'''\n",
    "              (?x)                   # set flag to allow verbose regexps \n",
    "              (?:[A-Z]\\.)+           # abbreviations, e.g. U.S.A. \n",
    "              |\\w+(?:[-&']\\w+)*      # words w/ optional internal hyphens/apostrophe  \n",
    "            '''  \n",
    "    return regexp_tokenize(s, pattern=pattern)\n",
    "\n",
    "def clean(s):\n",
    "    \"\"\"\n",
    "    Clean words\n",
    "    \"\"\"\n",
    "    s = [replDict.get(i.lower(), i.lower()) for i in s]\n",
    "    s = [wnl.lemmatize(i) if i in lemmWord else i for i in s]\n",
    "    s = [porter.stem(i) if i in stemWord else i for i in s]\n",
    "    s = [i[:-3] if i in ING else i for i in s]\n",
    "    s = [i[:-1] if i in D else i for i in s]\n",
    "    s = [i[:-2] if i in ED else i for i in s]\n",
    "    s = [i[:-2] if i in LY else i for i in s]\n",
    "    s = [i[:-3]+'y' if i in Y else i for i in s]\n",
    "    for i in range(len(s)):\n",
    "        if s[i] == \"better-paying\":\n",
    "            s[i] = 'better'\n",
    "            s.insert(i+1, 'pay')\n",
    "        if s[i] == \"dry-cleaned\":\n",
    "            s[i] = 'dry'\n",
    "            s.insert(i+1, 'clean')\n",
    "        if s[i] == \"ex-girlfriend\":\n",
    "            s[i] = 'ex'\n",
    "            s.insert(i+1, 'girlfriend')\n",
    "        if s[i] == \"life-threatening\":\n",
    "            s[i] = 'life'\n",
    "            s.insert(i+1, 'threatening')\n",
    "        if s[i] == \"thank-you\":\n",
    "            s[i] = 'thank'\n",
    "            s.insert(i+1, 'you')\n",
    "        if s[i] == \"midlife\":\n",
    "            s[i] = 'mid'\n",
    "            s.insert(i+1, 'life')\n",
    "        if s[i] == \"handprint\":\n",
    "            s[i] = 'hand'\n",
    "            s.insert(i+1, 'print')\n",
    "        if s[i] == \"lifejacket\":\n",
    "            s[i] = 'life'\n",
    "            s.insert(i+1, 'jacket')\n",
    "        if s[i] == \"bathwater\":\n",
    "            s[i] = 'bath'\n",
    "            s.insert(i+1, 'water')\n",
    "        if s[i] == \"sweatpants\":\n",
    "            s[i] = 'sport'\n",
    "            s.insert(i+1, 'wear')\n",
    "    return s\n",
    "\n",
    "def del_stop(s):\n",
    "    \"\"\"\n",
    "    Delete stop words\n",
    "    \"\"\"\n",
    "    return [i for i in s if i not in stopWord]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "premise = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-all.json', 'premise')\n",
    "asks_for = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-all.json', 'asks-for')\n",
    "alternative1 = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-all.json', 'alternative1')\n",
    "alternative2 = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-all.json', 'alternative2')\n",
    "\n",
    "rawLabel = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-all.json', 'most-plausible-alternative')\n",
    "rawLabel = [int(l) for l in rawLabel]\n",
    "\n",
    "cause = []\n",
    "effect = []\n",
    "for i in range(1000):\n",
    "    c = []\n",
    "    e = []\n",
    "    if asks_for[i] == 'cause':\n",
    "        c.append(alternative1[i])\n",
    "        c.append(alternative2[i])\n",
    "        e.append(premise[i])\n",
    "    else:\n",
    "        c.append(premise[i])\n",
    "        e.append(alternative1[i])\n",
    "        e.append(alternative2[i])\n",
    "    cause.append(c)\n",
    "    effect.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preWord = [del_stop(clean(cut(s))) for s in premise]\n",
    "a1Word = [del_stop(clean(cut(s))) for s in alternative1]\n",
    "a2Word = [del_stop(clean(cut(s))) for s in alternative2]\n",
    "cWord = [del_stop(clean(cut(s))) for s in sum(cause, [])]\n",
    "eWord = [del_stop(clean(cut(s))) for s in sum(effect, [])]\n",
    "allWord = cWord.copy()\n",
    "allWord.extend(eWord)\n",
    "allWords = set(sum(allWord, []))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import pickle\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62675002/62675002 [01:07<00:00, 935302.85it/s] \n"
     ]
    }
   ],
   "source": [
    "causalPair = []\n",
    "for i in tqdm(range(len(raw_text[:-1]))):\n",
    "    if raw_text[i][0] in allWords and raw_text[i][1] in allWords:\n",
    "        causalPair.append(raw_text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4046755"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(causalPair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: \t3000\n",
      "Distribution of sentence lengths (number of words):\n",
      "Min: 1   Max: 7   Mean: 2.897   Med: 3.000\n",
      "Found 2794 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tok_sentWords = allWord.copy()\n",
    "tokTexts = [' '.join(i) for i in tok_sentWords]\n",
    "tokenizer = Tokenizer(num_words=None,\n",
    "                      filters='',\n",
    "                      lower=True)\n",
    "tokenizer.fit_on_texts(tokTexts)\n",
    "word2index = tokenizer.word_index\n",
    "index2word = {i: w for w, i in word2index.items()}\n",
    "sentLens = np.array([len(i) for i in tok_sentWords])\n",
    "print('Number of sentences: \\t{:d}'.format(len(sentLens)))\n",
    "print('Distribution of sentence lengths (number of words):')\n",
    "print('Min: {:d}   Max: {:d}   Mean: {:.3f}   Med: {:.3f}'.format(np.min(sentLens), np.max(sentLens), np.mean(sentLens), np.median(sentLens)))\n",
    "print('Found %s unique tokens.' % len(word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4046755/4046755 [00:09<00:00, 426629.20it/s]\n"
     ]
    }
   ],
   "source": [
    "causalPair = [[word2index[s[i]] if i < 2 else s[i] for i in range(len(s))] for s in tqdm(causalPair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4046755/4046755 [00:06<00:00, 587787.71it/s]\n"
     ]
    }
   ],
   "source": [
    "fre = [int(i[-1]) for i in causalPair]\n",
    "allPair = []\n",
    "for i in tqdm(range(len(causalPair))):\n",
    "    allPair.extend([[causalPair[i][0], causalPair[i][1]]]*fre[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187662738"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allPair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/Users/lizhn7/Downloads/EXPERIMENT/COPA/CausalNet/index.pkl', 'wb') as fp:\n",
    "    pickle.dump((word2index, index2word), fp, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Word Embeddings with GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "VOCAB_SIZE =2795\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2793-99.93% tokens in vocab found in glove and copied to embedding.\n"
     ]
    }
   ],
   "source": [
    "glove_n_symbols = 1917495\n",
    "glove_index_dict = {}\n",
    "glove_embedding_weights = np.empty((glove_n_symbols, EMBEDDING_DIM))\n",
    "globale_scale = 0.1\n",
    "with open('/Users/lizhn7/Downloads/DATA/glove/glove.42B.300d.txt', 'r') as fp:\n",
    "    index = 0\n",
    "    for l in fp:\n",
    "        l = l.strip().split()\n",
    "        word = l[0]\n",
    "        glove_index_dict[word] = index\n",
    "        glove_embedding_weights[index, :] = [float(n) for n in l[1:]]\n",
    "        index += 1\n",
    "glove_embedding_weights *= globale_scale\n",
    "\n",
    "# Generate random embedding with same scale as glove\n",
    "np.random.seed(SEED)\n",
    "shape = (VOCAB_SIZE, EMBEDDING_DIM)\n",
    "scale = glove_embedding_weights.std() * np.sqrt(12) / 2 \n",
    "glove_embedding = np.random.uniform(low=-scale, high=scale, size=shape)\n",
    "\n",
    "# Copy from glove weights of words that appear in index2word\n",
    "count = 0 \n",
    "for i in range(1, VOCAB_SIZE):\n",
    "    w = index2word[i]\n",
    "    g = glove_index_dict.get(w)\n",
    "    if g is None:\n",
    "        ww = wnl.lemmatize(w)\n",
    "        g = glove_index_dict.get(ww)\n",
    "    if g is None:\n",
    "        ww = porter.stem(w)\n",
    "        g = glove_index_dict.get(ww)\n",
    "    if g is not None:\n",
    "        glove_embedding[i, :] = glove_embedding_weights[g, :]\n",
    "        count += 1\n",
    "print('{num_tokens}-{per:.2f}% tokens in vocab found in glove and copied to embedding.'.format(num_tokens=count, per=count/float(VOCAB_SIZE)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_embedding[word2index['unsupervise'], :] = glove_embedding_weights[glove_index_dict.get('unsupervised'), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fh = h5py.File('/Users/lizhn7/Downloads/EXPERIMENT/COPA/CausalNet/glove_embedding.h5', 'w')\n",
    "fh['glove_embedding'] = glove_embedding\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/Users/lizhn7/Downloads/EXPERIMENT/COPA/CausalNet/index.pkl', 'rb') as fp:\n",
    "    word2index, index2word = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([[s[0]] for s in allPair])\n",
    "y = np.array([[s[-1]] for s in allPair])\n",
    "\n",
    "xTrain, xVal, yTrain, yVal = train_test_split(x, y, test_size=0.15, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fh = h5py.File('/Users/lizhn7/Downloads/EXPERIMENT/COPA/CausalNet/Predict Effect/train.h5', 'w')\n",
    "fh['xTrain'] = xTrain\n",
    "fh['xVal'] = xVal\n",
    "fh['yTrain'] = yTrain\n",
    "fh['yVal'] = yVal\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [[s[0]] for s in allPair]\n",
    "y = [[s[-1]] for s in allPair]\n",
    "\n",
    "xTrain, _, yTrain, _ = train_test_split(np.array(x), np.array(y), test_size=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fh = h5py.File('/Users/lizhn7/Downloads/EXPERIMENT/COPA/CausalNet/word2vec/train.h5', 'w')\n",
    "fh['xTrain'] = xTrain\n",
    "fh['yTrain'] = yTrain\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Downstream Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import pickle\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 12\n",
    "SEED = 666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/Users/lizhn7/Downloads/EXPERIMENT/COPA/CausalNet/index.pkl', 'rb') as fp:\n",
    "    word2index, index2word = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "devSent1 = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-dev-pro.json', 'whole-sentence1')\n",
    "devSent2 = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-dev-pro.json', 'whole-sentence2')\n",
    "\n",
    "rawLabel = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-dev-pro.json', 'most-plausible-alternative')\n",
    "rawLabel = [int(l) for l in rawLabel]\n",
    "\n",
    "devSent = devSent1.copy()\n",
    "devSent.extend(devSent2)\n",
    "\n",
    "devWord = [del_stop(clean(cut(s))) for s in devSent]\n",
    "devWord = [[word2index[i] for i in s] for s in devWord]\n",
    "\n",
    "l = [0] * 1000\n",
    "for i in range(len(rawLabel)):\n",
    "    if rawLabel[i] == 1:\n",
    "        l[i] = 1\n",
    "        l[i+len(rawLabel)] = -1\n",
    "    if rawLabel[i] == 2:\n",
    "        l[i] = -1\n",
    "        l[i+len(rawLabel)] = 1\n",
    "\n",
    "label = l\n",
    "\n",
    "x = pad_sequences(devWord, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "y = np.array(label)\n",
    "\n",
    "xTrain, _, yTrain, _ = train_test_split(x, y, test_size=0., random_state=SEED)\n",
    "\n",
    "fh = h5py.File('/Users/lizhn7/Documents/Github/深度炼丹炉/COPA/CausalNet/train.h5', 'w')\n",
    "fh['xTrain'] = xTrain\n",
    "fh['yTrain'] = yTrain\n",
    "fh.close()\n",
    "\n",
    "dev1Word = [del_stop(clean(cut(s))) for s in devSent1]\n",
    "dev2Word = [del_stop(clean(cut(s))) for s in devSent2]\n",
    "\n",
    "dev1Word = [[word2index[i] for i in s] for s in dev1Word]\n",
    "dev2Word = [[word2index[i] for i in s] for s in dev2Word]\n",
    "\n",
    "val1 = pad_sequences(dev1Word, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "val2 = pad_sequences(dev2Word, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "yVal = np.array(rawLabel)\n",
    "\n",
    "fh = h5py.File('/Users/lizhn7/Documents/Github/深度炼丹炉/COPA/CausalNet/val.h5', 'w')\n",
    "fh['val1'] = val1\n",
    "fh['val2'] = val2\n",
    "fh['yVal'] = yVal\n",
    "fh.close()\n",
    "\n",
    "testSent1 = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-test-pro.json', 'whole-sentence1')\n",
    "testSent2 = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-test-pro.json', 'whole-sentence2')\n",
    "\n",
    "testLabel = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-test-pro.json', 'most-plausible-alternative')\n",
    "testLabel = [int(l) for l in testLabel]\n",
    "\n",
    "test1Word = [del_stop(clean(cut(s))) for s in testSent1]\n",
    "test2Word = [del_stop(clean(cut(s))) for s in testSent2]\n",
    "\n",
    "test1Word = [[word2index[i] for i in s] for s in test1Word]\n",
    "test2Word = [[word2index[i] for i in s] for s in test2Word]\n",
    "\n",
    "test1 = pad_sequences(test1Word, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "test2 = pad_sequences(test2Word, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "yTest = np.array(testLabel)\n",
    "\n",
    "fh = h5py.File('/Users/lizhn7/Documents/Github/深度炼丹炉/COPA/CausalNet/test.h5', 'w')\n",
    "fh['test1'] = test1\n",
    "fh['test2'] = test2\n",
    "fh['yTest'] = yTest\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "premise = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-dev.json', 'premise')\n",
    "asks_for = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-dev.json', 'asks-for')\n",
    "alternative1 = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-dev.json', 'alternative1')\n",
    "alternative2 = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-dev.json', 'alternative2')\n",
    "\n",
    "rawLabel = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-dev.json', 'most-plausible-alternative')\n",
    "rawLabel = [int(l) for l in rawLabel]\n",
    "\n",
    "sent1a = []\n",
    "sent1b = []\n",
    "sent2a = []\n",
    "sent2b = []\n",
    "for i in range(500):\n",
    "    if asks_for[i] == 'cause':\n",
    "        sent1a.append(alternative1[i])\n",
    "        sent1b.append(premise[i])\n",
    "        sent2a.append(alternative2[i])\n",
    "        sent2b.append(premise[i])\n",
    "    else:\n",
    "        sent1a.append(premise[i])\n",
    "        sent1b.append(alternative1[i])\n",
    "        sent2a.append(premise[i])\n",
    "        sent2b.append(alternative2[i])\n",
    "\n",
    "s1aWord = [del_stop(clean(cut(s))) for s in sent1a]\n",
    "s1bWord = [del_stop(clean(cut(s))) for s in sent1b]\n",
    "s2aWord = [del_stop(clean(cut(s))) for s in sent2a]\n",
    "s2bWord = [del_stop(clean(cut(s))) for s in sent2b]\n",
    "\n",
    "s1aWord = [[word2index[i] for i in s] for s in s1aWord]\n",
    "s1bWord = [[word2index[i] for i in s] for s in s1bWord]\n",
    "s2aWord = [[word2index[i] for i in s] for s in s2aWord]\n",
    "s2bWord = [[word2index[i] for i in s] for s in s2bWord]\n",
    "\n",
    "dev1a = pad_sequences(s1aWord, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "dev1b = pad_sequences(s1bWord, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "dev2a = pad_sequences(s2aWord, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "dev2b = pad_sequences(s2bWord, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "yVal = np.array(rawLabel)\n",
    "\n",
    "fh = h5py.File('/Users/lizhn7/Documents/Github/深度炼丹炉/COPA/CausalNet/val_s.h5', 'w')\n",
    "fh['dev1a'] = dev1a\n",
    "fh['dev1b'] = dev1b\n",
    "fh['dev2a'] = dev2a\n",
    "fh['dev2b'] = dev2b\n",
    "fh['yVal'] = yVal\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "premise = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-test.json', 'premise')\n",
    "asks_for = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-test.json', 'asks-for')\n",
    "alternative1 = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-test.json', 'alternative1')\n",
    "alternative2 = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-test.json', 'alternative2')\n",
    "\n",
    "rawLabel = load_data('/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-test.json', 'most-plausible-alternative')\n",
    "rawLabel = [int(l) for l in rawLabel]\n",
    "\n",
    "sent1a = []\n",
    "sent1b = []\n",
    "sent2a = []\n",
    "sent2b = []\n",
    "for i in range(500):\n",
    "    if asks_for[i] == 'cause':\n",
    "        sent1a.append(alternative1[i])\n",
    "        sent1b.append(premise[i])\n",
    "        sent2a.append(alternative2[i])\n",
    "        sent2b.append(premise[i])\n",
    "    else:\n",
    "        sent1a.append(premise[i])\n",
    "        sent1b.append(alternative1[i])\n",
    "        sent2a.append(premise[i])\n",
    "        sent2b.append(alternative2[i])\n",
    "\n",
    "s1aWord = [del_stop(clean(cut(s))) for s in sent1a]\n",
    "s1bWord = [del_stop(clean(cut(s))) for s in sent1b]\n",
    "s2aWord = [del_stop(clean(cut(s))) for s in sent2a]\n",
    "s2bWord = [del_stop(clean(cut(s))) for s in sent2b]\n",
    "\n",
    "s1aWord = [[word2index[i] for i in s] for s in s1aWord]\n",
    "s1bWord = [[word2index[i] for i in s] for s in s1bWord]\n",
    "s2aWord = [[word2index[i] for i in s] for s in s2aWord]\n",
    "s2bWord = [[word2index[i] for i in s] for s in s2bWord]\n",
    "\n",
    "dev1a = pad_sequences(s1aWord, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "dev1b = pad_sequences(s1bWord, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "dev2a = pad_sequences(s2aWord, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "dev2b = pad_sequences(s2bWord, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "yVal = np.array(rawLabel)\n",
    "\n",
    "fh = h5py.File('/Users/lizhn7/Documents/Github/深度炼丹炉/COPA/CausalNet/test_s.h5', 'w')\n",
    "fh['test1a'] = dev1a\n",
    "fh['test1b'] = dev1b\n",
    "fh['test2a'] = dev2a\n",
    "fh['test2b'] = dev2b\n",
    "fh['yTest'] = yVal\n",
    "fh.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
