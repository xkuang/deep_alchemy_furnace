{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "from spacy.parts_of_speech import NOUN, VERB, ADJ, ADV, NUM\n",
    "from nltk import regexp_tokenize\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "stopWords = {'e1': None, 'e2': None, 'ding': None, 'd': None, 'j': None, 'r': None, 't': None}\n",
    "indicator = ['lead', 'leading', 'because', 'thus', 'therefore', 'consequence', 'due', 'result', 'hence',\n",
    "             'cause', 'induce', 'inducing', 'causing', 'reason', 'effect']\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Load date from file\n",
    "    \"\"\"\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "    \n",
    "def isNoise(token):\n",
    "    \"\"\"\n",
    "    Check if the token is a noise or not \n",
    "    \"\"\"\n",
    "    is_noise = False\n",
    "    pos_tags = []\n",
    "    if token.pos not in [NOUN, VERB, ADJ, ADV, NUM]:\n",
    "        is_noise = True\n",
    "    elif token.is_stop == True:\n",
    "        is_noise = True\n",
    "    return is_noise\n",
    "\n",
    "def clean(token):\n",
    "    \"\"\"\n",
    "    Clean data\n",
    "    \"\"\"\n",
    "    return token.lemma_\n",
    "\n",
    "def cut(s):\n",
    "    \"\"\"\n",
    "    Word segmentation\n",
    "    \"\"\"\n",
    "    pattern = r'''\n",
    "              (?x)                   # set flag to allow verbose regexps \n",
    "              (?:[A-Z]\\.)+           # abbreviations, e.g. U.S.A. \n",
    "              |\\d+(?:\\.\\d+)?%?       # numbers, incl. currency and percentages \n",
    "              |\\w+(?:[-&']\\w+)*      # words w/ optional internal hyphens/apostrophe \n",
    "           '''  \n",
    "    return regexp_tokenize(s, pattern=pattern)\n",
    "\n",
    "def find_pn(ws):\n",
    "    \"\"\"\n",
    "    Find paired nominals\n",
    "    \"\"\"\n",
    "    for i in range(len(ws)):\n",
    "        if ws[i] == 'e1':\n",
    "            for j in range(i+1, len(ws)):\n",
    "                if ws[j] == 'e1':\n",
    "                    pn1 = ws[i+1:j] \n",
    "        if ws[i] == 'e2':\n",
    "            for j in range(i+1, len(ws)):\n",
    "                if ws[j] == 'e2':\n",
    "                    pn2 = ws[i+1:j]\n",
    "    return pn1, pn2\n",
    "\n",
    "def del_stop(ws):\n",
    "    \"\"\"\n",
    "    Delete stopwords\n",
    "    \"\"\"\n",
    "    return [i for i in [stopWords.get(i.lower(), i) for i in ws] if i != None]\n",
    "\n",
    "def del_indicator(ws):\n",
    "    \"\"\"\n",
    "    Delete causal indicators\n",
    "    \"\"\"\n",
    "    return [i for i in ws if i not in indicator]\n",
    "\n",
    "def write_to_file(content, name):\n",
    "    \"\"\"\n",
    "    Write data to json file\n",
    "    \"\"\"\n",
    "    with open(name, 'a', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(content, ensure_ascii=False) + '\\n')\n",
    "        f.close()\n",
    "        \n",
    "def gen_data(data, causal=True):\n",
    "    \"\"\"\n",
    "    Data generator\n",
    "    \"\"\"\n",
    "    for i in range(len(data)):\n",
    "        if causal:\n",
    "            yield {\n",
    "                'sentence': data[i],\n",
    "                'label': 1\n",
    "            }\n",
    "        else:\n",
    "            yield {\n",
    "                'sentence': data[i],\n",
    "                'label': 0\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/lizhn7/Downloads/EXPERIMENT/COPA/FINAL/semeval.txt'\n",
    "data = load_data(data_path).strip().split('\\t')[1:]\n",
    "data = [line.split('\\n')[:2] for line in data]\n",
    "sentence = [line[0] for line in data]\n",
    "label = [1 if line[-1] == 'Cause-Effect(e2,e1)' or line[-1] == 'Cause-Effect(e1,e2)' else 0 for line in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentWords = [cut(s) for s in sentence]\n",
    "sentWords = [del_stop(ws) for ws in sentWords]\n",
    "causalSent = [' '.join(sentWords[i]) for i in range(len(sentence)) if label[i] == 1]\n",
    "noncausalSent = [' '.join(sentWords[i]) for i in range(len(sentence)) if label[i] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trueWords = [[clean(i) for i in j if not isNoise(i)] for j in [nlp(i) for i in causalSent]]\n",
    "trueWords = [del_indicator(ws) for ws in trueWords]\n",
    "trueWords = [i for i in trueWords if len(i) <= 12]\n",
    "falseWords = [[clean(i) for i in j if not isNoise(i)] for j in [nlp(i) for i in noncausalSent]]\n",
    "falseWords = [del_indicator(ws) for ws in falseWords]\n",
    "falseWords = [i for i in falseWords if len(i) <= 12 and len(i) >= 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in gen_data(trueWords, causal=True):\n",
    "    write_to_file(i, '/Users/lizhn7/Downloads/EXPERIMENT/COPA/FINAL/semeval.json')\n",
    "\n",
    "for i in gen_data(falseWords, causal=False):\n",
    "    write_to_file(i, '/Users/lizhn7/Downloads/EXPERIMENT/COPA/FINAL/semeval.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
