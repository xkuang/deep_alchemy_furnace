{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Data-Preprocess\" data-toc-modified-id=\"Data-Preprocess-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data Preprocess</a></div><div class=\"lev1 toc-item\"><a href=\"#Build-Dataset\" data-toc-modified-id=\"Build-Dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Build Dataset</a></div><div class=\"lev2 toc-item\"><a href=\"#Tokenize-Text\" data-toc-modified-id=\"Tokenize-Text-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Tokenize Text</a></div><div class=\"lev2 toc-item\"><a href=\"#Create-Word-Embeddings-with-GloVe\" data-toc-modified-id=\"Create-Word-Embeddings-with-GloVe-22\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Create Word Embeddings with GloVe</a></div><div class=\"lev2 toc-item\"><a href=\"#Split-Data\" data-toc-modified-id=\"Split-Data-23\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Split Data</a></div><div class=\"lev1 toc-item\"><a href=\"#Save-Data\" data-toc-modified-id=\"Save-Data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Save Data</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import spacy\n",
    "from spacy.parts_of_speech import NOUN, VERB, ADJ, ADV, NUM, PROPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "DEV_DATA_DIR = '/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-dev.json'\n",
    "TEST_DATA_DIR = '/Users/lizhn7/Downloads/EXPERIMENT/COPA/LM/data/copa-test.json'\n",
    "#SEMEVAL_DIR = '/Users/lizhn7/Downloads/EXPERIMENT/COPA/FINAL/semeval.json'\n",
    "SEMEVAL_DIR = '/Users/lizhn7/Downloads/EXPERIMENT/COPA/FINAL/semeval_all.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, name):\n",
    "    \"\"\"\n",
    "    Load date from file\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file) as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        item = json.loads(line)\n",
    "        data.append(item[name])\n",
    "    return data\n",
    "\n",
    "def isNoise(token):\n",
    "    \"\"\"\n",
    "    Check if the token is a noise or not \n",
    "    \"\"\"\n",
    "    is_noise = False\n",
    "    if token.pos not in [NOUN, VERB, ADJ, ADV, NUM, PROPN]:\n",
    "        is_noise = True\n",
    "    return is_noise\n",
    "\n",
    "def del_stop(s):\n",
    "    \"\"\"\n",
    "    Delete stop words\n",
    "    \"\"\"\n",
    "    return [w for w in s if w not in stopWords and '-' not in w]\n",
    "\n",
    "def clean(token):\n",
    "    \"\"\"\n",
    "    Clean data\n",
    "    \"\"\"\n",
    "    return token.lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.token.Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "semevalSent = load_data(SEMEVAL_DIR, 'sentence')\n",
    "semevalLabel = load_data(SEMEVAL_DIR, 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, path):\n",
    "        self.rawPremise = load_data(path, 'premise')\n",
    "        self.ask_for = load_data(path, 'asks-for')\n",
    "        self.rawAlternative1 = load_data(path, 'alternative1')\n",
    "        self.rawAlternative2 = load_data(path, 'alternative2')\n",
    "        self.label = load_data(path, 'most-plausible-alternative')\n",
    "        self.premise = [del_stop(s) for s in [[clean(i) for i in j if not isNoise(i)] for j in [nlp(i) for i in self.rawPremise]]]\n",
    "        self.alternative1 = [del_stop(s) for s in [[clean(i) for i in j if not isNoise(i)] for j in [nlp(i) for i in self.rawAlternative1]]]\n",
    "        self.alternative2 = [del_stop(s) for s in [[clean(i) for i in j if not isNoise(i)] for j in [nlp(i) for i in self.rawAlternative2]]]\n",
    "            \n",
    "    def train_data(self):\n",
    "        t1 = [self.premise[i] + self.alternative1[i] for i in range(len(self.premise))]\n",
    "        t2 = [self.premise[i] + self.alternative2[i] for i in range(len(self.premise))]\n",
    "        t3 = [self.alternative1[i] + self.premise[i] for i in range(len(self.premise))]\n",
    "        t4 = [self.alternative2[i] + self.premise[i] for i in range(len(self.premise))]\n",
    "        l1, l2, l3, l4 = [], [], [], []\n",
    "        for i in range(len(self.label)):\n",
    "            if self.label[i] == '1':\n",
    "                l1.append(1), l2.append(0), l3.append(1), l4.append(0);\n",
    "            else:\n",
    "                l1.append(0), l2.append(1), l3.append(0), l4.append(1);\n",
    "        return t1+t2+t3+t4, l1+l2+l3+l4\n",
    "        \n",
    "    def test_data(self):\n",
    "        v1, v2 = [], []\n",
    "        for i in range(len(self.ask_for)):\n",
    "            if self.ask_for[i] == 'cause':\n",
    "                v1.append(self.alternative1[i] + self.premise[i])\n",
    "                v2.append(self.alternative2[i] + self.premise[i])\n",
    "            else:\n",
    "                v1.append(self.premise[i] + self.alternative1[i])\n",
    "                v2.append(self.premise[i] + self.alternative2[i])\n",
    "        return v1, v2, [int(l) for l in self.label]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainData.premise[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = trainData.rawAlternative1[331]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "He put out his back."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'back' in stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ABC"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp('ABC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab\n",
      "-\n",
      "vc\n"
     ]
    }
   ],
   "source": [
    "for i in nlp('AB-VC'):\n",
    "    print(i.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-PRON- PRON True\n",
      "put VERB True\n",
      "out PART True\n",
      "-PRON- ADJ True\n",
      "back NOUN True\n",
      ". PUNCT False\n"
     ]
    }
   ],
   "source": [
    "for i in nlp(a):\n",
    "    print(i.lemma_, i.pos_, i.is_stop)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.tokens.token.Token, got str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.token.Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from nltk import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = Data(DEV_DATA_DIR)\n",
    "valData = Data(DEV_DATA_DIR)\n",
    "testData = Data(TEST_DATA_DIR)\n",
    "\n",
    "xT, yT = trainData.train_data()\n",
    "x1Val, x2Val, yVal = valData.test_data()\n",
    "x1Test, x2Test, yTest = testData.test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He put out his back.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.rawAlternative1[331]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(432, ['cough'])]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in enumerate(trainData.premise) if len(i[-1]) == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scratch', 'back']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.alternative2[331]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['woman',\n",
       " 'know',\n",
       " 'friend',\n",
       " 'go',\n",
       " 'hard',\n",
       " 'time',\n",
       " 'woman',\n",
       " 'tolerate',\n",
       " 'friend',\n",
       " 'difficult',\n",
       " 'behavior']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1Val[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['woman',\n",
       " 'feel',\n",
       " 'friend',\n",
       " 'take',\n",
       " 'advantage',\n",
       " 'kindness',\n",
       " 'woman',\n",
       " 'tolerate',\n",
       " 'friend',\n",
       " 'difficult',\n",
       " 'behavior']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2Val[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: \t11495\n",
      "Distribution of sentence lengths (number of words):\n",
      "Min: 2   Max: 12   Mean: 7.054   Med: 7.000\n",
      "Found 12887 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tok_sentWords = x1Val+x2Val+x1Test+x2Test+semevalSent\n",
    "tokTexts = [' '.join(i) for i in tok_sentWords]\n",
    "tokenizer = Tokenizer(filters='')\n",
    "tokenizer.fit_on_texts(tokTexts)\n",
    "word2index = tokenizer.word_index\n",
    "index2word = {i: w for w, i in word2index.items()}\n",
    "sentLens = np.array([len(i) for i in tok_sentWords])\n",
    "print('Number of sentences: \\t{:d}'.format(len(sentLens)))\n",
    "print('Distribution of sentence lengths (number of words):')\n",
    "print('Min: {:d}   Max: {:d}   Mean: {:.3f}   Med: {:.3f}'.format(np.min(sentLens), np.max(sentLens), np.mean(sentLens), np.median(sentLens)))\n",
    "print('Found %s unique tokens.' % len(word2index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Word Embeddings with GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "VOCAB_SIZE = 12888\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1917495it [03:00, 10608.39it/s]\n",
      "100%|██████████| 12887/12887 [00:00<00:00, 80132.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12710-98.62% tokens in vocab found in glove and copied to embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "glove_n_symbols = 1917495\n",
    "glove_index_dict = {}\n",
    "glove_embedding_weights = np.empty((glove_n_symbols, EMBEDDING_DIM))\n",
    "globale_scale = 0.1\n",
    "with open('/Users/lizhn7/Downloads/DATA/glove/glove.42B.300d.txt', 'r') as fp:\n",
    "    index = 0\n",
    "    for l in tqdm(fp):\n",
    "        l = l.strip().split()\n",
    "        word = l[0]\n",
    "        glove_index_dict[word] = index\n",
    "        glove_embedding_weights[index, :] = [float(n) for n in l[1:]]\n",
    "        index += 1\n",
    "glove_embedding_weights *= globale_scale\n",
    "\n",
    "# Generate random embedding with same scale as glove\n",
    "np.random.seed(SEED)\n",
    "shape = (VOCAB_SIZE, EMBEDDING_DIM)\n",
    "scale = glove_embedding_weights.std() * np.sqrt(12) / 2 \n",
    "embedding = np.random.uniform(low=-scale, high=scale, size=shape)\n",
    "\n",
    "# Copy from glove weights of words that appear in index2word\n",
    "count = 0 \n",
    "for i in tqdm(range(1, VOCAB_SIZE)):\n",
    "    w = index2word[i]\n",
    "    g = glove_index_dict.get(w)\n",
    "    if g is None:\n",
    "        ww = wnl.lemmatize(w)\n",
    "        g = glove_index_dict.get(ww)\n",
    "    if g is None:\n",
    "        ww = porter.stem(w)\n",
    "        g = glove_index_dict.get(ww)\n",
    "    if g is None:\n",
    "        ww = lancaster.stem(w)\n",
    "        g = glove_index_dict.get(ww)\n",
    "    if g is None:\n",
    "        ww = w[:-1]\n",
    "        glove_index_dict.get(ww)\n",
    "    if g is None:\n",
    "        ww = w[:-2]\n",
    "        glove_index_dict.get(ww)\n",
    "    if g is None:\n",
    "        ww = w[:-3]\n",
    "        glove_index_dict.get(ww)\n",
    "    if g is not None:\n",
    "        embedding[i, :] = glove_embedding_weights[g, :]\n",
    "        count += 1\n",
    "print('{num_tokens}-{per:.2f}% tokens in vocab found in glove and copied to embedding.'.format(num_tokens=count, per=count/float(VOCAB_SIZE)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[word2index[w] for w in s] for s in semevalSent]\n",
    "x1Val = [[word2index[w] for w in s] for s in x1Val]\n",
    "x2Val = [[word2index[w] for w in s] for s in x2Val]\n",
    "x1Test = [[word2index[w] for w in s] for s in x1Test]\n",
    "x2Test = [[word2index[w] for w in s] for s in x2Test]\n",
    "\n",
    "x = pad_sequences(x, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "x1Val = pad_sequences(x1Val, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "x2Val = pad_sequences(x2Val, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "x1Test = pad_sequences(x1Test, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "x2Test = pad_sequences(x2Test, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "y = np.array(semevalLabel)\n",
    "yVal = np.array(yVal)\n",
    "yTest = np.array(yTest)\n",
    "\n",
    "xTrain, _, yTrain, _ = train_test_split(x, y, test_size=0., random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1148, 9495)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y), len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/lizhn7/Downloads/EXPERIMENT/COPA/FINAL/index.pkl', 'wb') as fp:\n",
    "    pickle.dump((word2index, index2word), fp, -1)\n",
    "\n",
    "fh = h5py.File('/Users/lizhn7/Downloads/EXPERIMENT/COPA/FINAL/embedding.h5', 'w')\n",
    "fh['embedding'] = embedding\n",
    "fh.close()\n",
    "\n",
    "fh = h5py.File('/Users/lizhn7/Downloads/EXPERIMENT/COPA/FINAL/train.h5', 'w')\n",
    "fh['xTrain'] = xTrain\n",
    "fh['yTrain'] = yTrain\n",
    "fh.close()\n",
    "\n",
    "fh = h5py.File('/Users/lizhn7/Downloads/EXPERIMENT/COPA/FINAL/val.h5', 'w')\n",
    "fh['x1Val'] = x1Val\n",
    "fh['x2Val'] = x2Val\n",
    "fh['yVal'] = yVal\n",
    "fh.close()\n",
    "\n",
    "fh = h5py.File('/Users/lizhn7/Downloads/EXPERIMENT/COPA/FINAL/test.h5', 'w')\n",
    "fh['x1Test'] = x1Test\n",
    "fh['x2Test'] = x2Test\n",
    "fh['yTest'] = yTest\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12888, 300)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
