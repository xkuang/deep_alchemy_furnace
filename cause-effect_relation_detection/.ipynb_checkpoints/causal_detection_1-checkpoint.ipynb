{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Data-Pre-Processing\" data-toc-modified-id=\"Data-Pre-Processing-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data Pre-Processing</a></div><div class=\"lev2 toc-item\"><a href=\"#Load-Data\" data-toc-modified-id=\"Load-Data-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Load Data</a></div><div class=\"lev2 toc-item\"><a href=\"#Word-Segmentation\" data-toc-modified-id=\"Word-Segmentation-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Word Segmentation</a></div><div class=\"lev2 toc-item\"><a href=\"#Explore-the-Data\" data-toc-modified-id=\"Explore-the-Data-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Explore the Data</a></div><div class=\"lev1 toc-item\"><a href=\"#Word-Embedding\" data-toc-modified-id=\"Word-Embedding-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Word Embedding</a></div><div class=\"lev2 toc-item\"><a href=\"#Tokenize-Text\" data-toc-modified-id=\"Tokenize-Text-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Tokenize Text</a></div><div class=\"lev2 toc-item\"><a href=\"#Create-Word-Embeddings-with-GloVe\" data-toc-modified-id=\"Create-Word-Embeddings-with-GloVe-22\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Create Word Embeddings with GloVe</a></div><div class=\"lev3 toc-item\"><a href=\"#Read-Glove\" data-toc-modified-id=\"Read-Glove-221\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Read Glove</a></div><div class=\"lev3 toc-item\"><a href=\"#Use-Glove-to-Initialize-Embedding-Matrix\" data-toc-modified-id=\"Use-Glove-to-Initialize-Embedding-Matrix-222\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Use Glove to Initialize Embedding Matrix</a></div><div class=\"lev1 toc-item\"><a href=\"#Build-Dateset\" data-toc-modified-id=\"Build-Dateset-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Build Dateset</a></div><div class=\"lev1 toc-item\"><a href=\"#Save-Dataset\" data-toc-modified-id=\"Save-Dataset-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Save Dataset</a></div><div class=\"lev1 toc-item\"><a href=\"#Checkpoint\" data-toc-modified-id=\"Checkpoint-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Checkpoint</a></div><div class=\"lev1 toc-item\"><a href=\"#Build-Model\" data-toc-modified-id=\"Build-Model-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Build Model</a></div><div class=\"lev2 toc-item\"><a href=\"#Set-Hyperparameters\" data-toc-modified-id=\"Set-Hyperparameters-61\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Set Hyperparameters</a></div><div class=\"lev2 toc-item\"><a href=\"#Import-Libraries\" data-toc-modified-id=\"Import-Libraries-62\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Import Libraries</a></div><div class=\"lev2 toc-item\"><a href=\"#Model-Visualization\" data-toc-modified-id=\"Model-Visualization-63\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Model Visualization</a></div><div class=\"lev2 toc-item\"><a href=\"#Train\" data-toc-modified-id=\"Train-64\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Train</a></div><div class=\"lev1 toc-item\"><a href=\"#Evaluate\" data-toc-modified-id=\"Evaluate-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Evaluate</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Load date from file\n",
    "    \"\"\"\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_path = 'train_data.txt'\n",
    "train_data = load_data(train_data_path).strip().split('\\t')[1:]\n",
    "train_data = [line.split('\\n')[:2] for line in train_data]\n",
    "test_data_path = 'test_data.txt'\n",
    "test_data = load_data(test_data_path).strip().split('\\t')[1:]\n",
    "test_data = [line.split('\\n')[:2] for line in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sent = [line[0] for line in train_data]\n",
    "test_sent = [line[0] for line in test_data]\n",
    "train_label = ['Causal' if line[-1] == 'Cause-Effect(e2,e1)' or line[-1] == 'Cause-Effect(e1,e2)' else 'Non-Causal' for line in train_data]\n",
    "test_label = ['Causal' if line[-1] == 'Cause-Effect(e2,e1)' or line[-1] == 'Cause-Effect(e1,e2)' else 'Non-Causal' for line in test_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"stopwords.txt\"\n",
    "stopWords = {w: None for w in open(filename).read().split()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cut(s):\n",
    "    \"\"\"\n",
    "    Word segmentation\n",
    "    \"\"\"\n",
    "    pattern = r'''\n",
    "              (?x)                   # set flag to allow verbose regexps \n",
    "              (?:[A-Z]\\.)+           # abbreviations, e.g. U.S.A. \n",
    "              |\\d+(?:\\.\\d+)?%?       # numbers, incl. currency and percentages \n",
    "              |\\w+(?:[-&']\\w+)*      # words w/ optional internal hyphens/apostrophe \n",
    "           '''  \n",
    "    return regexp_tokenize(s, pattern=pattern)\n",
    "\n",
    "def find_pn(ws):\n",
    "    \"\"\"\n",
    "    Find paired nominals\n",
    "    \"\"\"\n",
    "    for i in range(len(ws)):\n",
    "        if ws[i] == 'e1':\n",
    "            for j in range(i+1, len(ws)):\n",
    "                if ws[j] == 'e1':\n",
    "                    pn1 = ws[i+1:j] \n",
    "        if ws[i] == 'e2':\n",
    "            for j in range(i+1, len(ws)):\n",
    "                if ws[j] == 'e2':\n",
    "                    pn2 = ws[i+1:j]\n",
    "    return pn1, pn2\n",
    "\n",
    "def del_stop(ws):\n",
    "    \"\"\"\n",
    "    Delete stopwords\n",
    "    \"\"\"\n",
    "    return [i for i in [stopWords.get(i.lower(), i) for i in ws] if i != None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainWords = [cut(s) for s in train_sent] \n",
    "testWords = [cut(s) for s in test_sent] \n",
    "causalSent = [[' '.join(trainWords[i])] for i in range(len(trainWords)) if train_label[i] == 'Causal']\n",
    "trainWords = [del_stop(ws) for ws in trainWords]\n",
    "testWords = [del_stop(ws) for ws in testWords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lawsonite was contained a platinum crucible and counter-weight was a plastic crucible with metal pieces'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(trainWords[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainPn1[8], trainPn2[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Non-Causal'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1003"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in train_label if i == 'Causal']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in test_label if i == 'Causal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max([len(trainWords[i]) for i in range(len(train_label))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max([len(testWords[i]) for i in range(len(test_label))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max([len(trainWords[i]) for i in range(len(train_label)) if train_label[i] == 'Causal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max([len(testWords[i]) for i in range(len(test_label)) if test_label[i] == 'Causal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23594 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokWords = trainWords.copy()\n",
    "tokWords.extend(testWords)\n",
    "tokTexts = [' '.join(i) for i in tokWords]\n",
    "tokenizer = Tokenizer(filters='')\n",
    "tokenizer.fit_on_texts(tokTexts)\n",
    "word2index = tokenizer.word_index\n",
    "index2word = {i: w for w, i in word2index.items()}\n",
    "print('Found %s unique tokens.' % len(word2index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Word Embeddings with GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 23595\n",
    "EMBEDDING_SIZE = 300\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_n_symbols = 1917495\n",
    "glove_index_dict = {}\n",
    "glove_embedding_weights = np.empty((glove_n_symbols, EMBEDDING_SIZE))\n",
    "globale_scale = 0.1\n",
    "with open('/Users/lizhn7/Downloads/DATA/Glove/glove.42B.300d.txt', 'r') as fp:\n",
    "    index = 0\n",
    "    for l in fp:\n",
    "        l = l.strip().split()\n",
    "        word = l[0]\n",
    "        glove_index_dict[word] = index\n",
    "        glove_embedding_weights[index, :] = [float(n) for n in l[1:]]\n",
    "        index += 1\n",
    "glove_embedding_weights *= globale_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Glove to Initialize Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer, PorterStemmer, LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate random embedding with same scale as glove\n",
    "np.random.seed(SEED)\n",
    "shape = (VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "scale = glove_embedding_weights.std() * np.sqrt(12) / 2 \n",
    "embedding = np.random.uniform(low=-scale, high=scale, size=shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22382-94.86% tokens in vocab found in glove and copied to embedding.\n"
     ]
    }
   ],
   "source": [
    "# Copy from glove weights of words that appear in index2word\n",
    "count = 0 \n",
    "for i in range(1, VOCAB_SIZE):\n",
    "    w = index2word[i]\n",
    "    g = glove_index_dict.get(w)\n",
    "    if g is None:\n",
    "        w = wnl.lemmatize(w)\n",
    "        g = glove_index_dict.get(w)\n",
    "    if g is None:\n",
    "        w = porter.stem(w)\n",
    "        g = glove_index_dict.get(w)\n",
    "    if g is None:\n",
    "        w = lancaster.stem(w)\n",
    "        g = glove_index_dict.get(w)\n",
    "    if g is not None:\n",
    "        embedding[i, :] = glove_embedding_weights[g, :]\n",
    "        count += 1\n",
    "print('{num_tokens}-{per:.2f}% tokens in vocab found in glove and copied to embedding.'.format(num_tokens=count, per=count/float(VOCAB_SIZE)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Dateset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelDict = {'Non-Causal': 0, 'Causal': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_seq(ws, label):\n",
    "    \"\"\"\n",
    "    Pad words sequene to MAX_LEN and encode label to one-hot encoding\n",
    "    \"\"\"\n",
    "    sentText = [' '.join(i) for i in ws]\n",
    "    sentSeq = tokenizer.texts_to_sequences(sentText)\n",
    "    sentData = pad_sequences(sentSeq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "    labelData = np.array([[labelDict[i]] for i in label])\n",
    "    return sentData, labelData "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xTrain, yTrain = convert_seq(trainWords, train_label)\n",
    "xTest, yTest = convert_seq(testWords, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, _, yTrain, _ = train_test_split(xTrain, yTrain, test_size=0., random_state=SEED)\n",
    "xTest, _, yTest, _ = train_test_split(xTest, yTest, test_size=0., random_state=SEED)\n",
    "causalSent, _, causalSent, _ = train_test_split(causalSent, causalSent, test_size=0., random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fh = h5py.File('/Users/lizhn7/Downloads/DATA/semeval2010_task8_all_data/causal_detection/data', 'w')\n",
    "fh['xTrain'] = xTrain\n",
    "fh['yTrain'] = yTrain\n",
    "fh['xTest'] = xTest\n",
    "fh['yTest'] = yTest\n",
    "fh['embedding'] = embedding\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/lizhn7/Downloads/DATA/semeval2010_task8_all_data/causal_detection/causalSent', 'wb') as fp:\n",
    "    pickle.dump((causalSent), fp, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File('/Users/lizhn7/Downloads/DATA/semeval2010_task8_all_data/causal_detection/data', 'r') as fh:\n",
    "    xTrain = fh['xTrain'][:]\n",
    "    yTrain = fh['yTrain'][:]\n",
    "    xTest = fh['xTest'][:]\n",
    "    yTest = fh['yTest'][:]\n",
    "    embedding = fh['embedding'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/Users/lizhn7/Downloads/DATA/semeval2010_task8_all_data/causal_detection/causalSent', 'rb') as fp:\n",
    "    causalSent = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 45\n",
    "VOCAB_SIZE = 23595\n",
    "EMBEDDING_SIZE = 300\n",
    "RNN_SIZE = 150\n",
    "DROPOUT_RATE = 0.5\n",
    "RNN_DROPOUT_RATE = 0.5\n",
    "CNN_SIZE = 128\n",
    "WINDOW_SIZE = 3\n",
    "NUM_EPOCHS = 128\n",
    "BATCH_SIZE = 32\n",
    "STEPS_PER_EPOCH = 20\n",
    "TEST_STEPS = len(xTest)//BATCH_SIZE+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_EPOCHS: \t\t128\n",
      "STEPS_PER_EPOCH: \t20\n",
      "TEST_STEPS: \t\t85\n",
      "TRAIN_BATCHES: \t\t2560\n",
      "NUM_BATCHES \t\t251\n"
     ]
    }
   ],
   "source": [
    "print('NUM_EPOCHS: \\t\\t%d' % NUM_EPOCHS)\n",
    "print('STEPS_PER_EPOCH: \\t%d' % STEPS_PER_EPOCH)\n",
    "print('TEST_STEPS: \\t\\t%d' % TEST_STEPS)\n",
    "\n",
    "print('TRAIN_BATCHES: \\t\\t%d' % (NUM_EPOCHS * STEPS_PER_EPOCH))\n",
    "print('NUM_BATCHES \\t\\t%d' % (len(xTrain)//BATCH_SIZE+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, Dropout, Bidirectional, LSTM, Reshape, concatenate, Conv1D, BatchNormalization, GlobalMaxPooling1D, Dense\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.callbacks import*\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "seq = Input(shape=(MAX_LEN,), name='INPUT') \n",
    "emb_seq = Embedding(VOCAB_SIZE, EMBEDDING_SIZE, weights=[embedding], mask_zero=False, input_length=MAX_LEN, trainable=True, name='EMBEDDING')(seq)\n",
    "emb_seq = Dropout(DROPOUT_RATE, name='DROPOUT_1')(emb_seq)\n",
    "blstm = Bidirectional(LSTM(RNN_SIZE, return_sequences=True, implementation=0, dropout=RNN_DROPOUT_RATE, recurrent_dropout=RNN_DROPOUT_RATE), merge_mode='concat', name='BiLSTM_1')(emb_seq)\n",
    "blstm = Dropout(DROPOUT_RATE, name='DROPOUT_2')(blstm)\n",
    "blstm = Bidirectional(LSTM(RNN_SIZE, return_sequences=True, implementation=0, dropout=RNN_DROPOUT_RATE, recurrent_dropout=RNN_DROPOUT_RATE), merge_mode='concat', name='BiLSTM_2')(blstm)\n",
    "blstm = Dropout(DROPOUT_RATE, name='DROPOUT_3')(blstm)\n",
    "conv = Conv1D(CNN_SIZE, WINDOW_SIZE, padding='valid', activation='elu', name='CONV')(blstm)\n",
    "pool = GlobalMaxPooling1D(name='MAXPOOLING')(conv)\n",
    "pool = Dropout(DROPOUT_RATE, name='DROPOUT_4')(pool)\n",
    "output = Dense(1, activation='sigmoid', name='OUTPUT')(pool)\n",
    "model = Model(inputs=seq, outputs=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "INPUT (InputLayer)           (None, 45)                0         \n",
      "_________________________________________________________________\n",
      "EMBEDDING (Embedding)        (None, 45, 300)           7078500   \n",
      "_________________________________________________________________\n",
      "DROPOUT_1 (Dropout)          (None, 45, 300)           0         \n",
      "_________________________________________________________________\n",
      "BiLSTM_1 (Bidirectional)     (None, 45, 300)           541200    \n",
      "_________________________________________________________________\n",
      "DROPOUT_2 (Dropout)          (None, 45, 300)           0         \n",
      "_________________________________________________________________\n",
      "BiLSTM_2 (Bidirectional)     (None, 45, 300)           541200    \n",
      "_________________________________________________________________\n",
      "DROPOUT_3 (Dropout)          (None, 45, 300)           0         \n",
      "_________________________________________________________________\n",
      "CONV (Conv1D)                (None, 43, 128)           115328    \n",
      "_________________________________________________________________\n",
      "MAXPOOLING (GlobalMaxPooling (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "DROPOUT_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "OUTPUT (Dense)               (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 8,276,357\n",
      "Trainable params: 8,276,357\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"775pt\" viewBox=\"0.00 0.00 263.15 775.00\" width=\"263pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 771)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-771 259.1484,-771 259.1484,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 10200887648 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>10200887648</title>\n",
       "<polygon fill=\"none\" points=\"64.521,-730.5 64.521,-766.5 190.6274,-766.5 190.6274,-730.5 64.521,-730.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127.5742\" y=\"-744.3\">INPUT: InputLayer</text>\n",
       "</g>\n",
       "<!-- 10200888208 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>10200888208</title>\n",
       "<polygon fill=\"none\" points=\"41.6104,-657.5 41.6104,-693.5 213.5381,-693.5 213.5381,-657.5 41.6104,-657.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127.5742\" y=\"-671.3\">EMBEDDING: Embedding</text>\n",
       "</g>\n",
       "<!-- 10200887648&#45;&gt;10200888208 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>10200887648-&gt;10200888208</title>\n",
       "<path d=\"M127.5742,-730.4551C127.5742,-722.3828 127.5742,-712.6764 127.5742,-703.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"131.0743,-703.5903 127.5742,-693.5904 124.0743,-703.5904 131.0743,-703.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 10200888040 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>10200888040</title>\n",
       "<polygon fill=\"none\" points=\"52.4897,-584.5 52.4897,-620.5 202.6587,-620.5 202.6587,-584.5 52.4897,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127.5742\" y=\"-598.3\">DROPOUT_1: Dropout</text>\n",
       "</g>\n",
       "<!-- 10200888208&#45;&gt;10200888040 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>10200888208-&gt;10200888040</title>\n",
       "<path d=\"M127.5742,-657.4551C127.5742,-649.3828 127.5742,-639.6764 127.5742,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"131.0743,-630.5903 127.5742,-620.5904 124.0743,-630.5904 131.0743,-630.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 9851051088 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>9851051088</title>\n",
       "<polygon fill=\"none\" points=\"0,-511.5 0,-547.5 255.1484,-547.5 255.1484,-511.5 0,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127.5742\" y=\"-525.3\">BiLSTM_1(lstm_1): Bidirectional(LSTM)</text>\n",
       "</g>\n",
       "<!-- 10200888040&#45;&gt;9851051088 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>10200888040-&gt;9851051088</title>\n",
       "<path d=\"M127.5742,-584.4551C127.5742,-576.3828 127.5742,-566.6764 127.5742,-557.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"131.0743,-557.5903 127.5742,-547.5904 124.0743,-557.5904 131.0743,-557.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 10200888992 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>10200888992</title>\n",
       "<polygon fill=\"none\" points=\"52.4897,-438.5 52.4897,-474.5 202.6587,-474.5 202.6587,-438.5 52.4897,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127.5742\" y=\"-452.3\">DROPOUT_2: Dropout</text>\n",
       "</g>\n",
       "<!-- 9851051088&#45;&gt;10200888992 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>9851051088-&gt;10200888992</title>\n",
       "<path d=\"M127.5742,-511.4551C127.5742,-503.3828 127.5742,-493.6764 127.5742,-484.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"131.0743,-484.5903 127.5742,-474.5904 124.0743,-484.5904 131.0743,-484.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 10016390056 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>10016390056</title>\n",
       "<polygon fill=\"none\" points=\"0,-365.5 0,-401.5 255.1484,-401.5 255.1484,-365.5 0,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127.5742\" y=\"-379.3\">BiLSTM_2(lstm_2): Bidirectional(LSTM)</text>\n",
       "</g>\n",
       "<!-- 10200888992&#45;&gt;10016390056 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>10200888992-&gt;10016390056</title>\n",
       "<path d=\"M127.5742,-438.4551C127.5742,-430.3828 127.5742,-420.6764 127.5742,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"131.0743,-411.5903 127.5742,-401.5904 124.0743,-411.5904 131.0743,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 9851224696 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>9851224696</title>\n",
       "<polygon fill=\"none\" points=\"52.4897,-292.5 52.4897,-328.5 202.6587,-328.5 202.6587,-292.5 52.4897,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127.5742\" y=\"-306.3\">DROPOUT_3: Dropout</text>\n",
       "</g>\n",
       "<!-- 10016390056&#45;&gt;9851224696 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>10016390056-&gt;9851224696</title>\n",
       "<path d=\"M127.5742,-365.4551C127.5742,-357.3828 127.5742,-347.6764 127.5742,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"131.0743,-338.5903 127.5742,-328.5904 124.0743,-338.5904 131.0743,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 10016563096 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>10016563096</title>\n",
       "<polygon fill=\"none\" points=\"72.8369,-219.5 72.8369,-255.5 182.3115,-255.5 182.3115,-219.5 72.8369,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127.5742\" y=\"-233.3\">CONV: Conv1D</text>\n",
       "</g>\n",
       "<!-- 9851224696&#45;&gt;10016563096 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>9851224696-&gt;10016563096</title>\n",
       "<path d=\"M127.5742,-292.4551C127.5742,-284.3828 127.5742,-274.6764 127.5742,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"131.0743,-265.5903 127.5742,-255.5904 124.0743,-265.5904 131.0743,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 10016641432 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>10016641432</title>\n",
       "<polygon fill=\"none\" points=\"6.6035,-146.5 6.6035,-182.5 248.5449,-182.5 248.5449,-146.5 6.6035,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127.5742\" y=\"-160.3\">MAXPOOLING: GlobalMaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 10016563096&#45;&gt;10016641432 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>10016563096-&gt;10016641432</title>\n",
       "<path d=\"M127.5742,-219.4551C127.5742,-211.3828 127.5742,-201.6764 127.5742,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"131.0743,-192.5903 127.5742,-182.5904 124.0743,-192.5904 131.0743,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 9994599448 -->\n",
       "<g class=\"node\" id=\"node10\">\n",
       "<title>9994599448</title>\n",
       "<polygon fill=\"none\" points=\"52.4897,-73.5 52.4897,-109.5 202.6587,-109.5 202.6587,-73.5 52.4897,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127.5742\" y=\"-87.3\">DROPOUT_4: Dropout</text>\n",
       "</g>\n",
       "<!-- 10016641432&#45;&gt;9994599448 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>10016641432-&gt;9994599448</title>\n",
       "<path d=\"M127.5742,-146.4551C127.5742,-138.3828 127.5742,-128.6764 127.5742,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"131.0743,-119.5903 127.5742,-109.5904 124.0743,-119.5904 131.0743,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 10001245128 -->\n",
       "<g class=\"node\" id=\"node11\">\n",
       "<title>10001245128</title>\n",
       "<polygon fill=\"none\" points=\"71.1245,-.5 71.1245,-36.5 184.0239,-36.5 184.0239,-.5 71.1245,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127.5742\" y=\"-14.3\">OUTPUT: Dense</text>\n",
       "</g>\n",
       "<!-- 9994599448&#45;&gt;10001245128 -->\n",
       "<g class=\"edge\" id=\"edge10\">\n",
       "<title>9994599448-&gt;10001245128</title>\n",
       "<path d=\"M127.5742,-73.4551C127.5742,-65.3828 127.5742,-55.6764 127.5742,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"131.0743,-46.5903 127.5742,-36.5904 124.0743,-46.5904 131.0743,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generator_all(data, label, batch_size):\n",
    "    \"\"\"\n",
    "    Yield batches of all data\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    while True:\n",
    "        if count >= len(data): \n",
    "            count = 0\n",
    "        x = np.zeros((batch_size, MAX_LEN))\n",
    "        y = np.zeros((batch_size, 1))\n",
    "        for i in range(batch_size):\n",
    "            n = i + count\n",
    "            if n > len(data)-1:\n",
    "                break\n",
    "            x[i, :] = data[n]\n",
    "            y[i, :] = label[n]\n",
    "        count += batch_size\n",
    "        yield (x, y)\n",
    "        \n",
    "def data_generator(data, label, batch_size): \n",
    "    \"\"\"\n",
    "    Yield batches \n",
    "    \"\"\"\n",
    "    index = np.arange(len(data))\n",
    "    np.random.shuffle(index)    \n",
    "    batches = [index[range(batch_size*i, min(len(data), batch_size*(i+1)))] for i in range(len(data)//batch_size)]\n",
    "    while True:\n",
    "        for i in batches:\n",
    "            x, y = data[i], label[i]\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_train = data_generator(xTrain, yTrain, BATCH_SIZE)\n",
    "gen_test = data_generator_all(xTest, yTest, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = '/Users/lizhn7/Downloads/DATA/semeval2010_task8_all_data/causal_detection/cp_logs/1/weights.{epoch:03d}-{val_loss:.6f}.hdf5'\n",
    "log_string = '/Users/lizhn7/Downloads/DATA/semeval2010_task8_all_data/causal_detection/tb_logs/1'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "tensorboard = TensorBoard(log_dir=log_string) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.4116Epoch 00000: val_loss improved from inf to 0.37317, saving model to /Users/lizhn7/Downloads/DATA/semeval2010_task8_all_data/causal_detection/cp_logs/1/weights.000-0.373173.hdf5\n",
      "20/20 [==============================] - 33s - loss: 0.4107 - val_loss: 0.3732\n",
      "Epoch 2/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.4170Epoch 00001: val_loss improved from 0.37317 to 0.36309, saving model to /Users/lizhn7/Downloads/DATA/semeval2010_task8_all_data/causal_detection/cp_logs/1/weights.001-0.363095.hdf5\n",
      "20/20 [==============================] - 33s - loss: 0.4122 - val_loss: 0.3631\n",
      "Epoch 3/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.3225Epoch 00002: val_loss improved from 0.36309 to 0.23593, saving model to /Users/lizhn7/Downloads/DATA/semeval2010_task8_all_data/causal_detection/cp_logs/1/weights.002-0.235930.hdf5\n",
      "20/20 [==============================] - 33s - loss: 0.3177 - val_loss: 0.2359\n",
      "Epoch 4/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.2605Epoch 00003: val_loss did not improve\n",
      "20/20 [==============================] - 34s - loss: 0.2668 - val_loss: 0.2908\n",
      "Epoch 5/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.2042Epoch 00004: val_loss did not improve\n",
      "20/20 [==============================] - 33s - loss: 0.2075 - val_loss: 0.2680\n",
      "Epoch 6/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.2409Epoch 00005: val_loss did not improve\n",
      "20/20 [==============================] - 31s - loss: 0.2366 - val_loss: 0.2432\n",
      "Epoch 7/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.2006Epoch 00006: val_loss improved from 0.23593 to 0.16945, saving model to /Users/lizhn7/Downloads/DATA/semeval2010_task8_all_data/causal_detection/cp_logs/1/weights.006-0.169452.hdf5\n",
      "20/20 [==============================] - 29s - loss: 0.1969 - val_loss: 0.1695\n",
      "Epoch 8/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1931Epoch 00007: val_loss did not improve\n",
      "20/20 [==============================] - 30s - loss: 0.1981 - val_loss: 0.1995\n",
      "Epoch 9/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.2097Epoch 00008: val_loss improved from 0.16945 to 0.16027, saving model to /Users/lizhn7/Downloads/DATA/semeval2010_task8_all_data/causal_detection/cp_logs/1/weights.008-0.160265.hdf5\n",
      "20/20 [==============================] - 30s - loss: 0.2010 - val_loss: 0.1603\n",
      "Epoch 10/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.2375Epoch 00009: val_loss did not improve\n",
      "20/20 [==============================] - 32s - loss: 0.2411 - val_loss: 0.2139\n",
      "Epoch 11/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.2229Epoch 00010: val_loss did not improve\n",
      "20/20 [==============================] - 28s - loss: 0.2221 - val_loss: 0.2319\n",
      "Epoch 12/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1227Epoch 00011: val_loss improved from 0.16027 to 0.14073, saving model to /Users/lizhn7/Downloads/DATA/semeval2010_task8_all_data/causal_detection/cp_logs/1/weights.011-0.140733.hdf5\n",
      "20/20 [==============================] - 32s - loss: 0.1383 - val_loss: 0.1407\n",
      "Epoch 13/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1803Epoch 00012: val_loss did not improve\n",
      "20/20 [==============================] - 38s - loss: 0.1771 - val_loss: 0.1676\n",
      "Epoch 14/128\n",
      "19/20 [===========================>..] - ETA: 1s - loss: 0.1498Epoch 00013: val_loss did not improve\n",
      "20/20 [==============================] - 42s - loss: 0.1432 - val_loss: 0.1436\n",
      "Epoch 15/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1790Epoch 00014: val_loss did not improve\n",
      "20/20 [==============================] - 35s - loss: 0.1747 - val_loss: 0.1448\n",
      "Epoch 16/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.2067Epoch 00015: val_loss did not improve\n",
      "20/20 [==============================] - 33s - loss: 0.2000 - val_loss: 0.1450\n",
      "Epoch 17/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1456Epoch 00016: val_loss did not improve\n",
      "20/20 [==============================] - 33s - loss: 0.1443 - val_loss: 0.1479\n",
      "Epoch 18/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1583Epoch 00017: val_loss improved from 0.14073 to 0.13869, saving model to /Users/lizhn7/Downloads/DATA/semeval2010_task8_all_data/causal_detection/cp_logs/1/weights.017-0.138685.hdf5\n",
      "20/20 [==============================] - 28s - loss: 0.1517 - val_loss: 0.1387\n",
      "Epoch 19/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1748Epoch 00018: val_loss did not improve\n",
      "20/20 [==============================] - 26s - loss: 0.1872 - val_loss: 0.1924\n",
      "Epoch 20/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1413Epoch 00019: val_loss improved from 0.13869 to 0.13839, saving model to /Users/lizhn7/Downloads/DATA/semeval2010_task8_all_data/causal_detection/cp_logs/1/weights.019-0.138385.hdf5\n",
      "20/20 [==============================] - 28s - loss: 0.1454 - val_loss: 0.1384\n",
      "Epoch 21/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1498Epoch 00020: val_loss did not improve\n",
      "20/20 [==============================] - 26s - loss: 0.1476 - val_loss: 0.1432\n",
      "Epoch 22/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1878Epoch 00021: val_loss improved from 0.13839 to 0.13543, saving model to /Users/lizhn7/Downloads/DATA/semeval2010_task8_all_data/causal_detection/cp_logs/1/weights.021-0.135431.hdf5\n",
      "20/20 [==============================] - 27s - loss: 0.1844 - val_loss: 0.1354\n",
      "Epoch 23/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1502Epoch 00022: val_loss improved from 0.13543 to 0.13041, saving model to /Users/lizhn7/Downloads/DATA/semeval2010_task8_all_data/causal_detection/cp_logs/1/weights.022-0.130408.hdf5\n",
      "20/20 [==============================] - 26s - loss: 0.1462 - val_loss: 0.1304\n",
      "Epoch 24/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1489Epoch 00023: val_loss did not improve\n",
      "20/20 [==============================] - 26s - loss: 0.1485 - val_loss: 0.1384\n",
      "Epoch 25/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1380Epoch 00024: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.1434 - val_loss: 0.1552\n",
      "Epoch 26/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1081Epoch 00025: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.1105 - val_loss: 0.1412\n",
      "Epoch 27/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1436Epoch 00026: val_loss did not improve\n",
      "20/20 [==============================] - 26s - loss: 0.1495 - val_loss: 0.2040\n",
      "Epoch 28/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1513Epoch 00027: val_loss did not improve\n",
      "20/20 [==============================] - 26s - loss: 0.1517 - val_loss: 0.1503\n",
      "Epoch 29/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1256Epoch 00028: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.1282 - val_loss: 0.1781\n",
      "Epoch 30/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1402Epoch 00029: val_loss did not improve\n",
      "20/20 [==============================] - 26s - loss: 0.1383 - val_loss: 0.1532\n",
      "Epoch 31/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1432Epoch 00030: val_loss did not improve\n",
      "20/20 [==============================] - 26s - loss: 0.1373 - val_loss: 0.1487\n",
      "Epoch 32/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1232Epoch 00031: val_loss did not improve\n",
      "20/20 [==============================] - 26s - loss: 0.1189 - val_loss: 0.1452\n",
      "Epoch 33/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1300Epoch 00032: val_loss did not improve\n",
      "20/20 [==============================] - 26s - loss: 0.1337 - val_loss: 0.1820\n",
      "Epoch 34/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1443Epoch 00033: val_loss improved from 0.13041 to 0.12169, saving model to /Users/lizhn7/Downloads/DATA/semeval2010_task8_all_data/causal_detection/cp_logs/1/weights.033-0.121688.hdf5\n",
      "20/20 [==============================] - 27s - loss: 0.1470 - val_loss: 0.1217\n",
      "Epoch 35/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1568Epoch 00034: val_loss did not improve\n",
      "20/20 [==============================] - 28s - loss: 0.1684 - val_loss: 0.1618\n",
      "Epoch 36/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1362Epoch 00035: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.1308 - val_loss: 0.1513\n",
      "Epoch 37/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0958Epoch 00036: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.1065 - val_loss: 0.1282\n",
      "Epoch 38/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1224Epoch 00037: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.1206 - val_loss: 0.1305\n",
      "Epoch 39/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1020Epoch 00038: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.0974 - val_loss: 0.1313\n",
      "Epoch 40/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1324Epoch 00039: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.1327 - val_loss: 0.1346\n",
      "Epoch 41/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1705Epoch 00040: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.1651 - val_loss: 0.1377\n",
      "Epoch 42/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1241Epoch 00041: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.1269 - val_loss: 0.1730\n",
      "Epoch 43/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1093Epoch 00042: val_loss did not improve\n",
      "20/20 [==============================] - 28s - loss: 0.1053 - val_loss: 0.1294\n",
      "Epoch 44/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1312Epoch 00043: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.1385 - val_loss: 0.1638\n",
      "Epoch 45/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0775Epoch 00044: val_loss did not improve\n",
      "20/20 [==============================] - 26s - loss: 0.0749 - val_loss: 0.1389\n",
      "Epoch 46/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1090Epoch 00045: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.1085 - val_loss: 0.1301\n",
      "Epoch 47/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1474Epoch 00046: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.1436 - val_loss: 0.1295\n",
      "Epoch 48/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1065Epoch 00047: val_loss did not improve\n",
      "20/20 [==============================] - 26s - loss: 0.1056 - val_loss: 0.1363\n",
      "Epoch 49/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1186Epoch 00048: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.1141 - val_loss: 0.1296\n",
      "Epoch 50/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0862Epoch 00049: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.0899 - val_loss: 0.1289\n",
      "Epoch 51/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0948Epoch 00050: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.0981 - val_loss: 0.1305\n",
      "Epoch 52/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1160Epoch 00051: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.1211 - val_loss: 0.1747\n",
      "Epoch 53/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1276Epoch 00052: val_loss did not improve\n",
      "20/20 [==============================] - 26s - loss: 0.1241 - val_loss: 0.1356\n",
      "Epoch 54/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1185Epoch 00053: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.1262 - val_loss: 0.1776\n",
      "Epoch 55/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1202Epoch 00054: val_loss did not improve\n",
      "20/20 [==============================] - 26s - loss: 0.1154 - val_loss: 0.1520\n",
      "Epoch 56/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1238Epoch 00055: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.1184 - val_loss: 0.1478\n",
      "Epoch 57/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0852Epoch 00056: val_loss did not improve\n",
      "20/20 [==============================] - 26s - loss: 0.0827 - val_loss: 0.1417\n",
      "Epoch 58/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1057Epoch 00057: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.1090 - val_loss: 0.1648\n",
      "Epoch 59/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0795Epoch 00058: val_loss did not improve\n",
      "20/20 [==============================] - 26s - loss: 0.0881 - val_loss: 0.1296\n",
      "Epoch 60/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1166Epoch 00059: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.1258 - val_loss: 0.1593\n",
      "Epoch 61/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1256Epoch 00060: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.1227 - val_loss: 0.1443\n",
      "Epoch 62/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0715Epoch 00061: val_loss did not improve\n",
      "20/20 [==============================] - 26s - loss: 0.0785 - val_loss: 0.1278\n",
      "Epoch 63/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1042Epoch 00062: val_loss did not improve\n",
      "20/20 [==============================] - 26s - loss: 0.1039 - val_loss: 0.1545\n",
      "Epoch 64/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0695Epoch 00063: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.0663 - val_loss: 0.1421\n",
      "Epoch 65/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1029Epoch 00064: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.1009 - val_loss: 0.1545\n",
      "Epoch 66/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1314Epoch 00065: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.1272 - val_loss: 0.1359\n",
      "Epoch 67/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0956Epoch 00066: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.0945 - val_loss: 0.1544\n",
      "Epoch 68/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1148Epoch 00067: val_loss did not improve\n",
      "20/20 [==============================] - 26s - loss: 0.1125 - val_loss: 0.1321\n",
      "Epoch 69/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1008Epoch 00068: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.1088 - val_loss: 0.1799\n",
      "Epoch 70/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0642Epoch 00069: val_loss did not improve\n",
      "20/20 [==============================] - 26s - loss: 0.0615 - val_loss: 0.1414\n",
      "Epoch 71/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0989Epoch 00070: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.0943 - val_loss: 0.1315\n",
      "Epoch 72/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0886Epoch 00071: val_loss did not improve\n",
      "20/20 [==============================] - 26s - loss: 0.0866 - val_loss: 0.1452\n",
      "Epoch 73/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1018Epoch 00072: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.0984 - val_loss: 0.1394\n",
      "Epoch 74/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0995Epoch 00073: val_loss did not improve\n",
      "20/20 [==============================] - 26s - loss: 0.0948 - val_loss: 0.1381\n",
      "Epoch 75/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0712Epoch 00074: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.0716 - val_loss: 0.1346\n",
      "Epoch 76/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0853Epoch 00075: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.0828 - val_loss: 0.1520\n",
      "Epoch 77/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0628Epoch 00076: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 27s - loss: 0.0692 - val_loss: 0.2685\n",
      "Epoch 78/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1107Epoch 00077: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.1116 - val_loss: 0.1458\n",
      "Epoch 79/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0849Epoch 00078: val_loss did not improve\n",
      "20/20 [==============================] - 28s - loss: 0.0889 - val_loss: 0.1976\n",
      "Epoch 80/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1049Epoch 00079: val_loss did not improve\n",
      "20/20 [==============================] - 26s - loss: 0.0998 - val_loss: 0.1488\n",
      "Epoch 81/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1062Epoch 00080: val_loss did not improve\n",
      "20/20 [==============================] - 28s - loss: 0.1011 - val_loss: 0.1562\n",
      "Epoch 82/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0673Epoch 00081: val_loss did not improve\n",
      "20/20 [==============================] - 28s - loss: 0.0641 - val_loss: 0.1578\n",
      "Epoch 83/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0813Epoch 00082: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.0826 - val_loss: 0.1926\n",
      "Epoch 84/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0453Epoch 00083: val_loss did not improve\n",
      "20/20 [==============================] - 26s - loss: 0.0602 - val_loss: 0.1474\n",
      "Epoch 85/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0741Epoch 00084: val_loss did not improve\n",
      "20/20 [==============================] - 28s - loss: 0.0808 - val_loss: 0.2292\n",
      "Epoch 86/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0821Epoch 00085: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.0791 - val_loss: 0.1554\n",
      "Epoch 87/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0483Epoch 00086: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.0535 - val_loss: 0.1400\n",
      "Epoch 88/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0828Epoch 00087: val_loss did not improve\n",
      "20/20 [==============================] - 26s - loss: 0.0832 - val_loss: 0.1805\n",
      "Epoch 89/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0651Epoch 00088: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.0620 - val_loss: 0.1539\n",
      "Epoch 90/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0713Epoch 00089: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.0688 - val_loss: 0.1553\n",
      "Epoch 91/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1258Epoch 00090: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.1211 - val_loss: 0.1473\n",
      "Epoch 92/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0714Epoch 00091: val_loss did not improve\n",
      "20/20 [==============================] - 26s - loss: 0.0743 - val_loss: 0.1375\n",
      "Epoch 93/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0741Epoch 00092: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.0827 - val_loss: 0.1572\n",
      "Epoch 94/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0779Epoch 00093: val_loss did not improve\n",
      "20/20 [==============================] - 28s - loss: 0.0775 - val_loss: 0.2307\n",
      "Epoch 95/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0581Epoch 00094: val_loss did not improve\n",
      "20/20 [==============================] - 28s - loss: 0.0554 - val_loss: 0.1565\n",
      "Epoch 96/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0902Epoch 00095: val_loss did not improve\n",
      "20/20 [==============================] - 28s - loss: 0.0859 - val_loss: 0.1647\n",
      "Epoch 97/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0931Epoch 00096: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.0905 - val_loss: 0.1405\n",
      "Epoch 98/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0734Epoch 00097: val_loss did not improve\n",
      "20/20 [==============================] - 28s - loss: 0.0703 - val_loss: 0.1325\n",
      "Epoch 99/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1054Epoch 00098: val_loss did not improve\n",
      "20/20 [==============================] - 26s - loss: 0.1016 - val_loss: 0.1389\n",
      "Epoch 100/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0713Epoch 00099: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.0735 - val_loss: 0.1389\n",
      "Epoch 101/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0729Epoch 00100: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.0706 - val_loss: 0.1586\n",
      "Epoch 102/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0547Epoch 00101: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.0529 - val_loss: 0.2519\n",
      "Epoch 103/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1098Epoch 00102: val_loss did not improve\n",
      "20/20 [==============================] - 28s - loss: 0.1068 - val_loss: 0.1465\n",
      "Epoch 104/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0855Epoch 00103: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.0828 - val_loss: 0.1656\n",
      "Epoch 105/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0907Epoch 00104: val_loss did not improve\n",
      "20/20 [==============================] - 28s - loss: 0.0864 - val_loss: 0.1621\n",
      "Epoch 106/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0826Epoch 00105: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.0798 - val_loss: 0.1585\n",
      "Epoch 107/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0621Epoch 00106: val_loss did not improve\n",
      "20/20 [==============================] - 28s - loss: 0.0592 - val_loss: 0.1699\n",
      "Epoch 108/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0658Epoch 00107: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.0738 - val_loss: 0.1515\n",
      "Epoch 109/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0284Epoch 00108: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.0340 - val_loss: 0.1781\n",
      "Epoch 110/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0675Epoch 00109: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.0804 - val_loss: 0.1767\n",
      "Epoch 111/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0993Epoch 00110: val_loss did not improve\n",
      "20/20 [==============================] - 26s - loss: 0.0951 - val_loss: 0.1565\n",
      "Epoch 112/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0435Epoch 00111: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.0485 - val_loss: 0.1509\n",
      "Epoch 113/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0740Epoch 00112: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.0742 - val_loss: 0.1819\n",
      "Epoch 114/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0383Epoch 00113: val_loss did not improve\n",
      "20/20 [==============================] - 28s - loss: 0.0365 - val_loss: 0.1835\n",
      "Epoch 115/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0537Epoch 00114: val_loss did not improve\n",
      "20/20 [==============================] - 28s - loss: 0.0514 - val_loss: 0.2017\n",
      "Epoch 116/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.1167Epoch 00115: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.1174 - val_loss: 0.1601\n",
      "Epoch 117/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0497Epoch 00116: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.0526 - val_loss: 0.1536\n",
      "Epoch 118/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0892Epoch 00117: val_loss did not improve\n",
      "20/20 [==============================] - 28s - loss: 0.0887 - val_loss: 0.1518\n",
      "Epoch 119/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0679Epoch 00118: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 27s - loss: 0.0719 - val_loss: 0.2881\n",
      "Epoch 120/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0420Epoch 00119: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.0401 - val_loss: 0.1624\n",
      "Epoch 121/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0771Epoch 00120: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.0734 - val_loss: 0.1537\n",
      "Epoch 122/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0566Epoch 00121: val_loss did not improve\n",
      "20/20 [==============================] - 28s - loss: 0.0551 - val_loss: 0.1715\n",
      "Epoch 123/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0585Epoch 00122: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.0566 - val_loss: 0.1907\n",
      "Epoch 124/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0829Epoch 00123: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.0788 - val_loss: 0.1499\n",
      "Epoch 125/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0544Epoch 00124: val_loss did not improve\n",
      "20/20 [==============================] - 26s - loss: 0.0536 - val_loss: 0.1464\n",
      "Epoch 126/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0536Epoch 00125: val_loss did not improve\n",
      "20/20 [==============================] - 27s - loss: 0.0512 - val_loss: 0.1678\n",
      "Epoch 127/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0370Epoch 00126: val_loss did not improve\n",
      "20/20 [==============================] - 28s - loss: 0.0352 - val_loss: 0.2743\n",
      "Epoch 128/128\n",
      "19/20 [===========================>..] - ETA: 0s - loss: 0.0682Epoch 00127: val_loss did not improve\n",
      "20/20 [==============================] - 28s - loss: 0.0687 - val_loss: 0.1908\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(gen_train, \n",
    "                              steps_per_epoch=STEPS_PER_EPOCH, \n",
    "                              epochs=NUM_EPOCHS, \n",
    "                              verbose=1,\n",
    "                              callbacks=[checkpoint, tensorboard],\n",
    "                              validation_data=gen_test, \n",
    "                              validation_steps=TEST_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#threshold = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "threshold = [i/10 for i in range(1, 9, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate(pred, actu, THRESHOLD):\n",
    "    \"\"\"\n",
    "    Calculate Precision Recall F1-score\n",
    "    \"\"\"\n",
    "    pred = [1 if i >= THRESHOLD else 0 for i in pred]\n",
    "    actu = sum([list(i) for i in actu], [])\n",
    "    CTP = sum([1 for i in range(len(pred)) if pred[i] == 1 and actu[i] == 1])\n",
    "    CFN = sum([1 for i in range(len(pred)) if pred[i] == 0 and actu[i] == 1])\n",
    "    CFP = sum([1 for i in range(len(pred)) if pred[i] == 1 and actu[i] == 0])\n",
    "    CTN = sum([1 for i in range(len(pred)) if pred[i] == 0 and actu[i] == 0])\n",
    "    NCTP = CTN\n",
    "    NCFN = CFP\n",
    "    NCFP = CFN\n",
    "    NCTN = CTP\n",
    "    CP = CTP/(CTP+CFP)\n",
    "    CR = CTP/(CTP+CFN)\n",
    "    CF1 = 2*CP*CR/(CP+CR)\n",
    "    NCP = NCTP/(NCTP+NCFP)\n",
    "    NCR = NCTP/(NCTP+NCFN)\n",
    "    NCF1 = 2*NCP*NCR/(NCP+NCR)\n",
    "    ACC = (CTP+CTN)/(CTP+CFP+CFN+CTN)\n",
    "    print('Threshold: \\t%.3f' % (THRESHOLD))\n",
    "    print('Causal: \\tPreciion %.3f \\tRecall %.3f \\tF1-score %.3f' % (CP, CR, CF1))\n",
    "    print('Non-Causal: \\tPreciion %.3f \\tRecall %.3f \\tF1-score %.3f' % (NCP, NCR, NCF1))\n",
    "    print('Accuracy: \\t%.3f' % (ACC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2717/2717 [==============================] - 15s    \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "INPUT (InputLayer)           (None, 45)                0         \n",
      "_________________________________________________________________\n",
      "EMBEDDING (Embedding)        (None, 45, 300)           7078500   \n",
      "_________________________________________________________________\n",
      "DROPOUT_1 (Dropout)          (None, 45, 300)           0         \n",
      "_________________________________________________________________\n",
      "BiLSTM_1 (Bidirectional)     (None, 45, 300)           541200    \n",
      "_________________________________________________________________\n",
      "DROPOUT_2 (Dropout)          (None, 45, 300)           0         \n",
      "_________________________________________________________________\n",
      "BiLSTM_2 (Bidirectional)     (None, 45, 300)           541200    \n",
      "_________________________________________________________________\n",
      "DROPOUT_3 (Dropout)          (None, 45, 300)           0         \n",
      "_________________________________________________________________\n",
      "CONV (Conv1D)                (None, 43, 128)           115328    \n",
      "_________________________________________________________________\n",
      "MAXPOOLING (GlobalMaxPooling (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "DROPOUT_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "OUTPUT (Dense)               (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 8,276,357\n",
      "Trainable params: 8,276,357\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Threshold: \t0.100\n",
      "Causal: \tPreciion 0.694 \tRecall 0.890 \tF1-score 0.780\n",
      "Non-Causal: \tPreciion 0.984 \tRecall 0.946 \tF1-score 0.965\n",
      "Accuracy: \t0.939\n",
      "\n",
      "Threshold: \t0.200\n",
      "Causal: \tPreciion 0.829 \tRecall 0.860 \tF1-score 0.844\n",
      "Non-Causal: \tPreciion 0.981 \tRecall 0.976 \tF1-score 0.978\n",
      "Accuracy: \t0.962\n",
      "\n",
      "Threshold: \t0.300\n",
      "Causal: \tPreciion 0.880 \tRecall 0.848 \tF1-score 0.863\n",
      "Non-Causal: \tPreciion 0.979 \tRecall 0.984 \tF1-score 0.982\n",
      "Accuracy: \t0.968\n",
      "\n",
      "Threshold: \t0.400\n",
      "Causal: \tPreciion 0.905 \tRecall 0.817 \tF1-score 0.859\n",
      "Non-Causal: \tPreciion 0.975 \tRecall 0.988 \tF1-score 0.982\n",
      "Accuracy: \t0.968\n",
      "\n",
      "Threshold: \t0.500\n",
      "Causal: \tPreciion 0.918 \tRecall 0.787 \tF1-score 0.847\n",
      "Non-Causal: \tPreciion 0.971 \tRecall 0.990 \tF1-score 0.981\n",
      "Accuracy: \t0.966\n",
      "\n",
      "Threshold: \t0.600\n",
      "Causal: \tPreciion 0.920 \tRecall 0.771 \tF1-score 0.839\n",
      "Non-Causal: \tPreciion 0.969 \tRecall 0.991 \tF1-score 0.980\n",
      "Accuracy: \t0.964\n",
      "\n",
      "Threshold: \t0.700\n",
      "Causal: \tPreciion 0.932 \tRecall 0.756 \tF1-score 0.835\n",
      "Non-Causal: \tPreciion 0.967 \tRecall 0.992 \tF1-score 0.980\n",
      "Accuracy: \t0.964\n",
      "\n",
      "Threshold: \t0.800\n",
      "Causal: \tPreciion 0.936 \tRecall 0.716 \tF1-score 0.812\n",
      "Non-Causal: \tPreciion 0.962 \tRecall 0.993 \tF1-score 0.978\n",
      "Accuracy: \t0.960\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"775pt\" viewBox=\"0.00 0.00 263.15 775.00\" width=\"263pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 771)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-771 259.1484,-771 259.1484,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 10200887648 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>10200887648</title>\n",
       "<polygon fill=\"none\" points=\"64.521,-730.5 64.521,-766.5 190.6274,-766.5 190.6274,-730.5 64.521,-730.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127.5742\" y=\"-744.3\">INPUT: InputLayer</text>\n",
       "</g>\n",
       "<!-- 10200888208 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>10200888208</title>\n",
       "<polygon fill=\"none\" points=\"41.6104,-657.5 41.6104,-693.5 213.5381,-693.5 213.5381,-657.5 41.6104,-657.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127.5742\" y=\"-671.3\">EMBEDDING: Embedding</text>\n",
       "</g>\n",
       "<!-- 10200887648&#45;&gt;10200888208 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>10200887648-&gt;10200888208</title>\n",
       "<path d=\"M127.5742,-730.4551C127.5742,-722.3828 127.5742,-712.6764 127.5742,-703.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"131.0743,-703.5903 127.5742,-693.5904 124.0743,-703.5904 131.0743,-703.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 10200888040 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>10200888040</title>\n",
       "<polygon fill=\"none\" points=\"52.4897,-584.5 52.4897,-620.5 202.6587,-620.5 202.6587,-584.5 52.4897,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127.5742\" y=\"-598.3\">DROPOUT_1: Dropout</text>\n",
       "</g>\n",
       "<!-- 10200888208&#45;&gt;10200888040 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>10200888208-&gt;10200888040</title>\n",
       "<path d=\"M127.5742,-657.4551C127.5742,-649.3828 127.5742,-639.6764 127.5742,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"131.0743,-630.5903 127.5742,-620.5904 124.0743,-630.5904 131.0743,-630.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 9851051088 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>9851051088</title>\n",
       "<polygon fill=\"none\" points=\"0,-511.5 0,-547.5 255.1484,-547.5 255.1484,-511.5 0,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127.5742\" y=\"-525.3\">BiLSTM_1(lstm_1): Bidirectional(LSTM)</text>\n",
       "</g>\n",
       "<!-- 10200888040&#45;&gt;9851051088 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>10200888040-&gt;9851051088</title>\n",
       "<path d=\"M127.5742,-584.4551C127.5742,-576.3828 127.5742,-566.6764 127.5742,-557.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"131.0743,-557.5903 127.5742,-547.5904 124.0743,-557.5904 131.0743,-557.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 10200888992 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>10200888992</title>\n",
       "<polygon fill=\"none\" points=\"52.4897,-438.5 52.4897,-474.5 202.6587,-474.5 202.6587,-438.5 52.4897,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127.5742\" y=\"-452.3\">DROPOUT_2: Dropout</text>\n",
       "</g>\n",
       "<!-- 9851051088&#45;&gt;10200888992 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>9851051088-&gt;10200888992</title>\n",
       "<path d=\"M127.5742,-511.4551C127.5742,-503.3828 127.5742,-493.6764 127.5742,-484.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"131.0743,-484.5903 127.5742,-474.5904 124.0743,-484.5904 131.0743,-484.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 10016390056 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>10016390056</title>\n",
       "<polygon fill=\"none\" points=\"0,-365.5 0,-401.5 255.1484,-401.5 255.1484,-365.5 0,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127.5742\" y=\"-379.3\">BiLSTM_2(lstm_2): Bidirectional(LSTM)</text>\n",
       "</g>\n",
       "<!-- 10200888992&#45;&gt;10016390056 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>10200888992-&gt;10016390056</title>\n",
       "<path d=\"M127.5742,-438.4551C127.5742,-430.3828 127.5742,-420.6764 127.5742,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"131.0743,-411.5903 127.5742,-401.5904 124.0743,-411.5904 131.0743,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 9851224696 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>9851224696</title>\n",
       "<polygon fill=\"none\" points=\"52.4897,-292.5 52.4897,-328.5 202.6587,-328.5 202.6587,-292.5 52.4897,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127.5742\" y=\"-306.3\">DROPOUT_3: Dropout</text>\n",
       "</g>\n",
       "<!-- 10016390056&#45;&gt;9851224696 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>10016390056-&gt;9851224696</title>\n",
       "<path d=\"M127.5742,-365.4551C127.5742,-357.3828 127.5742,-347.6764 127.5742,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"131.0743,-338.5903 127.5742,-328.5904 124.0743,-338.5904 131.0743,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 10016563096 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>10016563096</title>\n",
       "<polygon fill=\"none\" points=\"72.8369,-219.5 72.8369,-255.5 182.3115,-255.5 182.3115,-219.5 72.8369,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127.5742\" y=\"-233.3\">CONV: Conv1D</text>\n",
       "</g>\n",
       "<!-- 9851224696&#45;&gt;10016563096 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>9851224696-&gt;10016563096</title>\n",
       "<path d=\"M127.5742,-292.4551C127.5742,-284.3828 127.5742,-274.6764 127.5742,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"131.0743,-265.5903 127.5742,-255.5904 124.0743,-265.5904 131.0743,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 10016641432 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>10016641432</title>\n",
       "<polygon fill=\"none\" points=\"6.6035,-146.5 6.6035,-182.5 248.5449,-182.5 248.5449,-146.5 6.6035,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127.5742\" y=\"-160.3\">MAXPOOLING: GlobalMaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 10016563096&#45;&gt;10016641432 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>10016563096-&gt;10016641432</title>\n",
       "<path d=\"M127.5742,-219.4551C127.5742,-211.3828 127.5742,-201.6764 127.5742,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"131.0743,-192.5903 127.5742,-182.5904 124.0743,-192.5904 131.0743,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 9994599448 -->\n",
       "<g class=\"node\" id=\"node10\">\n",
       "<title>9994599448</title>\n",
       "<polygon fill=\"none\" points=\"52.4897,-73.5 52.4897,-109.5 202.6587,-109.5 202.6587,-73.5 52.4897,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127.5742\" y=\"-87.3\">DROPOUT_4: Dropout</text>\n",
       "</g>\n",
       "<!-- 10016641432&#45;&gt;9994599448 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>10016641432-&gt;9994599448</title>\n",
       "<path d=\"M127.5742,-146.4551C127.5742,-138.3828 127.5742,-128.6764 127.5742,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"131.0743,-119.5903 127.5742,-109.5904 124.0743,-119.5904 131.0743,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 10001245128 -->\n",
       "<g class=\"node\" id=\"node11\">\n",
       "<title>10001245128</title>\n",
       "<polygon fill=\"none\" points=\"71.1245,-.5 71.1245,-36.5 184.0239,-36.5 184.0239,-.5 71.1245,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"127.5742\" y=\"-14.3\">OUTPUT: Dense</text>\n",
       "</g>\n",
       "<!-- 9994599448&#45;&gt;10001245128 -->\n",
       "<g class=\"edge\" id=\"edge10\">\n",
       "<title>9994599448-&gt;10001245128</title>\n",
       "<path d=\"M127.5742,-73.4551C127.5742,-65.3828 127.5742,-55.6764 127.5742,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"131.0743,-46.5903 127.5742,-36.5904 124.0743,-46.5904 131.0743,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = '/Users/lizhn7/Downloads/DATA/semeval2010_task8_all_data/causal_detection/cp_logs/1/weights.033-0.121688.hdf5'\n",
    "model.load_weights(filename)\n",
    "result = model.predict(xTest, batch_size=BATCH_SIZE, verbose=1)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "for THRESHOLD in threshold:\n",
    "    calculate(result, yTest, THRESHOLD)\n",
    "    print('')\n",
    "    \n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "192px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
